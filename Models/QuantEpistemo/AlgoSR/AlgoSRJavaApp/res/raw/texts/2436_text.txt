Les nouveaux principes de l’urbanisme et de la gestion des territoires initiés entre autre par la loi SRU et la LOADDT, insistent sur l’idée de projet : se projeter dans l’avenir, imaginer les territoires urbains dans le futur est devenu un préalable nécessaire à la planification : c’est en connaissant ce vers quoi l’on veut tendre demain que l’on peut aujourd’hui prévoir et réglementer. L’exercice de prospective est devenu une nécessité. Cependant, la prospective n’est pas un exercice facile. Il est extrêmement délicat de tenter une prévision du futur, et de nombreuses expériences montrent que, souvent, le simple prolongement d’une tendance ne fournit qu’une extrapolation médiocre des processus à l’oeuvre, déconnectée des réalités du terrain. Pour dépasser ce stade, la prospective tente aujourd’hui d’intégrer une plus grande complexité dans les paramètres à prendre en compte et les processus qui les lient ; de surcroît, elle s’efforce de ne pas raisonner dans un cadre absolu, mais réfléchit en termes de scénarios, afin de brasser un panel plus large de devenirs possibles, en vérifiant l’adage de M. Blondel : « l’avenir ne se prévoit pas, il se prépare ». La mise en œuvre d’une telle démarche prospective n’est toutefois pas aisée, et nécessite l’utilisation d’outils adaptés pour prendre en compte, gérer et analyser la complexité et l’enchevêtrement des processus, avant de les projeter dans le futur. Parmi ces outils, la modélisation apparaît comme un réel outil d’aide à la décision. En urbanisme et en aménagement du territoire, modéliser une série de processus (ceux qui conduisent à l’étalement ou au renouvellement urbain par exemple) consiste à les simplifier, de manière à mieux comprendre comment il interagissent entre eux, mais également à reproduire ou à modifier leur comportement in vitro, afin de tester des solutions qui permettraient d’en influencer les conséquences. En quelque sorte, on cherche donc à répondre à des questions du type : « que se passerait-il si je faisait ça ? ou « que deviendrait ceci si je faisait cela » ? La démarche proposée dans cet article reprend cette idée en l’appliquant spécifiquement à la question du développement urbain. Ici, la complexité du processus d’urbanisation est prise en compte par l’intermédiaire de trois questions simples, liées à des modèles mathématiques ou statistiques opérationnels, dont les principes sont faciles à comprendre : combien ? où ? et quoi ? A partir de l’exemple de la ville de Belfort, nous proposons dans une première partie d’appréhender cette simplification à travers ses trois étapes. Dans la deuxième partie, nous insistons sur la difficulté du paramétrage d’une telle modélisation et apportons des éléments pour mieux maîtriser cette phase pratique, indispensable à l’exercice prospectif.
La modélisation1 est généralement assimilée à une simplification de la réalité, qui vise à la reproduire et à mieux en connaître certains aspects. Pour la dynamique des systèmes urbains, tenir compte de cette simplification peut consister à considérer l’évolution urbaine à travers trois questions simples et opérationnelles à propos de l’occupation du sol des systèmes urbaines et périurbains : « combien ? », « où ? » et « quoi ? »2. Combien d’hectares évolueront dans le futur, où sont-ils situés et de quoi sont-ils composés ? La démarche prospective et la modélisation présentées ici reprennent ces questions en trois étapes distinctes et complémentaires. Mais elles s’appuient sur un préalable nécessaire qui servira de fil conducteur à la modélisation : la visualisation des dynamiques spatiales et leur évolution à travers un espace cellulaire.
L’une des principales difficultés liées à la mesure de l’occupation du sol réside dans la visualisation de son évolution. Certes, les archives et les cartothèques disposent de nombreux documents, l’IGN en à commercialisé beaucoup et depuis longtemps, les satellites survolent l’espace terrestre et créent chaque jour une information géographique plus détaillée. Mais, si ces informations qui s’échelonnent dans le temps offrent une vision diachronique de la dynamique de l’occupation du sol, elles ne sont pas suffisamment comparables entre elles pour en permettre une lecture précise. Une méthode plus rigoureuse s’impose pour étudier les évolutions dans l’espace et dans le temps. Le carroyage offre ici une solution simple et adéquate, qui consiste à visualiser chaque document (qui correspond alors à un état daté dans le temps) par l’intermédiaire d’une grille et de lire la dynamique de l’évolution du sol à travers chacune de ses cellules. Dans l’espace carroyé, considérons alors que chaque état E(t) correspond à un système définit par N carreaux de carroyage qui sont autant de cellules.Pour chacun des états, les cellules contiennent une variable d’occupation du sol. L’occupation du sol au temps t de la cellule Ni donnée du système spatial est notée k. Les catégories d’occupation du sol étant attribuées aux cellules de manière exclusive, à chaque cellule ne peut correspondre qu’une seule valeur d’occupation du sol k pour chaque date. Mathématiquement, ceci peut être formalisé ainsi (cf. de Almeida, 2002) :

Issus de sources diverses, les états peuvent alors s’assimiler à des matrices montrant l’occupation du sol pour chaque date. Leur superposition exacte engendre leur comparabilité exacte et, par après, une possibilité de lecture spatio-temporelle rigoureuse de l’occupation du sol, qui peut être cartographiée et quantifiée cellule par cellule (Antoni, 2002). A Belfort, le carroyage de trois documents cartographiques montrant l’occupation du sol en 1955, 1975 et 19953 a permis de calculer que la ville s’est étendue de 18% de 1955 à 1975, de 36,5% de 1975 à 1995, soit une évolution totale de 62% sur la période 1955-1995 (Figure 1). Le carroyage apparaît donc comme une méthode simple et adéquate pour visualiser l’évolution de l’occupation du sol d’un territoire, particulièrement ici la dynamique d’étalement urbain ; l’espace cellulaire qu’il produit est déterminant pour la modélisation de cette dynamique.

Figure 1
Toutefois, la lecture de trois images statiques juxtaposées de l’occupation du sol (1955, 1975, 1995) n’a que peu d’intérêt dans le cadre d’un exercice prospectif. La prise en compte de ce qui se passe entre chaque image peut par contre conduire à la formulation d’un processus de transition nettement plus intéressant. En confrontant les états deux à deux et cellule par cellule, il est en effet possible de connaître la mutation de l’occupation du sol de chaque cellule entre t et t+1, afin de saisir ces mutations de façon dynamique. Théoriquement, chaque cellule a la possibilité d’évoluer vers n’importe quelle autre catégorie d’occupation du sol, ou bien de rester dans sa catégorie d’origine. La dynamique du modèle peut donc se présenter comme une série de transitions possibles d’une occupation du sol de départ k à une occupation du sol d’arrivée l. Au niveau d’une cellule Ni donnée, une transition ∆ de k au temps t vers l au temps t+1 peut s’écrire ainsi :

De manière à simplifier la complexité engendrée par le nombre théorique de transitions possibles, on peut ne plus considérer les mutations au niveau des cellules, mais au niveau des catégories d’occupation du sol (ce qui revient à passer du niveau micro au macro). La transition agrégée pour le système complet est alors :

Ce faisant, il devient également possible de construire deux tableaux correspondant à des matrices de contingence qui relèvent le nombre de cellules passées d’une catégorie k à l entre t et t+1 (ici entre 1955 et 1975 et entre 1975 et 1995). L’occupation du sol peut alors se résumer en un vecteur pour chaque date, de taille égale au nombre de catégories d’occupation du sol considérées. Leur somme étant toujours égale à 1, ces vecteurs indiquent la proportion de chaque catégorie sur l’ensemble de l’espace considéré, ce qui fournit les éléments nécessaires à la construction d’une chaîne de Markov.
Une chaîne de Markov correspond en effet à « un processus dont les probabilités de transition sont des probabilités conditionnelles au passé » (Feller, 1968 ; Berchtold, 1998), qui exprime l’état d’une variable à une époque t en fonction d’observations passées de cette variable. Sa « mise en marche » (Rimbert, 1995) nécessite la préparation préalable : 1. De la description d’un état en valeurs relatives de sa masse totale (l’occupation du sol visualisée sous la forme d’un vecteur par exemple) ; 2. D’une matrice de transition où sont exprimées les probabilités de déplacement des différents groupes d’observations, d’une catégorie vers une autre ; 3. D’un opérateur de transformation diachronique, en l’occurrence une multiplication matricielle à itération. Cela veut dire qu’une simulation de l’état E(t+1) est possible si l’on multiplie le vecteur correspondant à un état E par la matrice de contingence correspondant, après que celle-ci ait été transformée en une matrice de probabilités de transition d’une catégorie d’occupation du sol vers une autre. Pour considérer les transitions comme des probabilités à partir des contingences observées, on pose simplement :

On considère alors la chaîne de Markov de la façon suivante :

Ainsi définie, une chaîne de Markov offre donc une possibilité de prospective en permettant de calculer un état futur à partir de l’état présent connu, en se basant sur l’observation des évolutions passées et leur probabilité. A Belfort, on calcule ainsi le nombre de cellules de chaque catégorie d’occupation du sol en 2015 à partir des états de 1955, 1975 et 1995 (avec un pas de temps régulier de 20 ans), ce qui correspond effectivement à quantifier les dynamiques urbaines.
Le modèle de transition (chaîne de Markov) permet donc bien de répondre à la question « combien ? » en déterminant le nombre de cellules de chaque catégorie d’occupation su sol dans le futur. Mais il ne dit rien sur leur localisation. Pour répondre à cette question « où ? », nous faisons référence à la notion d’interactions spatiales introduite par E.L. Ullman (1957), qui aide à prendre en compte la multitude des possibilités de localisation qu’offre une ville et son aire urbaine. Elle se définit comme une « action réciproque (rétroaction) entre deux ou plusieurs acteurs ou lieux dans un système. Tous les échanges (de matière, de personnes, d’information), par exemple au niveau individuel entre producteurs et clients, entre partenaires, ou au niveau agrégé entre des villes et des régions (ce sont les interactions spatiales), sont des interactions dans la mesure où ils provoquent des changements interdépendants dans les comportements ou dans les structures »4. E.L. Ullman a également proposé trois conditions permettant de prendre en compte et de caractériser ces interactions spatiales : la complémentarité, la substitution et la friction de la distance.
Généralement, une interaction résulte de la différence qui existe entre deux lieux. A partir de cette différence peut naître une demande de l’un et une offre de l’autre, c’est-à-dire une complémentarité entre des attributs demandés à un endroit et présents à un autre endroit. Nous parlons bien alors d’attributs et non uniquement d’objets ou de produits comme le font R. Abler et al. (1972) ou P. Haggett (1973) car dans ces cas, l’idée d’interaction sert à introduire un mouvement de produits (transportés par camions, par exemple, chez M. Helvig (1964)). Ici, l’interaction peut également s’interpréter comme une migration de personnes, possiblement définitive, si une complémentarité spécifique existe entre des gens qui veulent disposer d’un cadre particulier pour habiter, et des lieux qui disposent de ce cadre. C’est en ce sens qu’elle s’associe à la problématique de la dynamique urbaine. Toutefois, une telle complémentarité ne génère une interaction qu’en cas d’absence de substitution, comme a pu le montrer S.A. Stouffer (1940). En effet, si l’on considère une interaction possible entre un lieu i et un lieu j, il est nécessaire de considérer simultanément tout autre lieu k, qui pourrait également apparaître comme une origine ou une destination possible pour chaque mouvement, et de tester s’il n’est pas finalement une « meilleure » origine ou destination (Abler et al., 1972). L’interaction se ferait alors entre i et k, et non entre i et j. Pour l’estimer, il est nécessaire de ne pas considérer les lieux deux à deux, mais de les prendre en compte tous simultanément. Enfin, la troisième condition fait intervenir la notion de « transférabilité » (transferability), c’est-à-dire la friction de la distance, généralement mesurée en temps ou en coût : si le temps (ou l’argent) dépensé pour parcourir la distance qui sépare i et j est trop important, l’interaction ne se fera pas, même si il n’y a aucune substitution, et même si la complémentarité est parfaite. Si les choses ne peuvent bouger d’un lieu à un autre à cause de leur coût de déplacement, on les remplacera par d’autres choses, ou bien on s’en passera. Sur le plan théorique, un modèle de potentiel permet de prendre en compte simultanément ces trois conditions, nécessaires à la mise en place d’une interaction spatiale5 :

Le potentiel de chaque lieu étant égal à la somme des masses des lieux de son voisinage, divisées par la distance qui les sépare de ces lieux, le modèle tient compte du fait que tout lieu est fortement déterminé par une certaine forme de dépendance spatiale : « everything is related to everything, but near things are more related than distant things » (Tobler, 1976). Mais l’importance des relations entre les lieux est également fonction de leur masse, c’est-à-dire en quelque sorte de leur « capacité d’attractivité ». En adaptant un modèle de potentiel à une image satellite, dont la structure cellulaire est comparable à  celle d’un carroyage, C. Weber et al., (1998) assimilent chaque pixel à un lieu. Or, cette assimilation influence fortement les résultats parce qu’elle inclut la surface de chacune des cellules dans le calcul. Il est donc clair que les résultats obtenus à l’aide d’un modèle de potentiel à l’échelle de l’Europe par C. Grasland (1994), par exemple, pour évaluer un potentiel migratoire, sont peu comparables avec ceux que peut produire le même modèle pour évaluer les potentiels des dynamiques urbaines. Dans un cas, une forêt est représentée par un point correspondant à une masse, et dans l’autre, elle est représentée par n cellules ayant chacune leur masse propre. A masse identique, la forêt carroyée est donc n fois plus importante dans un cas que dans l’autre. Ce problème pose la question de la valeur des masses à utiliser, et avec elle, celle du paramétrage d’un modèle de potentiel, qui devra tenir compte de ces spécificités, et que nous étudierons dans la deuxième partie.
De surcroît, dans la formule standard, les potentiels sont calculés en fonction de masses m, déterminées de façon unique pour chaque catégorie d’occupation du sol : si l’on accorde une masse de 5 aux zones industrielle d’une ville par exemple, elle sera égale à 5 quelle que soit la nature de la zone d’activité étudiée. Pour limiter ce problème, il est possible de distinguer le comportement de chaque cellule au sein d’une même catégorie, par l’intermédiaire d’un coefficient de pondération noté v. Il pondère la masse m définie pour une catégorie d’occupation du sol en fonction de la nature individuelle des cellules. Parallèlement, le modèle de potentiel peut également tenir compte du code de l’urbanisme, essentiellement des POS ou des PLU (qui interdisent la construction ou qui la favorisent) par l’intermédiaire d’un coefficient de contrainte noté k6. Comme v, k n’intervient plus ici en lien avec les catégories d’occupation du sol, mais est directement lié aux cellules. Il multiplie la valeur de potentiel préalablement calculée :

Ainsi, il est possible de localiser le nombre n de cellules qui évolueront dans le futur (n étant déterminé à la première étape par les chaînes de Markov) : ce sont celles dont la valeur
de potentiel est la plus élevée. Cette deuxième étape permet donc effectivement de localiser les dynamiques en fonction des critères retenus par le modèle de potentiel, mais également des masses et des distances utilisées pour son paramétrage.
A la troisième étape, on considère enfin l’occupation du sol qui différencie chaque cellule. Car si l’on connaît leur nombre et leur place, on ne sait rien encore de ce qu’elles sont, c’est-à-dire de la catégorie d’occupation du sol à laquelle elles appartiennent. La méthode employée s’appuie sur les automates cellulaires, dont l’utilisation en géographie fait écho à la conception cellulaire de l’espace géographique qu’ont pu défendre W. Tobler (1979) et H. Couclelis (1985) et qui a permis de révéler le caractère profondément spatialisé de ce type d’outils (Couclelis, 1988). Le carroyage (cf. point 1.1) correspond en effet parfaitement à une configuration d’automates cellulaires, dont les catégories d’occupation du sol rappellent les deux seules possibilités (de vie ou de mort) de l’expérience de J. Conway (1970). Pour définir le problème de façon formelle, on pose alors que la catégorie de chaque cellule est déterminée par son voisinage, c’est-à-dire par la catégorie d’occupation du sol des cellules environnantes dans un rayon donné (cf. Batty et al., 1999) :

Issus de l’intelligence artificielle distribuée, les automates cellulaires ont alors le double avantage de permettre la détermination de la catégorie d’occupation du sol des cellules en fonction de leur voisinage (c’est-à-dire de différencier les cellules précédemment quantifiées et localisées), et de disposer de la souplesse nécessaire pour intégrer les deux précédents modèles : ils sont contraints avec les résultats du modèle de transition (étape 1) et du modèle de potentiel (étape 2), pour produire in fine une modélisation qui combine les trois étapes. Ceci est rendu possible par l’intermédiaire de règles, permettant de tenir compte de cas de figure multiples. Le problème majeur consiste alors à définir des règles pertinentes, ce qui rappelle la problématique générale de la modélisation et surtout de son paramétrage.
L’espace cellulaire jouant le rôle de fil conducteur, la juxtaposition de modèles associés et dédiés chacun à un rôle particulier offre une modélisation « simple ». L’expérience montre en effet que la communication du contenu, des tenants et des aboutissants des trois étapes et de leur combinaison est généralement aisée et pédagogique. Dans le cadre de simulations liées à un exercice prospectif, cette originalité se différencie fortement des travaux pourtant similaires de R. White et al. (2003) par exemple7, et facilite la mise en oeuvre d’une démarche de modélisation en collaboration avec les élus ou les techniciens en charge de gestion territoriale. Toutefois, si l’ensemble apparaît plus simple, le paramétrage de chaque modèle demeure complexe et déterminant dans la production de résultats liés à la simulation de l’évolution de l’occupation du sol. Aussi, autant que sur la modélisation en elle-même, il est indispensable de montrer comment ses paramètres peuvent être estimés, les biais qu’ils introduisent, et les incertitudes auxquelles ils demandent de faire face.
En effet, si l’enchaînement de modèles autorisant la prise en compte d’une multitude de cas complexes et différents est cohérente sur le plan théorique, ses applications concrètes apparaissent quant-à-elles comme une source d’humilité. Pour en prendre conscience, utilisons un exemple qui montre que le lien entre les résultats d’un modèle et les coefficients utilisés pour les déterminer sont variables et difficiles à appréhender. Pour obtenir un résultat égal à 10 - c’est l’exemple - plusieurs « chemins » sont possibles. Une addition offrira onze possibilités : 0+10, 1+9, 2+8, 3+7, 4+6, 5+5, 6+4, 7+3, 8+2, 9+1 et 10+0. Si l’on cherche uniquement la bonne solution, l’ensemble de ces possibilités peut être considéré comme juste. Par contre si la manière avec laquelle cette solution est obtenue importe, chaque possibilité reflète un processus différent ; aucune n’est équivalente. Des scénarios différents mènent donc à un résultat identique. Ainsi, si un processus est validé pour l’exactitude de son résultat, il ne peut être considéré comme correct tant que l’on n’aura pas montré qu’il est l’unique processus permettant d’atteindre ce résultat, ou encore que parmi une multitude de processus permettant de l’atteindre, il est le seul qui puisse être interprété correctement sur le plan théorique. Pour pallier cette difficulté, il est néanmoins possible de tester toutes les possibilités de manière à sélectionner celles qui offrent les résultats les plus proches de la réalité, voire celles qui reproduisent exactement la réalité8. Mais même dans ce cas, on ne pourrait affirmer que le résultat soit vraiment juste, dans la mesure où d’autres mesures, voire même d’autres modèles, c’est-à-dire d’autres processus auraient peut être menés à des résultats identiques : pour obtenir 10, on aurait aussi pu proposer 2x5 ou 2,67 x 3,25 +1,3225, ou n’importe quoi d’autre qui mène à 10. Considérer la prospective à travers la modélisation apparaît donc comme une gageure qui rend impossible la certitude de disposer d’un résultat exact.
Dans ce cadre, pour réduire la masse d’incertitudes, l’observation méticuleuse du passé est enrichissante. Sur le plan thématique d’abord, elle montre en effet des grandes tendances qui renseignent sur ce qui s’est passé de manière quantitative. Sur le plan des simulations ensuite, ces tendances peuvent être reproduites et extrapolées. La modélisation et l’usage qui en est fait change donc de cadre puisqu’il ne s’agit plus de faire correspondre les paramétrages à une réalité qu’on sait ne pas pouvoir reproduire exactement, mais plutôt de les associer à une réalité dont on connaît les limites ou que l’on imagine. La prospective autorise en effet à postuler une correspondance totale entre cette réalité et les coefficients mis en place pour la simuler. Ceci peut se faire par l’intermédiaire de scénarios. Dans le dictionnaire de R. Brunet et al. (1992), on lit effectivement que « la méthode des scénarios […] participe des modèles de simulation. Elle est une des entrées commodes de la prospective en économie, en aménagement du territoire ». Un scénario est défini comme une « méthode d’anticipation poussant à bout les conséquences logiques d’hypothèses ou de tendances préalables, sous des contraintes imposées ou contrastées ; ou au contraire, imaginant les conséquences d’un infléchissement, d’une nouvelle stratégie ». Chaque scénarios apparaît donc intéressant pour mettre en correspondance une certaine vision du futur avec une possibilité de modélisation, indépendamment de la manière avec laquelle cette vision du futur est mise au point ; et donc comme une série d’éléments pour paramétrer les modèles.
Au delà du principe théorique duquel relève le modèle de transition examiné dans la première partie, la question qui se pose consiste alors à paramétrer concrètement une chaîne de Markov, c’est-à-dire à construire une matrice de transition pour simuler. Il convient d’abord d’étudier précisément chaque matrice issue de l’observation du passé pour saisir la réalité des changements à laquelle correspondent leurs valeurs de transition (Tableau 1). Sur les deux périodes retenues pour la ville de Belfort, la catégorie espaces « libres » (champs, prairies, vergers, etc.) diminue très significativement. Entre 1955 et 1975, elle a eu 12% de chance d’évoluer vers une autre catégorie, parmi lesquels les principales modifications ne correspondent pas à une transition vers des espaces bâtis, mais bien d’abord à une recomposition des espaces naturels entre eux : les champs ont une probabilité de devenir de la forêt ou de l’eau plus forte que d’être urbanisés. Entre 1975 et 1995 par contre, les espaces « libres » n’ont plus qu’une probabilité de 0,82 de rester dans leur catégorie, soit 18% de chance d’évoluer. Les transitions majeures ont également changé : la transition vers la forêt reste la plus importante mais la tendance générale ne correspond plus d’abord à une recomposition des espaces naturels. Derrière le reboisement que montre la matrice, mais dans un ordre de grandeur presque trois fois moins important, les champs sont remplacés par des maisons individuelles, puis  des zones d’activité et des routes. La maison individuelle devient alors le type d’habitation privilégié de l’urbanisation, et s’accompagne d’une viabilisation nécessaire, qui laisse loin derrière la construction de bâti dense, pourtant encore important lors de la période 1955-1975. On retrouve globalement dans ces matrices les grands constats de l’urbanisation d’après-guerre à nos jours, confortés par chaque recensement de la population. Quelques cas particuliers montrent pourtant aussi qu’elles sont très fortement liées à la période pour laquelle elles ont été mesurées, et présentent des aberrations ou des dangers pour la modélisation.
Certains équipements apparus soudainement dans l’occupation du sol ont en effet correspondu à une innovation ou à une modification propre à une période et n’ont pas eu la même importance dans la période suivante. A Belfort par exemple, les opérations d’open planning et la création de l’autoroute s’associent à la première période et ne se retrouvent pas dans la seconde. De ce fait, une simulation produite par la matrice de transition de la période 1955-1975 montre une première occupation du sol en 2015, si les tendances de changement à partir de 1995 étaient celles observées entre 1955 et 1975. Mais un second vecteur peut être calculé de la même façon, à partir des probabilités de transition observées sur la période 1975-1995. L’interprétation de ces deux vecteurs renseigne sur les tendances inscrites dans les deux matrices et leur comparaison graphique permet de les mettre en perspective avec les vecteurs observés dans le passé. Bien que les valeurs des deux matrices ne soient pas aux antipodes, elles montrent des tendances qui s’opposent en partie. Sur le plan des espaces naturels, par exemple, la première tendance (1955-1975) accorde un avantage aux champs sur les forêts. Il en est de même pour les surfaces en eaux : elles apparaissent bien plus importantes en 2015 si l’on utilise la première matrice plutôt que la deuxième.

Tableau 1
Ces différences s’expliquent en partie. L’augmentation considérable des surfaces en eaux durant la première période (elles passent de 2,29 à 4,18% de l’occupation du sol) semble en effet due à l’exploitation de gravières pour la construction des autoroutes. Ainsi, l’augmentation des surfaces en eaux est prise en compte dans la première matrice (1975-1995), alors que celle des autoroutes ne l’est que dans la deuxième. De ce fait, une occupation du sol simulée à partir de la première matrice augmente trop fortement les surfaces en eaux en 2015 (en fait les gravières) ; une simulation à partir de la deuxième matrice augmente trop fortement les autoroutes. Un cas à peu près similaire existe pour la catégorie du bâti collectif, dont on a dit qu’il a été très fortement construit dans la première période, et quasiment pas dans la seconde. Une simulation à partir de la première matrice aura donc tendance a créer du bâti collectif comme si l’on était toujours dans les années 60, alors que la deuxième matrice n’en créera presque pas. Aucune des deux matrices observées dans le passé ne permet donc de donner une image réaliste de ce que l’avenir pourrait être. Elles indiquent néanmoins quelques valeurs que pourraient prendre les pourcentages de l’occupation du sol si tout se passait entre 1995 et 2015 comme cela s’est passé entre 1955 et 1975 ou entre 1975 et 1995. Pour pallier les aberrations identifiées, on propose alors d’uniformiser les deux matrices, afin de tenir compte simultanément des deux périodes (ou de plus), dont les tendances peuvent se compenser :

Ce « couplage » de matrices correspond en quelque sorte à une moyenne pondérée, sur laquelle il n’est pas invraisemblable de se fonder pour simuler l’avenir. L’exemple des autoroutes confirme cette vraisemblance. On lit en effet sur l’occupation du sol de 1975 qu’elles représentaient 0,05% de l’espace carroyé, alors qu’elle en représentait 0,16% en 1995. Or, en 1975, on compte un seul échangeur contre trois en 1995. Un échangeur autoroutier semble donc bien représenter environ 0,05% de l’espace considéré : le couplage (avec des pondérations égales ; α= 0,5) des deux matrices indique qu’en 2015, les échangeurs devraient représenter 0,24% de l’occupation du sol, soit 4 ou à 5 échangeurs. On sait que les projets de développement autour de l’agglomération belfortaine correspondent à ces simulations : un quatrième échangeur au moins est actuellement en projet. Pour simuler vers 2015, on peut également estimer que la deuxième période doit être plus représentée que la première parce qu’elle témoigne de tendances plus actuelles. La façon dont les espaces collectifs y sont considérés (très peu de constructions de tours ou de barres) suffit à s’en convaincre. Aussi, plutôt que de considérer les deux matrices de manière égale (α = 0,5), on peut estimer que la seconde est trois fois plus importante que la première et leur affecter un coefficient respectif de  α =0,75 et  α =0,25, ce qui produit un nouveau vecteur.

Tableau 2
Le tableau précédent (Tableau 2) synthétise ces résultats. Les deux premiers vecteurs sont obtenus en utilisant uniquement la première matrice (a) ou bien uniquement la seconde (b). Les suivants sont obtenus par uniformisation des deux matrices, soit par pondération égale (c), soit par pondération différentielle (d). Le dernier vecteur, enfin, présente la moyenne des quatre précédents (e). Bien sûr, pour les raisons évoquées précédemment, aucun d’entre eux ne peut être considéré comme juste, ni sur le plan thématique, ni sur le plan théorique. Mais la considération simultanée de ces quatre possibilités issues de l’observation du passé offre une fourchette intéressante de valeurs comparables les unes aux autres. Elle indique un ordre de grandeur dans lequel les choses pourraient effectivement prendre place si la dynamique urbaine continuait entre 1995 et 2015 comme elle a été observée entre 1955 et 1995. Ces différentes valeurs permettent de calculer un nombre de cellules simulé pour chaque occupation du sol à l’horizon 2015. Elles offrent de ce fait une extrapolation des transitions observées dans le passé, utile pour quantifier les modifications de l’occupation du sol.
Sur le plan de la localisation de ces modifications, une étude par postdiction9 permet ensuite de tester les coefficients du modèle de potentiel afin d’identifier ceux qui correspondent aux évolutions observées dans le passé. Connaissant l’état du terrain en 1975 et en 1995, le calcul des potentiels à partir de l’image de 1975 permet en effet de comparer les résultats simulés pour 1995 avec les résultats observés à cette date. Cette opération est rendue complexe par le fait que le nombre de cellules de chaque catégorie d’occupation du sol doit être pris en compte : comme nous l’avons souligné précédemment (cf. point 1.3), à valeur de masse égale, une catégorie d’occupation du sol importante en nombre de cellules engendrera des valeurs de potentiel plus importantes qu’une catégorie d’occupation du sol comprenant peu de cellules. Par rapport au réseau routier, il apparaît alors nécessaire de sous estimer le bâti, afin que les valeurs de potentiels ne croissent pas de manière exponentielle. Ce problème engendre une difficulté supplémentaire dans l’interprétation des résultats puisqu’il rend les masses m incomparables les unes avec les autres.
Néanmoins, une configuration cohérente a été définie, qui produit des résultats partiellement justes : 34% des cellules construites entre 1975 et 1995 (soit plus d’une sur trois) sont correctement localisées, c’est-à-dire à l’endroit exact que montre l’image de 1995. A 150 mètres près, ce sont plus de 75% des cellules (soit plus de trois sur quatre) qui sont correctement localisées. Cette configuration a été obtenue en composant plusieurs familles de catégories d’occupation du sol. En premier lieu, l’ensemble des espaces naturels est évalué comme absolument non-attractif ; on lui affecte donc une masse m=0. L’ensemble des catégories composant le bâti (résidentiel et non-résidentiel) constitue la deuxième famille, jugée assez attractive. Globalement, on lui affecte une masse m=2 10. Toutefois, à l’intérieur de cette deuxième famille, les maisons individuelles et les zones d’activité ont été distinguées comme deux cas particuliers. Les premières ont été évaluées comme plus attractives dans la mesure où elles correspondent à un archétype de l’habitation périurbaine, duquel il peut convenir de se rapprocher pour construire d’autres maisons individuelles. A contrario, les zones d’activités ont globalement été considérées comme non attractives du fait des nuisances qu’elles sont susceptibles de produire ; leur masse est nulle (m=0). Enfin, la dernière famille de catégories d’occupation du sol comprend les réseaux. Parmi ceux-ci, les gares et les autoroutes n’ont pas été traitées, parce que leur faible nombre de cellules aurait nécessité des valeurs de masse disproportionnées pour entrer en concurrence avec les catégories de bâti, ce qui explique probablement une partie des écarts entre les valeurs simulées et les valeurs observées (on sait en effet qu’ils importent dans les choix de localisation et dans les configurations spatiales qui résultent de ces choix ; cf. B. Merenne-Schoumaker (1981) par exemple). Seules les catégories correspondant au réseau routier (hors autoroutes) ont donc été codées, avec des valeurs de masse correspondant respectivement à 4 pour les nationales et 3 pour les départementales. Cette distinction entre l’attractivité présumée des deux réseaux découle directement de l’observation de la localisation des nouvelles constructions.
Ces mêmes observations montrent également que, quelle que soit la date considérée, au moins 20% des maisons individuelles et du bâti dense se situent dans les 50 premiers mètres autour d’une route départementale ; on en trouve au moins 50% à 200 mètres, et plus de 80% à 500 mètres. Les résultats peuvent tenir de ce constat : la considération du lien entre les réseaux de routes et l’urbanisation suggère que le rayon de calcul du potentiel de chaque cellule pourrait être réduit à 500 mètres. Sur le plan théorique, la réduction de cette fenêtre réduit également les possibilités de recherche de contre-opportunités et d’éventuels modèles de substitution. A masses égales, les résultats obtenus avec cette nouvelle fenêtre apparaissent sensiblement différents des précédents : il n’y a plus que 29% des cellules (contre 34%) qui sont localisées exactement au bon endroit ; à 150 mètres près, par contre, ce sont 91% (contre 75%) qui se trouvent placées au bon endroit.  Une reproduction de ces paramètres sur l’image de 1995, montre alors l’espace tel qu’il devrait apparaître en 2015, si les tendances observées entre 1975 et 1995 continuent de manière à peu près similaire avec a priori une erreur comparable (Figure 2).

Figure 2
A ce stade de la modélisation, l’image de la forme urbaine simulée est déjà relativement claire, et peut être largement commentée. Toutefois, les catégories d’occupation du sol des cellules qui sont censées évoluer ne sont pas connues : la dynamique modélisée n’est pas différenciée. La dernière étape, qui recourt aux automates cellulaires, permet de faire cette différentiation. On s’occupe alors principalement des deux catégories qui évolueront le plus massivement d’après le premier modèle – le bâti dense et les zones d’activités (respectivement +498 et +574 cellules). Pour ce faire, postulons11 dans un premier temps que les agrégats se feront par ressemblance et donc que les constructions denses se rapprocheront des constructions denses et que les constructions peu denses se rapprocheront des constructions peu denses. Comme cela a été souligné dans la première partie, les automates font alors intervenir la notion de voisinage ; or, les cellules ayant été localisées par le modèle de potentiel, leur voisinage est connu.
On pose alors que dans la limite des 3948 cellules de « bâti dense » prévues par le modèle de transition lors de la première étape, la transition des cellules indifférenciées vers des cellules de bâti dense peut se faire si : 1. La proportion de voisins dans un rayon de 2 cellules de type bâti dense est supérieure ou égale à 30% ; 2. La proportion de voisins dans un rayon de 2 cellules de type maisons individuelles  est supérieure ou égale à 30% ; 3. La proportion de voisins dans un rayon de 2 cellules de type « bâti collectif » est supérieure ou égale à 30%. Ensuite, dans la limite des 2267 cellules « zones d’activités », la transition peut se faire si : la proportion de voisins de ce type dans un rayon de 2 cellules est supérieure ou égale à 40%. Une troisième règle signifie ensuite que, dans la limite des 311 cellules de bâti collectif la transition peut se faire si : 1. La proportion de voisins de type bâti dense dans un rayon de 1 est au moins de 50% ; et 2. Le nombre de voisins de type « bâti collectif » dans un rayon de 2 est au moins de 1. Enfin, une quatrième règle fixe que, dans la limite des 761 cellules « équipements », prévues, la transition des cellules « maisons individuelles » vers des « équipements » peut se faire si la proportion de voisins de type « maison individuelle » dans un rayon circulaire de 1 est de 100%.
L’idée simulée est donc la suivante : le bâti résidentiel se positionne à proximité du bâti résidentiel existant (règle 1), et les zones d’activités à proximité des zones d’activités existantes (règle 2). A partir du moment où ces deux catégories de cellules sont localisées, elles se différencient encore : d’une part, le bâti résidentiel va se densifier et certaines cellules passeront de « bâti dense » à « bâti collectif » (règle 3) ; d’autre part, le bâti de « maisons individuelles » fortement concentré s’accompagnera plus spécifiquement d’ « équipements », dans l’idée d’une meilleure mixité (règle 4), évitant la mono-fonctionnalité produite par l’étalement urbain, dont les lotissements pavillonnaires sont un archétype. Il s’agit donc de repérer systématiquement ce type de concentrations et de les réaménager en y implantant des équipements (stades, etc.), voire des structures d’encadrement (établissements socioculturels, administrations, etc.) ou des activités (artisanat, petits commerces, etc.). Dix-sept itérations de ces quelques règles mènent à une situation de blocage, dans laquelle la majorité des cellules est affectée à une catégorie d’occupation du sol. Du fait de la complexité des règles imposées par les différents modèles, une petite minorité de cellules reste en effet indéterminée. Par contre, l’ensemble se fait en correspondance avec les résultats obtenus aux étapes précédentes : les modèles apparaissent alors véritablement complémentaires ; le premier quantifie les simulations, le deuxième les localise et le troisième les différencie. Les résultats (Figure 2) semblent quant à eux plutôt réalistes et reproduisent des configurations d’expansion spatiale classiques : l’étalement urbain prend place à proximité immédiate des réseaux routiers. Dans la première couronne, la simulation montre une accentuation des digitations le long des principaux axes ; dans la deuxième couronne, la rurbanisation s’amplifie et consolide l’importance des villages périphériques dans le processus d’urbanisation à venir.
Si l’état d’aboutissement actuel du modèle permet aujourd’hui de générer rapidement des simulations simples et opérationnelles quant à l’avenir des territoires urbains, il ne répond pas encore spécifiquement à toutes les questions que se posent les urbanistes et les aménageurs : le vieillissement des zones résidentielles et d’activités n’est pas directement pris en compte, la distance au centre n’intervient pas en tant que telle, le modèle tourne en système clos, et tient peu compte d’évolutions qui pourraient provenir de son espace régional (autres villes par exemple), etc. De ce fait, la démarche mise en œuvre fait l’objet d’évolutions et de développement constants. Deux voies orientent ces recherches en cours.  Dans un premier  temps, il s’agit de compléter l’outil par des indicateurs nouveaux permettant d’évaluer la pertinence des différents scénarios produits, notamment en termes de coûts (coût social, environnemental, financier, etc.) des différentes options envisagées. Ensuite, il s’agit de se servir du modèle comme d’un outil de suivi pour évaluer la conformité entre la réalité observée sur le terrain et les prérogatives du scénario de développement retenu. Le cas échéant, le modèle permettrait alors d’asseoir une démarche itérative de planification, visant à minimiser les écarts constatés. Ainsi, le modèle offre une réelle visibilité rétrospective (il permet de mesurer ce qui s’est passé) et prospective (il permet d’extrapoler ces tendances et/ou de les modifier pour qu’elles correspondent à une volonté) des territoires urbains : « Que risque-t-il de se passer, et à quelle condition ? ». De facto, il s’agit donc véritablement d’un outil de réflexion accompagnant les décisions d’aménagement, et permettant d’en évaluer les conséquences. Au service des élus et du monde politique, il ne peut se dispenser d’un travail collaboratif organisé autour de débats participatifs.
