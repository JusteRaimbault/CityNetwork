Cartographers and city planners have long expressed their desire to represent cities in more than just a nadir view. A quick browse through many old maps often turns up examples where a city is represented in both plan form and a view that can be more easily related to by the average human. Many of these old city maps have key houses and public buildings shown in an artistic ‘oblique’ view around the edges of the main map, whilst others attempted to represent the whole city obliquely with the sides of buildings depicted, along with key features, animals and even people (see Figure 1.) Meanwhile, architects and planners have long used physical models to represent their plans for changes to an area, from Alexander the Great (Arrian Anabasis III.2, Translated by E.I. Robson 1929) through to Sir Christopher Wren and his intricate models of St. Paul’s Cathedral (which can still be seen inside the Cathedral itself)

Figure 1 : London circa 1553 showing the detail of the area around Bishops gate (now the financial district of London).
So trying to represent cities in 3-D is nothing new. Like many things in modern society though, people are now using digital techniques to carry out this work. This is partly motivated by the availability of the technology and partly fuelled by public expectations. In the last decade in particular, there has been a slow but steady shift in the way that the public expect to visualise and query digital data. Digital media and the computer games industry are largely responsible for fuelling this demand. The result is that in many instances, two-dimensional maps no longer suffice when communicating ‘spatial’ ideas to an audience. For the creators of these models / views / animations, a choice often had to be made between using Computer Aided Design (CAD) or Geographical Information Systems (GIS) as their preferred modelling environment. Whilst 3-D GIS has been available for a long time, the superior rendering, compact file sizes and more intuitive user interfaces has usually meant that CAD packages (like AutoCAD and 3-D Studio Max) emerge as the model builder’s preferred environment. What is more, any GIS derived data that end up in the CAD package is often stripped of its cartographic x, y, z data (the very essence of ‘GIS data’) and replaced with a CAD based coordinate system, where the y axis instead of the z axis often represents height and the spatial ‘origin’ (i.e. x=0, y=0, z=0) often has no relation to real world coordinates.
This demand for 3-D functionality has not been lost on the GIS software industry, and over the last five years there have been increasing levels of 3-D capabilities embedded within GIS packages. Meanwhile the computer games industry keep pushing 3-D visualisations to ever-increasing levels of realism, always staying several steps ahead. The authors argued in 2003 that 3-D GIS would only become truly useful when it can handle and manage not only GIS data but also CAD derived models (Hudson-Smith and Evans, 2003). This target has recently been achieved by creating a 3-D model of central London which incorporates GIS based data, CAD derived models and exploits computer games software to deliver a product that can be extremely powerful when viewed on a desktop computer, and when pushed, it can even be delivered over the Internet.
As part of a worldwide survey for the Corporation of the City of London (Batty et al, 2001) we appraised the development of digital city models developed for GIS purposes, for CAD visualizations and for the computer games industry. If we focus on London, UK, in particular, the extents and levels of detail of the models vary considerably.
Whilst there are a number of computer games based models of London that have been developed over the years, and these provide very detailed visualisations, they have limited ability to be integrated into GIS and CAD packages. As a result these are excluded from the following review.
The first is a model developed by Miller-Hare (http://www.millerhare.com/), a London based design consultancy specialising in the production of visualisations of architectural design. The development of city models is often on an ad-hoc basis with small areas developed over time on a project-by-project basis. This is certainly how the Miller-Hare model of London evolved at first, which has been developed out of their surveys over the last fifteen years. As technology has moved on, so the model has been developed in increasing levels, and as particular projects have arisen, the model has been improved at these specific locations. These different levels of detail are shown in table 1.
Level of Detail
Description
A
Detailed architectural model including fenestration
B
Detail equivalent to 1:100 measured building survey
C
Detailed elevation
D
Major details of building elevation
E
Accurate building volume
F
Roofscape
G
Prismatic block models – coarse massing model
Table 1 : Level of detail in 3-d city models
The majority of the buildings in the Miller-Hare model are represented as basic blocks with building footprints extruded by a height derived from aerial photogrammetry. Extruded footprints provide a basic prismatic model at level G, while upgrading of the model on a project-by-project basis has resulted in a number of sites at a levels A and B. Figure 2 illustrates a view of the model.

Figure 2 : The Miller Hare model
Similar in nature to the Miller-Hare model is a photogrammetric model developed by London’s City University who were commissioned in 1996 by a commercial property company (Trafalgar House). The City University model has a large spatial coverage of detailed buildings, (although on the local scale, the more refined parts of the Miller-Hare model offer a greater level of detail). The model was produced using Bentley Microstation, a high end CAD package. It includes a high level of roof morphology detail, and was constructed over 3 person months at an estimated cost of 75000 € (£50000). The model has been used to visualize a number of projects in the City including the proposed Millennium Tower for Foster and Partners. Figure 3 illustrates part of the City University model with a view over Tower Bridge.

Figure 3 : The City University/Trafalgar House model
The only other extensive model that we know about is that developed by Kohn Pedersen Fox Associates (KPF), (http://www.kpf.com/). This model was commissioned from WS Atkins in a number of stages and the coverage is extensive and expanding. The model now covers all of the City of London, a large portion of the West End and the South Bank from Vauxhall to London Bridge and scattered fragments such as Paddington Basin, Baker Street, Canary Wharf and the Millennium Dome (Figure 4). The model is built on the OS datum (KPF built it on top of OS digital data) and they claim that their accuracy exceeds that of the OS data. The heights are derived from aerial photos using photogrammetry.
The model is structured in three parts - the buildings, which are modelled as blocks, generally with flat tops but significant roof pitches. The buildings extend to 3m below the lowest adjacent ground level. The Digital Terrain Model (DTM) is triangulated from the photogrammetry. Finally there are a set of 'pavement islands', which define the boundary between road and pavement. In selected areas the 'pavement islands' are used to locally lift up the pavements by 200mm out of the DTM, to give the impression of the pavement and kerb edge.

Figure 4 : The KPF model showing the model extent (top) and some of the detail (below)
KPF were so concerned, for legal purposes, that the heights should be correct that they commissioned a report from UCL to verify the model. Here at UCL we took the KPF model and compared it to a LiDAR (Light Detection and Ranging System) data set. LiDAR is a relatively new remote sensing technique that is revolutionising topographic terrain mapping. It is a very rich data source, and in this case consisted of x, y, z points at 1 metre intervals. The comparison was not altogether easy, first there were gross errors where buildings had been demolished or built (the data captures were about a year apart). Secondly there were subtleties; where air conditioning units and plant blocks on top of buildings distorted the recorded heights. However KPF and UCL eventually agreed that the model was correct to within ±1 metre vertically.
The major failing of the three-dimensional models discussed so far, is that they do not link to any significant secondary data sets. They are purely 3-D representations of the buildings and terrain of London, and all of them are primarily used in a CAD environment. They are incredibly useful in this sense, but if this data was brought into a 2-D GIS then it is rather like being allowed to use a basemap of buildings but then not being able to add any further layers of information, (for example, traffic flows, or population density). In addition, these models are all owned and used in restricted circumstances by architectural or property companies. The models are primarily used to support the planning permission applications for large budget new buildings, and rarely to look at the overall impact of a policy, or to identify neighbourhoods in the city, which stand to gain or lose from future changes. With all of this in mind, The Greater London Authority (GLA) commissioned ourselves (CASA) to develop a model that could be used by the GLA in the future, and which would handle GIS data with ease.
Virtual London is a partnership between the Greater London Authority (GLA), CASA as the contractors, British Telecom, London Connects and the Corporation of London, under a central government initiative known loosely as ‘e-Democracy’. The goal of this initiative is to increase levels of participation by citizens in the democratic process at a local and regional level, and to test the role of advanced technologies in achieving this. The idea is to explore a range of innovative tools, which will stimulate participation in democratic processes, an essential component in creating a sense of citizenship. It examines the role new technology plays in connecting the public-at-large to policy makers, and whether new technology can help engage parts of the community who for social, economic or cultural reasons do not take part in traditional forms of public consultation.
The extent of the model was set to cover an area of 20 km2, centred approximately on St Paul’s Cathedral. The first stage of the project involved developing an accurate block model of building heights. To achieve this we used a range of data sources including OS MasterMap data (www.ordnancesurvey.co.uk) and LiDAR from Infoterra Plc (www.infoterra-global.com/). The LiDAR data was filtered in ESRI ArcGIS 9.0 using a spatial join between the building polygons and the LiDAR points. This allowed us to calculate a range of height statistics for each building block (maximum, minimum, average and modal). Each building was uniquely identifiable by its ‘Topological Object Identifier’ or TOID (a unique reference system used by the Ordnance Survey). Using these identifiers, key buildings (such as St. Paul’s Cathedral) could be removed if a more detailed data source existed for that particular building (for example a CAD model).
Following this a terrain model was developed, again using LiDAR, (minus the buildings and other man made structures), supplemented with OS spot heights, and finally ‘flat features’ of a fixed height (for example the river Thames, canals, docks and other water bodies) were forced to a fixed average height (derived from LiDAR) in order to represent these water bodies as horizontal features. An aerial photograph was then applied to this surface, and the buildings were added to give the first stage of the model.
Following this, the attention was turned to modelling key London landmark buildings, along with the buildings along the edge of the river Thames closest to the GLA headquarters (City Hall). For much of this work a software package used by computer games companies was utilised, namely RealViz Imagemodeler. This package allowed us to use a number of photographs of each building to accurately calibrate the 3-D space that the photos covered. Once the photos were calibrated, an element of scale could then be added to the 3-D space by surveying one known distance, (for example the length of one face of the building). The modelling could begin, usually simply using blocks and cubes that could be shaped, warped and moulded to create the building shape (rather like ‘digital clay’), using the photos as a guide to the modelling process. Once the building shape had been achieved, then the same photos can be used to texture map the facades of the building. The final product could then be exported and correctly positioned within the OS spatial framework. Each building took 2-3 person days to construct and georeference, depending upon the level of detail that was required.

Figure 5 : Fishmongers Hall next to London Bridge – one of the landmark buildings being modelled in RealViz Imagemodeler
Other buildings were modelled more rapidly using 3-D ‘sketching’ software from a company who make a product called SketchUp (www.sketchup.com) and these models were refined using CAD software such as 3-D Studio Max.
This allowed us to begin to integrate into the GIS block model, these CAD derived models. These were an improvement on the original block structure that had previously represented that particular building block, but were always fitted to the exact location that the previous block had occupied. One new development that has arisen through the collaboration of CAD companies like SketchUp and GIS companies such as ESRI, are tools for quickly exporting basic blocks, adding roof morphology (or whatever detail is required), and then re-importing the refined building into the GIS without losing the correct spatial reference. This ‘blurring of the boundaries’ between CAD and GIS is a development that has allowed us to create these detailed sections of the overall model far more rapidly than was ever possible in the past.
Another change that has made 3-D GIS a reality when it comes to developing city models is the ability to store complex 3-D objects (rather than extruded 2.5-D objects such as a building footprint) within the GIS. Until recently, much of what claimed to be ‘3-D GIS’ was actually upon closer inspection 2.5-D (2-D features extruded for visualisation purposes, and of very little use when attempting to carry out analysis). ESRI have addressed this shortfall by developing a new form of feature called ‘Multipatch’ which can be stored within the ESRI geodatabase alongside any other spatial feature. In the past complex features were either stored as a Triangulated Irregular Network (TIN) which had its own limitations: no two x, y pairs were able to have different z values (i.e. you can never store a vertical feature such as a wall without breaking the topological rules) or as 2.5-D extruded features (extruded footprints) or as external references to CAD files. Now it is possible to store complex 3-D geometry as a multipatch object, along with the associated tables for storing relationships and topology within the geodatabase along with associated attribute data. This is how all the complex CAD derived buildings are stored in Virtual London.
One of the ‘by-products’ of using photogrammetric techniques is that the position and lens type of each camera used to create the 3-D space is generated in x, y, z coordinates. By importing these into the overall model, it was possible to check the views from those exact camera locations, generated from within the model, alongside the photos themselves, and this was used as a qualitative method for checking the basic model from a number of key view points. Another check that was carried out was to use the building heights that had been derived photogrammetrically, to compare with the height that the LiDAR data had generated for the same building. In most cases (in particular the buildings with flat roofs) these heights rarely varied more than +/-1 metre. The location of buildings on the x, y plane was considered to be highly accurate since it used the revised Ordnance Survey framework for the precise positioning.

Figure 6 : A general overview of the model (top) and photo (left), which can be compared to the camera matched image of the model generated from the same location (right)
In parallel to this work, CASA staff also spent some time collecting panoramic images of key locations. These images were collected using standard techniques for 3 rows of 12 photographs. These were then merged into a single shot using RealViz Stitcher software. By wrapping the resulting image onto the surface of a sphere, a set of 360° x 180° panoramas were created. These sphere were then located in the correct x, y, z space within the 3-D model to provide ‘bubbles of reality’ where the user could navigate to and then, by moving inside the sphere, a panoramic view of the city is seen from that location. This also provides a means for checking height data and roof morphology. Although panoramas provide an essentially low-end method of gaining a sense of the city, they are critical in Virtual London for they provide a unique view of the city compared to a purely 3-D approach which has dominated earlier versions of such models for other large cities (Batty, et al, 2001).
Total costs for developing the model were approximately 42000 € (£30000).
The first people to see the Virtual London work were clearly impressed by the geometric detail of the buildings and the terrain, but like previous city models, most people focused upon the architectural representation and had little concept about adding other layers to the model.  In light of this we started to develop new layers of information that could be integrated into the model and to get people thinking in a more abstract way about what the model could be used for. At first we highlighted particular buildings, for example all those buildings that were new in the last 10 years or less. Then in response to the fact that some people found it hard to gain their bearings, we added administrative boundaries to show where each local government boundary started and ended.

Figure 7 : Buildings less than 10 years old (top) can be identified in the model (red are the most recent, through to green as 10 years old (grey blocks are > 10 years old). Above, shows administrative boundaries within the model, which proved useful for some people.
Another data set that proved interesting was an index of the vitality of the commercial success of parts of the city. This data set (the product of another CASA contract) was floated in over the model in order to be able to visualise the vitality of ‘town centres’ alongside the physical geometry of the model. This work then inspired more ideas; a rise of sea level of 5 metres was developed to show the extent of flooding in the event of global warming was added.
In collaboration with the Environmental Research Group (Kings College London), an air pollution data set was added representing Nitrogen oxide (NOx). This data layer uses all of the emissions sources in London, but this is clearly dominated by road traffic. The data is processed to calculate an annual average daily total for the pollutant. Vehicle speeds, which make up part of the equation for the NOx model, are calculated from a moving car. Vehicle age profiles are taken from the UK national stock model. The emissions toolkit uses a dispersion model and is validated using the extensive measurement database of the London Air Quality Network (see the figures below). For more information about the pollution data go tohttp://www.london.gov.uk/mayor/environment/air_quality/research/index.jsp

Figure 8 : Index of ‘Town Centres’ shows in red the areas that are most commercially productive whilst in orange and yellow through to green are the areas that have lower commercial returns.

Figure 9 : Sea level changes can be simulated within the model

Figure 10 : Nitrogen oxide (NOx) air pollution (annual average predicted for 2005) simulated as a layer within the model. This pollutant, often nicknamed ‘urban smog’is largely derived from vehicle emissions. Red shows higher levels whilst blue represents the lower levels (data courtesy of Environmental Research Group, Kings College London)
Whilst many people would like to access the full Virtual London model, the nature of the file size, required software and data copyright issues mean that this is not currently practical at a fully un-restricted level (i.e. open Internet access). Various suggestions have been made to circumvent this, ranging from allowing restricted access to the model at key local government locations, to developing a touch-screen information ‘Pod’ version of the model that the public can use at various public locations. Software products like Google’s new ‘Keyhole’ product show some potential for locking the data up whilst providing un-restricted public access in the future.
At CASA we feel that it is important to deliver at least part of this material across the web, and so we have experimented with software used by the computer games industry to deliver an interactive way of exploring parts of the model online.
The Internet (for the time being) restricts the level of detail that we can show but it massively enhances the range of communication that is available and the number of potential viewers. We developed a prototype that then evolved into a more robust interface using virtual world software based on Adobe Atmosphere and Viewpoint. This lets us make a digital 3-D space from the virtual city model itself or make a different ‘world’, for example, an exhibition space where we can place the model as an ‘exhibit’. The key advantage is that whatever is in the virtual world is seen from the vantage point of the user as an ‘avatar’ or digital representation of themselves with the users collectively or individually manipulating the objects that comprise this world.
What we found when we produced our world-wide surveys of virtual cities for the Corporation of London in 2000 (Batty et al, 2001) was that many users and clients were uncomfortable with the new digital media. They wanted some form of tangibility, some link to the material world, which is the subject of their interest. The Corporation of London, for example, postponed their decision to acquire virtual city models despite demands by its various departments due to the fact that their physical model was regarded as forming an important medium for discussion and negotiation in ways they considered a digital replacement could not. A half-way house is to use the digital model in a context that makes it appear in material form. By producing a simulation of a place where negotiation and discussion of planning information can occur and by putting the digital model into the place as an artefact, which can be manipulated by those in that place, a digital rendition of the traditional forum, or ‘simulacra’ in Baudrillard’s (1994) terms, can be emulated. We show such a simulation in Figure 11 where we see the digital model placed on a virtual table with avatars around it, manipulating buildings. This is a fairly experimental context but has so far proved very popular with both adults and children alike.
However, it is not just an interface that is required to enable people to use Virtual London in a more hands on way. We have demonstrated that a small number of basic data layers have proved to be very popular, but there are many other layers of information that could be integrated relatively rapidly. Once such layer would be to tag historical information to the model, for example when a building was built, and who the architect was. This would allow a historical view of the architecture when dealing with the context of an area, and this is of particular importance when buildings are shown without image textures or detailed architectural elements modelled in, since a relatively ornate Georgian building might appear almost identical to a 1960’s concrete block, if both of them have a similar topographic footprint, and are extruded to a similar height.
Other information could be tagged to the buildings. For example the floorspace, the company names, the energy use, the number of floors (to assist emergency services when responding to an incident) and so on. Land use would be another key layer to add. Traffic levels, noise levels, and the location of transport hubs would be of interest to many people. Socio-economic data is another layer that could be useful to city planners and the public alike. And of course, these layers don’t all have to be restricted to things that happen ‘above ground’. The London Tube network could be added beneath the model, as could utility company information such as sewage, telephone, gas and other networks, which would open up opportunities for using the model for ‘augmented reality’ purposes for example for seeing where these ‘buried features’ exist.
We have also experimented fairly successfully with using the model to carry out shadow analysis, since the model exists in real world coordinates, it is relatively simple to simulate a solar light source and get this position accurate for a particular date and time of day. This can be particularly important when demonstrating the shadows that are cast by a proposed tall building over the surrounding area. In a similar way, viewshed analysis is relatively straightforward, although it is computationally intensive to carry out in such a detailed model. Thus simple questions like, ‘what can be seen from here’, and ‘will this building obstruct views of other important buildings’ can be carried out.

Figure 11 : Avatars within the Adobe Atmosphere gallery space, looking at a reduced resolution version of part of the model. People operating avatars can express views, press buttons to see a new (proposed) building appear
The demand for ‘true 3-D’ representations of our cities is growing, partly fuelled by public expectations, and partly fuelled by the complexity of the ‘reality’ that we are now capable of modelling. In a busy vibrant city like London, the ‘z’ (height) is as important as the ‘x, y’ (easting, northing), and in some cases it is more important. Over the last two decades, the computer games industry, architecture firms and CAD software technology have led the way in this field. GIS software often claimed to have 3-D capabilities, but usually only delivered 2.5-D solutions. However, recent developments have meant that 3-D GIS software is now really set to compete in this arena.
The division between CAD and GIS is also starting to blur, with improved exchanges of data between the two. AutoDesk have been very keen to draw attention to this, and building a bridge between CAD and GIS is key to Autodesk’s current product development strategy. Recent collaborations between companies such as ESRI and SketchUp have demonstrated how this integration can improve productivity for both the CAD and the GIS company.
Local and central government are also becoming aware that this technology is available and no longer prohibitively expensive. What is more, they are aware that the architectural companies are able to assess their own planning applications within their own city models (for example, for shadow analysis), but that the planning authority (i.e. government) has no way of verifying these ‘results’. Through this, the GLA within London have generated a huge amount of interest in the potential of Virtual London with other governing bodies in London (Camden Council, London Development Agency) all keen to become involved in the future.
To date, the public have received the work with enthusiasm in those situations where we have been able to demonstrate the model. Many people have said that they find it so much easier to understand than a (2-D) map. This has led us to observe on several occasions that a far higher proportion of the population are ‘2-D illiterate’ or find it very difficult to read maps and plans. However, the increased level of complexity that comes with the 3-D representation of data within these models can sometimes be distracting and unnecessary when 2-D representations would suffice. There are also a number of issues that need to be solved surrounding the copyright of data before these types of models could be delivered freely to the public over the Internet.
If we set aside the problems of data copyright and bandwidth, then we can envisage an interface in the future that allows users to log on and navigate to particular parts of the city or the digital space in order to discuss or view a simulation of a particular issue, be it the shadows cast by a proposed tall building, or the levels of air pollution on a particular street. The focus is no longer on getting the geometry right and building a single virtual city but on developing many different variants which can be tailored to various purposes and delivered to a diversity of users in different ways using different media. 3-D GIS has a key role to play here, since it is currently the easiest way to allow lots of diverse data layers to be incorporated when required. But the solution must be delivered in a way that goes beyond the technical nature of GIS and is both effective and relevant: to develop virtual cities in which the preferences and abilities of the users with respect to their interests in the problem in hand, can best augmented through these virtual realities.
The days of 2-D GIS are far from over, but in an increasingly complex world the 2-D thematic map that has been the core of GIS analysis for a decade and more, may have to start to evolve if it is to survive.
