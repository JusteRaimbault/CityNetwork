Depuis tout temps, l’homme sait que, dans le monde qui l’entoure, les phénomènes évoluent aussi bien dans l’espace que dans le temps. Très rapidement, il s’est d’ailleurs rendu compte, notamment en observant le ciel et les successions « nuits – jours », que ces évolutions sont interdépendantes puisque le « mouvement » (en réalité apparent) des éléments célestes se fait de manière régulière dans le temps…
Par ailleurs, l’homme a également ressenti très tôt le besoin de représenter sur carte les divers éléments constituant son environnement. Avec l’avènement des technologies informatiques est apparue la volonté de gérer ces éléments (de plus en plus précis mais aussi de plus en plus volumineux) dans des bases de données. Cependant, l’information géographique est quelque peu particulière dans le sens où, d’une part, elle fait référence à des caractéristiques à la fois spatiales et attributaires, et d’autre part, elle peut être représentée en modes vecteur ou image. Les bases de données géographiques et les différents systèmes qui les gèrent ont malgré tout réussi à « s’adapter » pour permettre une prise en compte de cette double spécificité.
Mais un autre défi pointe aujourd’hui à l’horizon. En effet, en voyant la croissance actuelle des capacités informatiques, les utilisateurs d’informations géographiques montrent de plus en plus leur volonté de voir intégrer la dimension temporelle dans leurs jeux de données afin de pouvoir enrichir leur analyse spatiale par une « étude évolutive » des phénomènes considérés. En d’autres termes, c’est la notion d’historicité de l’information géographique qui semble de plus en plus intéresser les acteurs du domaine. On cherche ainsi à « restituer » l’historique des phénomènes afin de mieux comprendre ce qui s’est produit dans le passé et, éventuellement, anticiper certaines évolutions futures. Les recherches actuelles portant sur les « lignes de vie » (Event-History Analysis – [Thériault et al. 2002], [Claramunt 2003]) vont d’ailleurs entièrement dans ce sens puisque leur principal objectif est de proposer une « reconstitution » de l’évolution d’un phénomène comme une succession d’« épisodes » et d’« évènements »… Le problème aujourd’hui posé peut donc se décliner de la manière suivante :
il faudrait assurer, au sein des bases de données géographiques, une gestion simultanée des dimensions spatiale et temporelle. Cela passe par une réflexion sur la modélisation et le stockage des informations correspondantes ;
il faudrait que les « éléments temporels » (identifiants, attributs, métadonnées,…) stockés dans ces bases de données traduisent le plus fidèlement possible l’évolution réelle qu’ont connue les entités considérées. Cela passe notamment par la mise sur pieds de stratégies de mise à jour adaptées au mieux à la réalité du terrain ;
il faudrait que ce « nouveau type d’informations » puisse être utilisé adéquatement par les utilisateurs. Cela passe par une nouvelle manière de concevoir les interrogations de la base de données (ex. : introduction des « requêtes spatio-temporelles »).
Dans la suite du texte, nous découvrirons tout d’abord le contexte spatio-temporel dans lequel nous serons amenés à travailler en présentant quelques concepts théoriques nécessaires à une bonne compréhension de cette « nouvelle approche », notamment dans le domaine des SIG. Nous passerons ensuite en revue les différents formalismes (avec leurs éventuelles extensions) qui ont été proposés pour modéliser, d’une part, de manière conceptuelle les entités spatiales, et d’autre part, les évolutions que celles-ci peuvent connaître tout au long de leur existence. Nous pourrons alors aborder la problématique de la mise à jour de ces entités qui constitue le véritable fondement de la gestion de leur historicité au sein d’une base de données spatiales. Nous discuterons finalement du problème des requêtes spatio-temporelles en présentant deux approches, reliées respectivement aux représentations en modes vecteur et image.
Comme nous l’avons introduit au paragraphe précédent, on se rend compte que l’intégration de la dimension temporelle dans les bases de données constitue aujourd’hui un réel challenge. Cependant, le temps étant un phénomène continu, celui-ci ne peut être directement implémenté sous format informatique au vu des « limitations physiques » de ce stockage. Ainsi, si on souhaite introduire l’aspect temporel dans une base de données, il sera nécessaire de le « discrétiser ». Pour ce faire, on propose de définir au préalable deux notions fondamentales :
l’intervalle temporel ([Claramunt et al. 1994]) : il s’agit de la durée d’observation du phénomène étudié. Cette durée peut être relativement variable : on fera, par exemple, une observation sur une journée pour étudier les activités humaines quotidiennes tandis qu’on devra travailler sur plusieurs centaines de millions d’années pour des études géologiques ;
la résolution temporelle ([Raynal 1997]) : il s’agit de la « granularité » avec laquelle on percevra l’évolution du phénomène au cours du temps au sein de la base de données. Elle sera par exemple d’une heure pour l’étude des activités humaines quotidiennes et de cent millions d’années pour une étude menée en géologie.
Sur base de ces deux concepts, on peut définir trois possibilités d’évolution des phénomènes ([Claramunt et al. 1994], [Renolen 2000], [Hornsby & Egenhofer 2000],…) :
- l’évolution discrète : l’intervalle temporel (la durée d’observation du phénomène) est inférieur à la résolution temporelle (la granularité de cette observation). On va donc associer l’évolution du phénomène à des instants précis dans le temps même si, en réalité, ce dernier connaît une « évolution constante ».
Exemple : Pour le calcul des précipitations, on relève le total des pluies tombées en une journée (résolution temporelle) alors que la durée d’une averse (intervalle temporel) est généralement beaucoup plus courte.
Figure 1
l’évolution continue : l’intervalle temporel est ici supérieur à la résolution temporelle et on peut percevoir l’évolution du phénomène de manière continue dans le temps.
Exemple : Le débit d’une rivière suit une évolution de type continue puisque l’observation de ce phénomène, d’une durée généralement longue (intervalle temporel), est réalisée à l’aide de mesures « ponctuelles » dans le temps (résolution temporelle).
Figure 2
- l’évolution par paliers : on a ici affaire à un cas plus particulier où le phénomène évolue suite à des évènements (ou stimuli) qui l’affectent. Il s’agit, en réalité, de phénomènes de « nature statique », mais qui évoluent au cours du temps suite à des évènements qui, par ailleurs, peuvent être « ponctuels » ou « avec durée ».
Exemples : (1) La propriété d’une parcelle est un phénomène de nature statique mais qui peut évoluer suite à un évènement qualifié de « ponctuel » (ex. : vente foncière). (2) L’état d’une route est également un phénomène statique mais qui évoluera ici plutôt suite à des évènements dits « avec durée » (ex. : travaux de rénovation).
Figure 3
On cherche de plus en plus à intégrer cette notion d’évolution dans l’analyse temporelle des phénomènes étudiés (cf. travaux de l’équipe de Badard à l’IGN France). Pour ce faire, il est indispensable de trouver des solutions qui permettront d’assurer une gestion optimale de ce « nouvel aspect ». On retrouve, à la base de cette gestion temporelle de l’information, les méthodes de mise à jour dont l’objectif est de faire passer un phénomène considéré de son état « ancien » à son état « nouveau ». Deux possibilités peuvent être envisagées à ce propos :
la première consiste à effectuer la mise à jour à intervalles réguliers. Après un certain laps de temps (généralement identique), on décide de faire une observation totale du territoire analysé et de noter par la suite quelles sont les conséquences des évènements survenus… Ce type de procédures sera préféré, par exemple, pour la mise à jour de la base de données d’un Institut Géographique National (généralement réalisée par lever photogrammétrique) car cette dernière n’a pas d’obligation à être tenue à jour « en temps réel » ;
la seconde consiste à mettre à jour les données « en réponse à un changement observé ». Ainsi, dès qu’il y a évolution, on effectue une mise à jour. Cette solution sera préférée lorsque le phénomène étudié n’est pas soumis à des changements « répétitifs ». On utilisera notamment ce type de procédures pour des bases de données spatiales qui se doivent d’être à jour pour assurer l’efficacité des applications auxquelles elles sont reliées (ex. : nécessité, pour un service d’urgence, de connaître « en temps réel » l’évolution des encombrements de circulation sur le réseau routier afin d’assurer un acheminement le plus rapidement possible des véhicules de secours sur les lieux des accidents).
Comme nous l’avons vu, l’information géographique possède certaines caractéristiques qui la différencient des informations traditionnelles (cf. § 1). Toutefois, des méthodes de mise à jour ont déjà été développées dans ce domaine, souvent pour répondre à un besoin bien spécifique (ex. : mise à jour du cadastre lors d’une détection de changements a priori [Spéry 1999])… Ces méthodes sont encore régulièrement basées sur un modèle relationnel classique et donc sur la distinction entre la spatialité et la thématique de cette information géographique. Il faut cependant se rappeler que les entités évoluent de manière simultanée sur ces deux niveaux et que la cause d’une évolution attributaire s’avère souvent être une évolution d’ordre spatial. L’approche orientée-objet apparaît comme la solution idéale pour solutionner ce problème de cohérence temporelle des données spatiales, mais, comme nous le verrons dans la suite, il lui reste encore à « faire ses preuves » dans les applications pratiques…
L’« association » de l’espace et du temps constitue un problème pour de nombreuses disciplines cherchant à étudier l’évolution de phénomènes dans les continuums spatiaux et temporels. Ainsi, il n’est pas rare qu’en sciences démographiques par exemple, l’analyse d’une problématique soit rendue plus aisée grâce à la constitution d’une succession de cartes assurant une visualisation de l’évolution du phénomène sur le territoire considéré. Nous pouvons notamment citer ici les travaux portant sur la « temporalité urbaine » ([Pumain & Lepetit 1993], [Bertagnole et al. 1998],…) où l’un des objectifs est d’illustrer, à l’aide d’une « série chronologique » de cartes, la manière dont croît et se propage une population… Plus récemment, d’autres recherches ont poussé plus loin encore l’analyse de la dynamique spatiale, en se penchant notamment sur les évolutions paysagère ([Poix & Michelin 2000], [Badariotti et al. 2002],…) et urbaine ([Thériault et al. 2002]). L’objectif principal est ici d’identifier les principaux facteurs influençant ces phénomènes de manière à les modéliser et, si possible, à identifier leurs évolutions probables (l’anticipation de certaines modifications peut en effet être considérée comme un véritable « outil d’aide à la décision »…).
On se rend compte par ailleurs, dans nombre de recherches, de l’intérêt de pouvoir intégrer et gérer l’ensemble de ces informations spatio-temporelles au sein d’une seule base de données, afin de profiter de toute la souplesse qui est offerte par ce type de stockage d’informations. C’est d’ailleurs dans cette direction que semblent aujourd’hui aller la majorité des Systèmes d’Information Géographique (SIG). À l’origine, ces derniers ont été constitués pour pouvoir intégrer, dans des systèmes informatiques spécifiques, la notion d’espace comme une variable permettant une analyse spatiale des divers phénomènes qui nous entourent. Afin de dépasser ce cadre définissant en réalité un « simple état statique observable » ([Cheylan et al. 1994]), on cherche aujourd’hui de plus en plus à y associer la « variable temporelle » de manière à pouvoir utiliser cette dernière pour une analyse évolutive des phénomènes spatiaux.
Toutefois, lorsqu’on considère une telle évolution, il est fondamental de se re-situer dans son contexte historique ainsi que d’identifier a priori à quel(s) « type(s) de temps » elle se réfère. Snodgrass & Ahn (1985) proposent ainsi une distinction chronologique en parlant :
de temps réel : il s’agit de l’époque à laquelle se déroule un évènement considéré ;
de temps d’observation : c’est l’instant d’observation du résultat de cet évènement ;
de temps d’enregistrement : il correspond au moment où ce résultat est stocké au sein de la base de données créée au préalable pour l’analyse du phénomène étudié.
Les écarts existant entre ces types de temps sont à la source de ce qu’on pourrait appeler le « décalage temporel » de la base de données. En effet, s’il existe un écart important entre les temps réel et d’enregistrement par exemple, une étude menée sur base de ces observations obsolètes risque vite de ne plus avoir beaucoup d’intérêt…
Par ailleurs, ces différents types de temps font référence à des concepts distincts au niveau de l’entité spatiale considérée. En effet, comme le fait remarquer Caron (1991) :
le temps réel (qui est également appelé « temps effectif ») fait référence à la réalité des évènements affectant l’entité et donc à la naissance, l’évolution et la mort de celle-ci ;
le temps d’enregistrement (également qualifié de « temps base de données ») est, quant à lui, plutôt relié à l’évolution à la fois spatiale et thématique de l’entité dans la base de données et donc aux entrées, modifications et sorties de ses occurrences dans celle-ci.
Ainsi, une maison construite le 16 février (date de sa « naissance » – lien avec le temps réel) peut ne voir l’entrée de son occurrence dans la base de données du gestionnaire du territoire (lien avec le temps d’enregistrement) que le 21 du même mois, par exemple.
Pour pouvoir implémenter des observations faites sur un phénomène, de quelque nature qu’il soit, au sein d’une base de données, il est nécessaire de passer par une étape préalable qui est la modélisation. On peut définir un modèle comme étant « une représentation formelle d’un phénomène du monde réel à un certain degré d’approximation » ([Renolen 2000]). Dans une base de données, le modèle aura pour objectif de représenter de manière graphique l’ensemble des données et les interactions entre celles-ci. Pour la constitution d’une base de données, il est indispensable de construire a priori ce qu’on appelle le Modèle Conceptuel de Données (ou MCD). Si, par ailleurs, cette base de données est de nature spatio-temporelle, il peut être également intéressant de réfléchir au Modèle d’Évolution afin de rendre compte des diverses conséquences des changements affectant les entités sur le territoire considéré.
Le modèle conceptuel correspond à la description de la structure des données sans souci de leur implémentation en machine. Traditionnellement, on réalise un tel modèle au départ du formalisme « Entité – Relation » (E-R) proposé par Chen (1976). Ce dernier nous permet de représenter les entités accompagnées de leurs attributs (sous forme de « boîtes ») ainsi que les relations qui existent entre elles (sous forme de liens). Malheureusement, un tel formalisme ne permet pas, à lui seul, de prendre en considération l’évolution des phénomènes étudiés et il ne pourra donc être utilisé directement en guise de « fondement » à une gestion d’historicité. Certains auteurs ont toutefois réfléchi à l’intégration de ce nouvel aspect au sein des bases de données relationnelles en proposant diverses extensions pour des formalismes préexistants. Nous pouvons notamment citer ici :
le modèle E-R-T (Entity-Relationship with Time) proposé par McBrienet al. (1992). Dans ce modèle, les éléments peuvent être annotés par un T, signifiant que l’entité ou la relation existe uniquement pour un instant ou un intervalle donné (ex. : un pays peut être une monarchie ou une république suivant l’époque considérée), ou par un H, traduisant le fait que l’entité ou la relation existe dans une perspective historique (ex. : relation « de sang » entre un petit-fils et son grand-père) ;
le formalisme MODUL-R présenté par Caronet al. (1993). On définit ici un module dit « référentiel » qui fournira les outils de modélisation tant au niveau spatial que temporel. Parmi ceux-ci, on retrouve des pictogrammes déterminant le mode d’implantation spatial des entités (simple, composé ou complexe) ainsi que la manière dont sont considérés les phénomènes en terme de « perception temporelle » (point temporel, intervalle temporel, ou encore les deux dans un pictogramme « complexe » ou « alternatif ») ;

Figure 4
l’extension temporelle ([Sheeren 1999]) du formalisme CONGOO ([Pantazis 1994]). On propose également ici l’utilisation d’un pictogramme () pour illustrer la dépendance temporelle des entités au niveau de leurs caractéristiques spatiales et attributaires ainsi que des relations (topologiques et autres) existant entre elles.
Actuellement, le modèle orienté-objet est de plus en plus apprécié dans le monde des SIG vu les nombreux avantages qu’il y apporte. Par ailleurs, la notion d’objet semble plus proche de la réalité et est ainsi plus aisée à appréhender par les différents intervenants d’un même projet. En ce qui concerne la dimension temporelle des entités, le modèle orienté-objet offre plus de facilités que le modèle relationnel traditionnel car il permet d’assurer une gestion complète de l’historicité qui sera encapsulée dans l’objet. De plus, la gestion des objets temporels et non temporels sera menée sur un même pied d’égalité et les requêtes spatio-temporelles seront plus faciles à mettre en place étant donné qu’elles ne porteront plus que sur un seul objet, et non sur plusieurs tuples faisant référence à une même entité mais à des époques différentes… Tout ceci permet d’expliquer le nombre croissant de recherches actuelles dont l’objectif est de proposer une architecture spatio-temporelle basée sur une approche de type « orientée-objet » (ex. : modèle MADS [Parent et al.]). Cependant, comme le fait remarquer Gregory (2002), la majorité des systèmes pratiques qu’on peut rencontrer actuellement sont encore basés sur des solutions relationnelles qui ont, par le passé, prouvé toute leur « robustesse »…
Lorsqu’on souhaite intégrer l’aspect temporel au sein d’une base de données géographiques, il est également intéressant de modéliser l’évolution des phénomènes considérés. Pour ce faire, certains auteurs ont proposé d’utiliser, en parallèle au formalisme E-R, des diagrammes de flux qui montrent comment un phénomène évolue dans le temps grâce à certains procédés. Nous pouvons penser, par exemple, au Data Flow Diagram (ou DFD) de Gane & Sarson (1978) qui permet d’illustrer, dans un « environnement de base de données », la manière dont une entité évolue au cours du temps suite à divers procédés qui l’affectent.

Figure 5
Nous pouvons également citer la solution originale du « graphe historique » introduit par Hornsby & Egenhofer (2000). Le modèle est basé sur quatre primitives fondamentales : l’objet existant (a), l’objet non existant avec historique (b), l’objet non existant sans historique (c) et la transition entre ces trois « états » d’objets (d). Grâce à cette dernière, il est possible de construire une matrice (3x3) reprenant les neuf « transformations d’états » possibles.

Figure 6
Il existe également une nomenclature temporelle spécifique pour les six transitions qui font intervenir deux « états » différents d’un objet considéré… On retrouvera ainsi (dans l’ordre de la matrice représentée à la figure 6) : l’élimination (1), la destruction (2), la réincarnation (3), l’oubli (4), la création (5) et le rappel (6).
Il se peut également que des changements simultanés apparaissent de manière dépendante sur deux objets A et B. Dans ce cas, on aura une transition « traditionnelle » pour les deux états de A, une autre pour les deux états de B, ainsi qu’une transition qualifiée d’« objets-croisés » qui caractérisera la transformation « A→ B ». Si on reprend les neuf changements précités pour un objet et qu’on les fait interagir avec les neuf sur l’autre objet, on obtiendra donc 81 combinaisons possibles (matrice 9x9). Cependant, toutes ces combinaisons ne seront pas « imaginables » en réalité (ex. : incohérence si les objets A et B sont non existants a priori). Au final, en tenant compte de ces contraintes et des transitions « similaires » redondantes, on obtient 18 possibilités de transitions (matrice 3x6) qui caractérisent toutes les évolutions possibles entre deux objets considérés.

Figure 7
On peut également modéliser l’évolution d’entités géographiques au départ d’un formalisme orienté-objet. De nouveau, cette modélisation sera plus facile à appréhender puisque la notion d’évolution d’un objet est bien plus proche de ce qui existe dans le monde qui nous entoure. On nous propose ainsi, dans le formalisme UML (Unified Modelling Language qui permet de modéliser les bases de données selon cette approche orientée-objet) des diagrammes qui ont pour objectif une prise en considération de certains aspects dynamiques du système considéré. Ainsi, les diagrammes d’activité permettent de décrire une succession d’actions touchant un phénomène donné tandis que les diagrammes d’états-transitions vont, eux, plus se centrer sur la modélisation du cycle de vie de ces objets. Malheureusement, on se rend compte que cette approche orientée-objet est encore très peu utilisée en pratique, sans doute pour les mêmes raisons que celles exposées précédemment pour la modélisation conceptuelle…
L’évolution d’une entité au sein d’une base de données géographiques nécessite une mise à jour de cette dernière. Pour ce faire, nous devons nous recentrer sur la distinction entre les définitions attributaire et spatiale des entités. Si la première demande une simple mise à jour de leurs tuples, la seconde nécessitera un update sur leurs représentations spatiales.
On retrouve, dans la littérature, de nombreuses méthodes de mise à jour pour des attributs de type alphanumérique (cf. [Tansel et al. 1993]). Nous pouvons citer ici les trois méthodes « générales » proposées par Langran (1992) :
le table-versioning : toute nouvelle valeur d’un attribut entraîne la création d’une nouvelle version de la table considérée, version dans laquelle apparaîtra la mise à jour. La méthode est très simple à mettre en œuvre mais est également très lourde en terme de redondance d’informations étant donné le recopiage inutile des valeurs inchangées ;
l’attribute-versioning : toute nouvelle valeur de l’attribut sera ici stockée sur la même ligne que les anciennes et sera associée à son « intervalle de validité ». On aura donc des enregistrements de longueur variable suivant le nombre de mises à jour effectuées, ce qui va à l’encontre de la recherche d’un état cohérent pour la base de données puisqu’on contredit la première forme normale de dépendance fonctionnelle ([Codd 1970]) ;
le tuple-versioning : selon cette méthode, toute nouvelle valeur apportée à un attribut d’une entité entraîne la création d’un nouvel enregistrement (donc d’un nouveau tuple). Un des problèmes essentiels rencontrés ici est que la table peut vite devenir « imposante » si on a affaire à des mises à jour fréquentes. Les critères des requêtes temporelles devront par ailleurs préciser à quel(s) tuple(s) de l’entité ils se réfèrent…
Snodgrass & Ahn (1985) distinguent, quant à eux, sur base de la dimension temporelle introduite, quatre types de bases de données relationnelles, dont les trois dernières peuvent s’apparenter à la méthode du tuple-versioning présentée par Langran :
la base de données statique : la mise à jour correspond ici à un simple effacement (s’il y a suppression de l’entité) ou remplacement (s’il y a modification de l’entité) de la donnée sans aucune prise en considération de son historicité (ce qui veut dire qu’on ne saura plus retrouver la valeur ancienne après la mise à jour). Ce type de base de données n’apporte donc aucune dimension temporelle ;
la base de données rollback : on propose l’ajout d’un attribut à la table, à savoir le temps d’enregistrement. On retrouve ici une première intégration de la dimension temporelle mais on aura un risque d’imprécision s’il existe un décalage important entre les temps réel et d’enregistrement (cf. § 2.2). De plus, une correction apportée à une entité demande la création d’un nouveau tuple, et une requête portant sur la période s’étendant entre les deux époques considérées (c’est-à-dire celle du premier enregistrement et celle de la correction) donnera inévitablement un résultat erroné ;
la base de données historique : l’attribut ajouté à la table sera ici le temps réel. On n’aura donc plus qu’un seul tuple pour chacune des entités et il n’y aura pas de problème pour les requêtes puisqu’on ne considère plus le temps d’enregistrement mais bien le temps réel. Cependant, une correction sur une des entités ne laissera aucune trace de l’état précédent ;
la base de données temporelle : on ajoute à la table les deux types de temps (réel et d’enregistrement). De cette manière, on va à l’encontre des désavantages cités ci-dessus (ex. : une correction faite sur une entité laissera une « trace » dans la base de données).
Nous pouvons illustrer les différentes solutions que nous venons de présenter par l’exemple suivant ([Sheeren 1999]) :
« Soit une base de données relationnelle illustrant l’occupation du sol d’une région et qui possède une table nommée "Parcelles" caractérisée par trois attributs : le numéro cadastral, le nom du propriétaire et sa superficie. »
Imaginons qu’une observation faite sur la parcelle n° 10a en janvier 1990 nous apprenne que le nom du propriétaire est Mr Langlois et que la superficie de cette parcelle vaut 5 hectares. Considérons que les temps réel et d’enregistrement (cf. § 2.2) sont équivalents, c’est-à-dire que les autorités responsables sont informées dès qu’il y a changement de propriétaires et que la base de données du gestionnaire du territoire considéré est ainsi directement mise à jour… En janvier 1990, la table "Parcelles" pourra ainsi se représenter suivant les différents types de solutions que nous venons d’exposer :

Figure 8
Supposons qu’on apprenne, en février 1995, que Mr Dupont a racheté cette parcelle à Mr Langlois. On pourra alors représenter la situation de ces différentes manières :

Figure 9
Si on se rend compte, en avril 1995, que c’est Mr Ballot et non Mr Dupont qui a racheté la parcelle à Mr Langlois en février 1995, on aura alors :

Figure 10
Cette mise à jour fera appel à des méthodologies différentes suivant que le phénomène étudié est représenté en mode vectoriel (sous forme d’entités) ou en mode image (grille de pixels)… Ici aussi, diverses solutions peuvent être envisagées :
Utilisation d’une séquence temporelle :
La première possibilité qu’on entrevoit est la création d’une séquence d’« instantanés » (ou snapshots) qui vont permettre de représenter le phénomène à différentes époques (notées ti). Elle est reprise par de nombreux auteurs ([Langran 1992], [Peuquet & Duan 1995], [Gregory 2002],…) comme étant la méthode de base (car la plus facile à appréhender) pour la mise à jour spatiale. On va donc réaliser une copie entière du territoire d’analyse à chacune des mises à jour et on associera à ces dernières leur intervalle de temps de validité, intervalle qui sera régulier ou non suivant la méthodologie de mise à jour qui aura été choisie (cf. § 2.1). De nombreuses possibilités peuvent être envisagées en pratique au départ d’une telle séquence de snapshots afin d’illustrer l’évolution d’un phénomène géographique au cours du temps (ex. : animation satellitaire en météorologie).
Cette méthode présente une ressemblance « assez marquée » avec celle du table-versioning présentée précédemment pour les mises à jour attributaires. On peut d’ailleurs lui reprocher les mêmes lourdeurs au niveau de la redondance d’informations. En effet, si seules quelques entités ou pixels ont été touchés par un changement entre ti-1 et ti, on risque alors d’avoir, lors de la mise à jour, un recopiage inutile des autres entités ou pixels.
Ex. :

Figure 11
Au lieu de recopier l’ensemble du territoire lors de chacune des mises à jour, on peut aussi enregistrer, pour chaque entité (en mode vecteur) ou pixel (en mode image), la séquence des changements survenus depuis l’époque de référence (notée t0). On passe ainsi d’une approche dite « orientée-territoire » à une approche dite « orientée-entités » ([Belussi et al. 1999])… On peut ici faire la comparaison avec la méthode de l’attribute-versioning pour les mises à jour attributaires. Et on pourra également lui reprocher le fait que toutes les entités ou pixels ne seront pas caractérisés par un même nombre de mises à jour, ce qui rendra plus complexe la résolution des requêtes spatio-temporelles. De plus, elle ne tient aucunement compte des contraintes topologiques entre entités voisines (on pourrait toutefois les vérifier a posteriori). Cette solution offre malgré tout deux avantages par rapport à la méthode « traditionnelle » que nous venons d’exposer : elle limite tout d’abord la redondance d’informations puisque la mise à jour touche les seuls entités ou pixels modifiés ; de plus, elle permet de gérer assez aisément l’historicité d’une entité ou d’un pixel pour les différentes mises à jour menées tout au long de son existence (depuis l’époque de référence t0 jusqu’à l’époque actuelle tn).
Une solution originale (appelée Event-based Spatio Temporal Data Model ou ESTDM) basée sur une succession de changements pour données en mode image (sous forme de pixels) a été imaginée par Peuquet & Duan (1995). Elle utilise une ligne du temps sur laquelle sont repris chronologiquement tous les évènements que le territoire a connus depuis une époque dite « de référence » (notée t0) jusqu’à l’époque actuelle (notée tn). Les évènements sont associés à leur époque d’apparition (notée ti) et à une liste de triplets (r,c,v) qui donnera, pour la position de chacun des pixels « touchés » (connue en rangée r et colonne c), sa nouvelle valeur (notée v). L’implémentation débute par la définition du fichier image de référence à l’époque t0. On lui associe ensuite un fichier d’en-tête qui reprendra le nom de la thématique étudiée, un pointeur vers le fichier image de référence ainsi que deux pointeurs vers les évènements initiaux et finaux de la ligne du temps. Finalement, on pourra construire cette ligne du temps de manière à expliciter dans l’ordre chronologique tous les évènements qui ont touché le territoire étudié. Chacun de ces évènements sera lui-même caractérisé, d’une part, par un pointeur vers un tableau reprenant tous les changements de valeurs de pixels identifiés, et d’autre part, par des pointeurs vers les évènements « précédant » et « suivant ». Notons que le pointeur qui précède l’évènement associé à l’instant t1 est dirigé vers le fichier d’en-tête tandis qu’on donnera la valeur « NULL » à celui qui suit l’évènement de l’époque tn.

Figure 12
Lorsqu’on aura affaire à une mise à jour, il suffira alors :
d’identifier les pixels touchés par l’évènement associé à la nouvelle époque tn ;
de changer la valeur « NULL » de l’ancien dernier évènement (associé dorénavant à tn-1) en un pointeur vers le nouveau (correspondant au « nouveau » tn) ;
et d’associer à ce dernier un « pointeur précédant » vers l’ancien dernier évènement et un « pointeur suivant » à qui on donnera la valeur « NULL ».
Utilisation de couches de mise à jour :
Cette méthode, proposée par Langran (1992) et résumée par Gregory (2002), consiste à créer une nouvelle couche lors de chaque mise à jour (donc à chaque époque ti), couche qui reprendra tous les changements survenus depuis la mise à jour précédente (donc depuis ti-1). On dispose également d’une couche dite « de base » qui représentera, quant à elle, la situation générale du territoire considéré à l’époque de référence t0. Pour reconstituer la situation à un instant quelconque tx, il suffira donc de faire des superpositions spatiales entre la couche de référence et toutes les couches de mise à jour créées entre t0 et tx.
Ex. :

Figure 13
Le grand avantage de cette deuxième méthode est la réduction considérable de l’espace disque alloué au stockage des informations, et cela même si on a affaire à de fréquentes mises à jour. En effet, nous n’aurons plus ici le problème de la redondance d’informations que nous avions rencontré avec la méthode des snapshots puisque seuls les entités ou pixels qui ont subi un changement depuis ti-1 seront repris sur la couche de mise à jour. Cependant, il faut se rendre compte de la complexité pour reconstituer la situation correspondant à un instant quelconque tx lorsqu’on a affaire à de nombreuses mises à jour sur le territoire. En effet, les opérations de superposition augmenteront parallèlement avec le nombre des couches de mise à jour.
Utilisation d’une couche historique :
Cette solution a été proposée par différents auteurs ([Langran 1992], [Belussi et al. 1999], [[Gregory 2002],…). Elle consiste à créer ce que Belussi et al. ont appelé une « couche historique » qui reprend toutes les extensions issues des opérations de mises à jour spatiale. On n’aura donc plus qu’une couche regroupant, à elle seule, toutes les mises à jour effectuées. En complément à cette couche historique, on retrouvera, comme précédemment, une couche de base décrivant la situation à l’époque de référence t0. Ainsi, la reconstitution de la situation correspondant à une époque tx quelconque passera par une opération de superposition spatiale entre la couche de base et la couche historique dans son état correspondant à l’époque tx.
Ex. :

Figure 14
On diminue encore, avec cette méthode, l’espace disque alloué au stockage de l’information puisqu’on n’aura plus que deux couches au total au lieu d’une par opération de mise à jour. Par ailleurs, comme le fait remarquer Gregory (2002), l’espace sera ainsi traité de manière « temporelle » et le temps de manière « spatiale » étant donné que chaque nouvelle extension (issue d’une opération de mise à jour) sera reliée à sa « caractéristique temporelle » grâce à la création d’un nouvel attribut (généralement, le temps d’enregistrement – cf. § 2.2) en plus de l’attribut de changement (ex. – cf. fig. 17 : on associe ici l’attribut temporel à l’attribut de changement d’affectation du sol).
Après avoir analysé quelques méthodes de mise à jour dans les SIG, nous pouvons maintenant nous pencher sur la problématique des requêtes spatio-temporelles qui pourront y être posées. Nous sommes en fait ici confrontés à un problème de taille. En effet, les mises à jour, qu’elles soient menées à intervalles réguliers ou en réponse à un évènement (cf. § 2.1), s’effectuent souvent de manière instantanée à une époque ti. Si la requête spatio-temporelle porte sur une entité à cette époque ti, la réponse pourra être immédiate. Par contre, si elle appelle un instant tx qui est compris entre deux époques de mises à jour et que le territoire a subi un changement entre celles-ci, il faudra trouver une solution pour déterminer ses caractéristiques à cet instant. Nous allons exposer ici deux méthodes statistiques proposées pour solutionner ce problème.
L’objectif du travail de Ratcliffe est de trouver une méthodologie pour appréhender un phénomène spatial caractérisé par une dimension temporelle non fixée. La solution théorique qu’il propose est de calculer la probabilité qu’un évènement considéré se produise à un instant quelconque tx si celui-ci appartient à un intervalle [t1, t2] (t1 et t2 étant deux instants connus). On pourra alors associer aux différentes entités leur probabilité d’apparition correspondante. Afin d’illustrer ce problème, Ratcliffe & McCullagh (1998) ont réalisé une étude sur les vols dans les magasins d’un centre urbain (phénomène en implantation spatiale ponctuelle). De manière générale, les effractions ont lieu lorsque le vendeur est absent, c’est-à-dire la nuit. On peut donc affirmer que le vol a été commis entre le départ de celui-ci le soir (instant qu’on va appeler « START Time ») et son retour dans le magasin le lendemain lorsqu’il constate l’effraction (« END Time »). Comme on ne connaît pas précisément l’instant où a eu lieu cet évènement, on va découper la période [START Time, END Time] en différentes époques (par exemple, en heure) et on va calculer la probabilité qu’il se soit produit durant chacune d’elles. Pour ce faire, on dispose de deux possibilités :
la première se base sur le fait que les périodes peuvent être caractérisées par des « poids différents », qui devront alors être déterminés au départ d’une fonction exogène ;
la seconde consiste à donner à chacune de ces périodes un poids équivalent. La probabilité que l’évènement se produise durant chacune d’entre elles sera ici simplement donnée par :
Une méthode de visualisation de cette probabilité est également proposée dans le travail de Ratcliffe afin de présenter aux utilisateurs les résultats de leurs requêtes spatio-temporelles. Elle consiste à figurer le phénomène étudié en deux dimensions au sein d’une couche de type snapshot qui illustrera celui-ci à l’instant correspondant au « critère temporel » de la requête. Chaque évènement (ex. : un vol dans un magasin) y est représenté selon ses coordonnées (X,Y). On y associe un cylindre dit « temporel » perpendiculaire à la couche, ce qui permet de représenter la dimension temporelle suivant la troisième dimension de l’espace. Si la base du cylindre correspond au START Time et le sommet au END Time, on peut dire que le temps s’écoule de manière continue entre ces deux instants. L’intersection du cylindre avec le plan de la couche (correspondant, par définition, au critère temporel de la requête) est un cercle d’autant plus sombre que la probabilité d’apparition de l’évènement à cet instant est élevée. Afin de faciliter la visualisation du phénomène (ex. : pour l’aide à la décision), il est encore possible d’envisager le « passage » de ce phénomène discret en une implantation continue (ex. : krigeage avec prise en compte des distances et des « poids de probabilité »)…
L’objectif poursuivi par Dragicevic & Marceau est similaire à celui de Ratcliffe si ce n’est que le problème est ici posé en mode image. On peut le résumer de la manière suivante :
«  Si un pixel d’une image (issue par exemple d’une classification) appartient à une catégorie A à un instant t1 et à une catégorie B à un instant postérieur t2, comment déterminer ses propriétés à un instant tx compris entre t1 et t2 ? »
La solution proposée est d’effectuer une interpolation entre les deux images correspondant aux instants t1 et t2 afin de créer la couche représentative de l’instant tx. On va donc comparer deux à deux les pixels correspondant dans les couches de t1 et de t2 et voir s’ils ont changé de catégories entre ces deux époques. Si ce n’est pas le cas, on affirmera qu’il n’y a pas eu de changement. Par contre, s’il y a eu modification de catégories entre t1 et t2, le problème sera de savoir quand ce changement a eu lieu. Notre unique certitude est que le pixel appartenait à la catégorie A à l’instant t1 et qu’il appartient à B en t2. On propose donc de définir, sur base d’une approche probabiliste, une fonction d’appartenance dite « floue » (notée « mC(t) ») pour chacune des catégories C afin de montrer l’évolution existant entre les deux instants t1 et t2. Cette proposition est bien entendu beaucoup plus riche que l’approche booléenne qui affirme simplement que le pixel appartient à une catégorie en t1 et à la même ou à une autre en t2 sans donner aucune information supplémentaire sur le laps de temps compris entre t1 et t2…

Figure 15
Les fonctions mA(t) et mB(t) sont basées sur la vitesse et le temps estimés pour passer d’une catégorie à l’autre. Elles permettront donc de déterminer, pour n’importe quel tx compris entre t1 et t2, quel est le « pourcentage d’appartenance » de chaque pixel de l’image à l’une des deux catégories considérées. Ainsi, une image illustrant la répartition d’un phénomène à l’instant tx devra traduire, pour chaque pixel, les valeurs des fonctions d’appartenance à cet instant pour les différentes catégories considérées. Remarquons que la somme de ces fonctions ne doit pas nécessairement être complémentaire (mA + mB peut être différent de 100%) mais que chacune de ces sommes doit donner un « pixel univoque » (ex. : 30% + 60% ≠ 40% + 50%).
On peut toutefois reprocher quelques incertitudes à cette méthode. Tout d’abord, si un pixel appartient à la catégorie A à l’instant t1 et à la catégorie B à l’instant t2, qui nous dit qu’il n’est pas passé au préalable par un état intermédiaire dans une catégorie M ? De même, si ce pixel appartient à l’instant t2 à la même catégorie A qu’à l’instant t1, comment être sûr qu’il n’a pas fait « un aller-retour » avec une catégorie B sur le laps de temps [t1, t2] ? Pour solutionner ces deux problèmes, il faudrait s’assurer qu’on a procédé à une mise à jour dès qu’un pixel de l’image a connu un changement. On est donc contraint d’utiliser ici la méthode de la mise à jour « en réponse à un changement observé » (cf. § 2.1)…
On retrouve, dans la littérature, un nombre sans cesse croissant de recherches qui ont pour principal objectif de trouver des solutions optimales à la gestion spatio-temporelle des SIG. Cela provient, d’une part, de la volonté accrue des utilisateurs de voir apparaître la notion d’évolution d’entités dans leurs jeux de données et, d’autre part, des possibilités informatiques (mémoire et traitements) de plus en plus importantes pour la gestion de ces informations… Cependant, ces études ont régulièrement été menées pour répondre à un problème posé dans une application bien précise et rares sont celles qui ont tenté de proposer une solution générale et théorique à cette problématique. On se rend compte par ailleurs que les diverses alternatives proposant des solutions temporelles pour la gestion de l’historicité des bases de données de type géographique (ex. : diagrammes dynamiques offerts par UML – cf. § 3.2) sont, en réalité, fort peu utilisées en pratique. En effet, si l’approche orientée-objet sur laquelle sont construits ces modèles semble actuellement la plus adaptée à ce type de recherches, on peut se rendre compte que peu de projets l’utilise dans la pratique, notamment suite à un certain « manque d’expérience » dans le domaine qui nous préoccupe…
En résumé, on peut donc dire que la majorité des projets actuels qui traitent de la dynamique spatio-temporelle restent « confinés » dans des applications d’ordre pratique en se basant essentiellement sur les concepts traditionnels des bases de données relationnelles. C’est sans doute là la raison pour laquelle les méthodologies de mise à jour qu’on retrouve aujourd’hui dans le domaine des SIG font encore une distinction entre évolutions spatiale et thématique. Cependant, il faut remarquer que ces deux types d’évolution sont « interdépendants » et que seule l’approche orientée-objet pourrait assurer la cohérence temporelle des bases de données géographiques. On ressent donc aujourd’hui un besoin important de recherches théoriques permettant de constituer un modèle spatio-temporel standard (ex. : MADS [Parentet al.]). D’un autre côté, on se rend également compte que l’application d’un tel modèle à un projet pratique nécessitera une phase ultérieure (une phase qu’on pourrait qualifier d’« ajustement ») pour qu’elle puisse s’adapter au mieux aux problèmes rencontrés.
Cette phase de recherche devrait, au préalable, identifier la méthodologie qui pourra assurer une gestion optimale des mises à jour des données considérées ainsi que de leur historicité… Il sera nécessaire, dès cette première étape, de penser à la représentation logique des résultats de ces mises à jour (ex. : comment faire intervenir dans la base données le fait que les entités sont passées par différents états depuis leur naissance jusqu’à l’époque actuelle ?) et de voir à quelle fréquence elles devront être menées dans les divers projets utilisant cette méthodologie (ex. : pour telle application, vaut-il mieux réaliser des mises à jour à intervalles réguliers ou en réponse à un changement observé ? – cf. § 2.1). Pour faciliter cette étape du travail, il serait sans doute préférable de passer par une phase de prototypage afin d’améliorer le système de manière évolutive et que celui-ci réponde aux objectifs de gestion temporelle fixés a priori.
On pourrait ainsi résumer la méthode de conception proposée de la manière suivante :

Figure 16
                            
