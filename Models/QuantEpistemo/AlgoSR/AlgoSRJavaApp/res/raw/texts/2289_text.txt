In this paper I will revisit the questions about modelling of a populations' spatial mobility that were raised by Lena Sanders in Cybergeo last year. My reasons for writing are largely personal. A year ago I assumed the directorship of SMC, an institute with which I had been associated as a visiting scientist since its foundation two years earlier. With the directorship came responsibility for a large spatial microsimulation model of the sort Sanders describes. The Sverige microsimulation model is intended to simulate the demographic and some economic behaviour of every person in Sweden representing every Swede by a distinct simulation model. All these micro-models are to run in parallel on a single (large) microcomputer. At the time of writing, Sverige runs with 6.8 million simulated people and is within two weeks of its beta release. The remaining problems all relate to computational resource allocation and are well-understood. Sverige will be running with 9 million as an applicable prototype by the end of the year.
For several years prior to joining SMC, I was interested in synergetic methods and have published a number of self-organising models too. Most of these are now described in an on-line book (Winder, 2000). It might be imagined, therefore, that I would have strong opinions about the issues raised in Sanders' paper, though the truth is that I was unaware of any intellectual or theoretical conflict between the two approaches until I read it and discussed it with her.
This does not mean I believe Sanders analysis is incorrect. Far from it, I think she has summarised a number of fundamental philosophical and theoretical differences between two modelling schools. But there are more than two species of modeller hiding in this thicket and the microsimulation modeller has never seemed distinctive enough to claim my attention until recently. Part of the reason for this has been that Sanders' taxonomy of modellers and mine are (I suspect) slightly different. For me, a synergetic modeller is a thermodynamic modeller working on self-organisation. Practical appeals to the mean value equation, or even the Master Equation are not mandatory. Until recently, I thought the term microsimulation was just the name a few economists and social scientists like to use for thermodynamic modelling. After working on a microsimulation model for a year or so, I am no longer sure : microsimulation modelling feels different, somehow.
Let me begin my guided tour of individual-based social-science theory by introducing you to some of the more abundant wildlife. I begin with the physicists.
The terms synergetic and self-organising are completely synonymous but different types of modeller tend to use one in preference to the other. Those who favour the term : synergetics usually have their roots in physics, particularly non-equilibrium thermodynamics. The names Haag and Haken and a useful but crazily expensive series of books published by Springer Verlag are associated with this school. There is a second, related school of physics modellers who tend to prefer the words self-organisation. Of these, the most celebrated is Prigogine. Prigogine's work in physical science won him a Nobel prize and the stuff he wrote about demography and biology has its enthusiasts though, in my opinion, it is too shallow and hand-waving to be taken seriously. Some of Prigogine's younger research associates have been more successful. Sanders mentioned our mutual friend Peter Allen. The names of Nicolis and Stengers are also important. Both have published a lot of affordable and helpful material.
These two groups of physicists ; the Stuttgart and the Brussels schools, disagree about philosophical and methodological issues too dull and too esoteric for most ordinary mortals, but both are working on a formal set of models called Master Equations which are sometimes useful for studying self-organising processes. Probably the single defining feature that sets most of the physicists apart from the rest, is that the former are using analytical methods applied to the Master Equations themselves to make generalisations about self-organising processes. Peter Allen, however, is an exception to this rule in that he, like Sanders, me and a number of others is now exploring a mix of computational and analytical approaches to the Master Equation. The reason we do this is that, to make master equations analytically tractable, one must usually impose constraints on them that often make them unrealistic for social- and life-science modelling. You can do better social science if you abandon analysis in favour of computer simulations.
This brings us naturally to those whose approach is primarily (often exclusively) computational. Computational specialists are often maverick scientists or IT specialists who discovered the idea of individual-based modelling and embraced the computational difficulties they raise. Among the most venerable silverbacks in this group are people who sigh nostalgically for a dead language called Simula but usually write C or C++ these days. Others were too young for batch queues and paper-tapes but fell in love with procedures or, if younger still, object orientation at an impressionable age. They write in fortran, C++ or Smalltalk as a rule. The latest generation of young whizz-kids include some that have fallen for agent-based modelling and write in arcane, new languages whose very names are foreign to me.
Many of these computational modellers came into the business from mainstream computer science or artificial intelligence, though some, myself among them, tottered in from biology or geography and use very unsexy computational techniques. If you want to locate me in the scheme of these things, my own models are typically built in BASIC or an elderly dialect of fortran.
Having dealt with the analytical and computational classes of micro-modeller, we must now turn our attention to the microsimulation modeller. This is a very diverse group. Many are economists, geographers or political scientists by training. Few of them are interested in the analytical approach. Some have the IT skills needed to build a model without professional help, but not many. Most are dependent on cheap IT specialists (grad students and the like) but are not really competent to supervise their work closely.
Parenthetically, I observe that this dependence on inexperienced programmers is evidence of something rotten in our Humanities departments. For the last ten or fifteen years, social scientists have dodged the responsibility for their own research by hiring computer people to do the work for them. I know this to be true because I was one of those consultants. Because of this, I have learned that wise social scientists will take everything they hears from an inexperienced technician with a grain or two of salt.
Graduate students and young post-docs, in particular, will swear on a stack of Bibles that models built in 1976 were beyond the comprehension of ordinary mortals until 1994. Left to their own devices, they will reinvent everything from the flint axe to the syllogism, deliver unreadable, undocumented code and spend precious departmental funds on software upgrades no-one really needs.
Make no mistake, these people are smart, often brilliant, but they have been trained to believe that everything is either new, obsolescent or obsolete and by obsolete, I mean more than six years old. Given time and training, they often become gifted modellers, but those employed in most social science departments do not get much time and training. In many cases, these green graduates start work knowing more than any senior professor on the team. The better ones develop some real academic competence in their chosen field, while the rest concentrate on staying one jump ahead of their employers long enough to escape into the commercial sector.
This cannot be right. When Carnot began his ground-breaking research on the thermodynamic cycle he did not, as far as I know, write down the equations of motion, hand them to an algebra person and say "sort this one out for me, there's a good chap.". That social scientists delegate important problem-solving tasks to inexperienced technicians and imperfectly understood, untested computer programmes is scandalous !
Right or not, it happens and, when we interpret the differences and similarities Sanders describes in her review, we must remember that microsimulation modellers are not, in general, talking about concrete computer programmes or abstract mathematical regularities but about their personal understanding of issues many of them do not completely understand. Perceptions are notoriously personal and contextually specific. The same glass may be half-full for one observer and half-empty for another !
Let me illustrate the role of perception in these issues by means of a concrete example. Lena Sanders talks in her paper about transition probabilities. I quote :
"The philosophical differences between the two approaches appear very well when considering the assumptions made in the synergetics approach in order to build the mean value equation from the master equation. First, one supposes homogeneity in the individual behaviour, at least inside a given group. On the contrary, the microsimulation supposes diversity, and argue that the only way of taking this diversity into account, is to consider each individual in the simulation. A few variables of age, sex, family structure, educational level, profession, will characterise quite different types of individuals. In addition the simulation is based on a probabilistic approach that can give two different behaviours for two similar individuals."
I believe this to be a correct representation of what microsimulation and synergetic modellers say, but is it true ?
A microsimulation model is a computer programme and digital computers use finite arithmetic. Computational modellers are not working with infinite fields as analytical modellers so often are. As Sanders tells us, a few variables of age, sex, family structure, educational level, profession, will characterise quite different types of individuals but the number of possible types, though large, is finite and the probabilities of making certain transitions within each of those types is constant.
Classical (i.e. analytical) synergetic modelling methods are not good at handling very many groups, but these are practical, not theoretical constraints. There is no reason in principle why the methods could not be extended in this direction. Indeed, under certain conditions, it is perfectly proper to assume that most individual-based equations of motion imply the existence of an (unknown) Master Equation. When this is so, the corresponding microsimulation model will, after a suitable "warm-up period" be sampling the probability distribution corresponding to the steady-state solution of that Master Equation. This allows us to characterise the eigen-solution of the Master Equation computationally without ever invoking a mean value equation. When we do this, every embellishment that can be built into a microsimulation model can go into the synergetic model
Thus, the difference between the two approaches is quantitative not qualitative : (computational) microsimulation modellers usually have more groups than (analytical) synergetic modellers but those who mix computational and analytical methods can bridge this divide.
Sanders also reminds us that :
"... the synergetics approach supposes independence between the individual decisions whereas the ambition of the microsimulation is to take into account the interdependence between individuals decisions. An example is : if the random drawing makes a particular individual move from area i between time t and t+t', and if that individual has a family all the other members of the family will also deterministically move."
Once again, I am not sure this distinction makes sense at a computational and analytical level. When a microsimulation modeller builds a model that works this way, he or she is simply working at the meso- rather than at the micro-level. This calls for a natural extension of the Master Equation, the Nested Master Equation (Winder et al 1998) in which individuals are aggregated into larger units. The method often generates interesting behaviours, and is often easier to handle computationally than analytically, but does not, of itself pose any great challenge to synergetic theory.
Finally, Sanders reminds us about :
"... the Markow assumption which says that decisions made at time t do not depend on previous history. The ambition for microsimulation is to take into account past events, which is a central cause of diversity. A simple example is that the probability to have a child between t and t+t', will differ, for the same age, according to the number of already existing children in the family."
Well, yes and no. In Sweden, most women have small families but a small number have very many. If we try to choose a single probability distribution to simulate both behaviours, we often end up with ridiculous life histories. From a modeller's perspective, it is not self-evident that these impossible life-histories actually matter at an aggregate level. If our purpose is to predict the demand for beds in a delivery ward, for example, it may not matter that the same virtual woman comes in twice in six months as long as the number of deliveries per month is correct. However, if we wish to predict the demand for income support family by family, it may matter and, if it does, the Markov assumption must be relaxed.
However, it would be an error to imagine that we cannot also handle these problems using synergetic method. Women who have ten babies have distinctive types of lifestyle : they breed early and breed often. If we can estimate the likelihood that a woman will be a heavy breeder, we can add a variable to our simulation (months since last birth event) and use different probability distributions for the two populations. The probability of a live birth six months after the last will be zero for both groups. At 15 months it will be very nearly zero among typical females and rather higher among the atypical group. A simple way of making this analytically tractable would be to take a long time step (five years, say) and work with the probabilities of zero, one, two, three or more live births in that interval.
So to conclude this section, I would accept that Sanders' paper captures stated differences between the two approaches , but argue that most, if not all of them turn out to be illusory when we dig into the mathematical and computational bases of the methods.
In our respective papers, both Sanders and I have been obliged to assume some knowledge among readers. Sanders has provided a handy bibliography to help readers get that knowledge quickly. I have not, partly because she has already done it and partly because I see a consistent mismatch between what modellers do, both computationally and analytically, and what they say they do. Rather than read what modellers say and take it on trust, I advocate that we look behind their work to see what they do and form our own judgements about the species and genera of modellers we encounter.
We have both been writing about a well-defined genus of modeller which I will now call the thermodynamic modeller. Thermodynamic modellers are usually working with an ensemble or assembly of distinct, indivisible entities and can, in principle, specify generic equations of motion or dynamic rules for those individual things. However, for one reason or another, they cannot specify these dynamics in a purely deterministic way. Perhaps the collective behaviour of the assembly is irreducibly stochastic, the data needed for deterministic modelling is unavailable or the interactive behaviour of the equations of motion is analytically intractable.
Fortunately, thermodynamic modellers are not usually interested in individual behaviours and want to generate aggregate statistics for the assembly as a whole. These aggregate variables describe the state of the whole assembly (temperature and pressure of the gas, number of goats or local population density of humans, for example).
So every thermodynamic modeller has at least two types of state variables (the micro-state of the individual and the macro-state of the assembly) and two types of dynamic system to consider (the micro-system or equations of motion that map micro-state variables onto themselves and a parallel macro-system that maps macro-state variables onto themselves).
In practice, micro-state variables can seldom be initialised perfectly and, even when they can, the interactions between individuals can only be handled (i.e. simulated or analysed) stochastically. The art of thermodynamic modelling consists of choosing equations of motion and macro-state variables that co-evolve in a tractable and useful way. The use, by physicists, of state variables like pressure, temperature, entropy and so-forth was no accident, they were chosen to be interesting, measurable and tractable given the equations of motion. Social science modellers are sometimes inclined to forget this. You cannot chose micro-state variables and equations of motion casually and expect to predict the macro-state variables politicians want to know about unless you have good theoretical grounds for believing them to be connected in a tractable way. I will return to this issue later.
I have presented here a minimalist definition of the genus thermodynamic modeller. Most modern thermodynamic modellers are working with assemblies that have meso-scale structure. There may be different classes of individual (types of gas or sex categories among humans), nested structures (each atom forms part of a molecule, each human is part of a family which), for some purposes acts as a linked entity. There may be hierarchies of agents (individuals, families, nation states) or heterarchies (families, firms, clubs and societies). All of this makes our work more difficult and interesting but does not alter its basic, thermodynamic nature.
Characterising the species of thermodynamic modeller is much harder than defining the genus itself but, following Sanders' paper and my own deliberations, I believe we can begin to move towards a workable taxonomy. Modellers who come to this work from physical science can draw on a century or more of pre-computer literature and tend to work with individuals (atoms) which have rather simple equations of motion. They are very well equipped to practice analytical methods.
There are also some biologists in this field. Population genetics involves a form of thermodynamic model in which individual genomes interact and exchange genetic material under definite selection gradients to produce shifts in the gene pool. However, biologists are also interested in ecosystem theory and the equations of motion for environmental agents are not so simple and tractable. Theoretical biologists and evolutionary biologists often use a mix of analytical and computational approaches.
Most social scientists work with humans at one level or another. The demographic equations of motion for a human are often arcane and usually difficult to parameterise. A few social science modellers, like Sanders herself, are actively exploring analytical methods, though most believe this to be a forlorn hope and so restrict their attention to purely computational work.
So we have a broad gradient from the hard, analytical physical sciences through the slightly softer life sciences in which a mix of analytical and computational methods are found, to the softest of social sciences in which computational methods predominate. There is a natural inverse correlation between the simplicity of the underlying equations of motion and the applicability of analytical method. It all makes sense at both an intuitive and an analytical level.
The second gradient I wish to define is substantively independent from the first, at least at a first glance. Some modellers, particularly social science modellers, seem to maintain a substantive independence between micro- and macro-scale dynamics. They are happy to use income and social status to predict the probability of a migration event, for example, but do not consider regional population density in the equations. These, I believe, correspond broadly to the class of modellers which Sanders calls microsimulation modellers. They are modellers for whom the macro-state is merely a statistical aggregation of micro-scale dynamics and for whom thermodynamic modelling is merely a book-keeping exercise. You count the number of people migrating into and out of locus X and that's that.
At the other extreme, corresponding (I think) to Sanders' conception of a synergetic modeller, are those who construct explicit feed-back loops between macro and micro dynamics. For these modellers, migration probabilities may be modified by macro-scale regularities so that individual migration decisions change spatial patterns of population density which in turn modify the likely migratory behaviour of individuals. This generates a fast dynamic at the micro-level, a slow dynamic at the macro-scale and a complex, often non-linear dynamic coupling between the two which is the basis of self-organising or synergetic behaviours.
Synergetic behaviour is of particular interest to Geographers because it can result in the spontaneous development of spatial pattern. It is also of interest to archaeologists (like me) because it can result in irreversible macro-scale dynamics arising spontaneously as a corollary of micro-scale decisions. This is just about as good a definition of history as one can obtain and corresponds broadly to many observations of the archaeological record. The transition from hunting and gathering to farming, for example, obliged a small group of people to settle and to deny passers-by access to the foodstocks they carried. This must have disrupted both the social and the ecological systems in which they were embedded and marginalised other hunter-gatherer groups. Since agriculture proved to be a successful stratagem, agriculturalists spread and gradually disrupted more and more of the landscape, forcing hunters and gatherers into ever more marginal environments. Eventually, they had no choice but to farm or starve.
This is a classical instance of self-organisation : of a decision taken at the micro-level changing the balance of future probabilities in such a way as to precipitate an irreversible change both in the cultural and demographic regularities. It can only be modelled thermodynamically and, even then, requires us to specify a dynamic coupling between micro and macro-scale dynamics. Models in which demographic "decisions" are uncoupled from meso- or macro-scale dynamics cannot self-organise in this way.
Thus, if it really were true that microsimulation modellers were unaware of the fact that each individual was part of the environment of itself and of other individuals, it would be possible to draw a sharp line between microsimulation and synergetic modellers. However, this is not so. Many microsimulation models contain (albeit weak) couplings between micro and macro-dynamics and so are, at least in theory, capable of spontaneous self-organisation. However, they hardly ever do so, because they are typically "aligned" into docile compliance with the modeller's prior expectations.
Alignment is, in my view, the defining attribute of a species of social science modeller, many of whom call themselves microsimulation modellers. With apologies to self-proclaimed microsimulation modellers who eschew alignment, I am going to reserve the term for thermodynamic modellers who work in the social sciences and use alignment hereafter.
Alignment works as follows :
An equation (typically a regression equation) is used to estimate the probability of some event (migration, say) for every individual (or family) in the population.
On the basis of these probabilities, stochastic decisions to migrate or not are made for each individual.
The population is then split into a number of alignment groups on the basis of age, sex, and other socioeconomic factors. Ideally, these alignment groups should be substantively independent of the variables used to compute individual probabilities.
The number of predicted migration events is compared to the number expected (usually on the basis of some historical data) and the migration probabilities of group members are adjusted up or down to compensate for any discrepancies between expected and observed numbers.
New migration decisions are made using the aligned probability values.
The reason for this procedure is that microsimulation models usually fail to simulate known time-series data. By aligning the model, goodness of fit to an observed time series can be guaranteed. Opinions vary as to the admissibility of this procedure. Most microsimulation modellers accept alignment as an unfortunate, but unavoidable necessity while other thermodynamic modellers (myself among them) consider it to be an indefensible fiddle which, to use Popper's celebrated phrase, effectively "immunises the model against empirical refutation".
After many hours of argument, I have concluded that there is no universal answer to this question. Some modellers think alignment is indispensable, others think it unacceptable and no amount of friendly discussion seems to lead to consensus. Readers of this article must form their own opinions. However, the use of alignment has two inescapable corollaries.
Firstly, alignment makes it very difficult to use microsimulation models as predictive tools. Since the models cannot be made to track real historical trajectories without alignment, and alignment data are not available for the future, you should not use microsimulation for prediction without a strong likelihood of poor results. Some modellers try to work round this problem by constructing alignment scenaria into the future but this implies that the only way microsimulation modellers can predict the future is by persuading someone who knows more than they do to tell them what's going to happen : it hardly seems impressive.
The second corollary is that, even when dynamic connections between micro- and macro-scale dynamics are implemented into a microsimulation model, alignment effectively prevents the model from self-organising by distorting the underlying equations of motion to drag the macro-variables back to a known time series. If, like me, you believe the empirical evidence for spontaneous self-organisation in historical systems to be overwhelming, you must conclude that microsimulation modelling, that is, thermodynamic modelling with alignment, is of no value in the investigation of historical systems.
Aligned microsimulation models such as Corsim and its derivative Dynacan are extremely well-funded by national agencies, and our own, well-funded Sverige model, which drew heavily on Corsim in its design phase, also uses alignment in some modules. So readers can conclude that not everyone in policy-relevant science agrees with my assessment of the method. However, it is fair to say that the existence of alignment in microsimulation modelling will stand as a massive obstacle to the integration of it with synergetic method. Unless we learn to model socio-economic and demographic processes without alignment, we must accept that these models are merely algorithms for generating large artificial population samples and have no real predictive value.
From my perspective, this is absolutely unacceptable and we at SMC have begun developing non-aligned alternatives to some of our classical microsimulation modules. Our intention is to use the prototype we have developed to find out whether alignment really is a necessary evil. If we can build demographic models that run well without alignment and make explicit feedback links between macro- and micro- dynamics, we may have moved a little closer to Lena Sanders' ideal of a complementarity between the two modelling paradigms. If we can't, we must accept that the microsimulation and synergetic paradigms, as I characterise them here, are mutually irreconcilable.
The opinions expressed here are my own.
