A usual way to model Forest dynamics is to model trees behaviour at an individual level in order to be as close as possible to what is considered as the basic processes, hoping that aggregating from elementary brick toward more collective levels such as a forest stand is only a matter of mere computation. This modelling approach has been emphasised for twenty years in Forest and Ecological Modelling, as powerful hardware and software make a realistic simulation of any forest quite possible. But realistic simulation does not mean understanding, and many questions remain open, if not unanswered or even addressed. Moreover, numerical simulation can reach a high sophistication degree, such as a mathematical understanding of the model is as difficult as an understanding of the process itself.
One way to reach a better understanding is to simplify the mathematical model, which sometimes can boil down to a sketch, or a skeleton of the modelled object. The objective is to lower as much as possible the sophistication of the model, while keeping as much as possible of its qualitative behaviour. If this process is successful, it is then possible to hypothesize that the understanding of the mathematical behaviour of the model can enlighten the understanding of the modelled process. But this assumption cannot be taken for granted, and must always be falsified. As many sophisticated models can boil down toward the same simplified skeleton, any result will then be more general than an explanation of the specific case from which the question was raised. In some domains of physical sciences, such as phase transition for example, an active research field is to enumerate if possible a small number of simplified models which can encompass most of real situations. Real situation boiling down towards the same simple model are called ‘universality classes’. Such an achievement has not been reach in ecological or biological sciences, where the objects are intrinsically more complex. But ecologists and forester work with so called generic models which are supposed to encapsulate the qualitative behaviour of the biological systems, not affected by sophistication when more and more details are taken in consideration.
A generic growth model with nearby connections has been developed to model growth of homogeneous forests as a network of connected cells : an individual is a tree, and the population is a forest. The trees are located at the nodes of a grid, and interactions between neighbours are driven by competition for resources. If we call the size of the tree located at the node of a linear grid, with a growth function of the form
it is shown analytically (exact computations are omitted in this brief presentation, and are submitted for publication) that there exist a level of resources which switches the behaviour of the forest (namely the set of trajectories cell by cell) between an homogeneous pencil of growth curves into the development of two subpopulations. The result is generic, in the sense that it does not depend on a choice of a growth function f or of the interaction function Red. It is then possible to chose a very simple grtowth function, such as a logistic curve, or a more sophisticaed one. There is an analogy with second order paramagnetic / antiferromagnetic phase transition, but this transition is closer to transitions which occur during the growth of surfaces in homogeneous media, such as shape of bacteria populations, etc. ...
The extension to spatial connections which are not restricted to be neighbourhoods is easy, and open the way to a connection with modelling the growth of city networks. In case of growth of trees, it is assumed that
as trees keep on growing until the time where they die. In case of city networks, negative development are possible, where . As no condition on the sign of dx/dt is assumed in the demonstration of the result, it can be extended towards city networks.
As an interpretation, the level of resources is analogous to the temperature in a physical system in an Ising model. Indeed, the interaction function is often on the form

such as . C
can be interpreted as a competition index, and as the reaction of central cell of size x to competition driven by neighbours of size y and z. The variable T is a measure of the intensity of the competition for given sizes. If sizes are fixed, the reduction factor Red will be higher (namely the growth slower) for small T and lower (namely the growth quicker) for high T. T is naturally related to the strength of the interaction, and as such analogous to a temperature in an Ising model.
In a medium, temperature is homogeneous, and the mean expectation of available resources is homogenous from cell to cell too. There is then a possibility to have this level evolving and changing from cell to cell, as an adaptative process of a city to its neighbourhood. As we are looking for auto-organisation, the link with evolutionary processes can be emphasised, where mutations and cross-over play the role of exploring the space of potential processes as a stochastic process, and fitness and selection drive the deterministic evolution of a given system. This type of processed which associate iteratively an exploratory phase and a selection phase are very common, as a mechanism for evolution as well as in global optimisation, such as genetic algorithms and simulated annealing.
When compared to synergetics, slow variables are stochastic, whereas quick variables are deterministic. It can be noticed as an open question for discussion that this way of formalising connections between random and deterministic processes is opposite to the usual connection in physical systems, where quick variables are diffusion processes which drive the system towards the thermal equilibrium, and slow variables are deterministic and drive the system adiabatically at constant thermal equilibrium.
A second point can be open to discussions : most of models are designed by engineers and/or scientists. While doing so, they (we) emphasise the deterministic selection process : among all possible processes, only those who compel with the rules at the base of the model will be selected. The exploratory phase is then ignored, and it is indeed very difficult to formalise it mathematically. In most case, it is taken into account as a pure random process. Is exploration really random ? How to model the exploration of adaptative processes ?
