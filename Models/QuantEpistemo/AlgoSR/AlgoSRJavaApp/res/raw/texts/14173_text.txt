Un grand nombre de phénomènes non linéaires évoluent par séquences, c'est à dire d'une façon continue interrompue par des sauts brusques; ces sauts font passer à chaque fois à des niveaux d'organisation de plus en plus complexes en suivant une hiérarchie. Cette complexification débouche sur des situations critiques, correspondant à de forts changements. Citons comme exemples les tremblements de terre, les éruptions volcaniques et autres manifestations climatiques ou, dans d'autres domaines, les fluctuations des marchés financiers….qui se caractérisent par une série de manifestations nommées "précurseurs" précédant la transition violente (séisme, krach boursier…). Celle-ci est suivie aussitôt de "répliques" qui se succèdent à des intervalles de plus en plus grands. Evidemment la dynamique linéaire classique ne permet pas d'expliquer ces phénomènes.
Cependant, en analysant plusieurs de ceux-ci, D. Sornette et ses collaborateurs ont montré qu'ils pouvaient être décrits par une " log périodicité" corrigeant des lois puissance dont l'exposant, c'est à dire la dimension fractale, est un nombre complexe. La justification par ces auteurs (Sornette 2002, 1998, Sornette, Sammis 1995, Sornette, Johansen, Bouchaud 1996, Johansen, Sornette 2001) trouve dans l'association" groupe de renormalisation et invariance d'échelle". Ainsi se définit un temps critique, temps vers lequel ou à partir duquel les phénomènes étudiés se manifestent par vagues dont la fréquence des accélérations ou des décélérations suit une loi mathématique. L'exposé détaillé de ce modèle peut être trouvé dans la référence 1.
De leur coté, en s'appuyant sur la " relativité d'échelle" qui est une forme plus générale de compréhension des lois d'échelles, L.Nottale, J.Chaline et P.Grou ont montré dans une série d'articles (Nottale, Chaline, Grou 2000, Chaline, Nottale, Grou 1999, Nottale, Chaline, Grou 2000, Grou, Nottale, Chaline 2002) que l'évolution des primates, des équidés, des rongeurs, des dinosaures….mais aussi celle des crises économiques dans les sociétés occidentales et précolombiennes, pouvait être analysée, également, par la log périodicité. Par ailleurs, dans un travail précédent portant sur un domaine fort éloigné, celui de la musique, et pour une plage de temps plus brève, l'un des deux auteurs a pu constater que les différentes étapes marquantes de la chronologie de l'histoire du Jazz suivaient également une loi log périodique (Brissaud 2007). Dans l'étude présente nous nous proposons de vérifier sur un autre "arbre d'évolution", celui des accélérateurs de particules utilisés en Physique Nucléaire et Physique des Particules, la validité de cette loi pour tenter de conclure à son universalité.
Pour étayer leur formalismeL.Nottale, J.Chaline et P.Grou (Nottale, Chaline, Grou 2000, Chaline, Nottale, Grou 1999, Nottale, Chaline, Grou 2000, Grou, Nottale, Chaline 2002) s'appuient sur l'analogie avec les arbres "végétaux". Ils supposent au départ que les sections des branches d'arbres suivent une géométrie fractale auto similaire. Ils démontrent de façon assez simple par analogie avec les embranchements (le départ des branches sur un tronc) que les sauts majeurs dans les systèmes critiques se succèdent à des instants Tn tels que log(Tn-Tc) = log(T°-Tc) - nxlog(g) où Tc est le moment qui correspond à la limite finale théorique de l'évolution après n sauts (la variable temps T remplaçant les longueurs X utilisées pour les arbres !). Dans le cas des séismes, chaque événement à l'instant Tn est un précurseur alors qu'à l'instant Tc va se produire le tremblement de terre. T° est une constante et g le facteur d'auto similarité quand on détermine les séries d'intervalles de temps en échelles logarithmiques log(Tn-Tc). Les deux inconnues Tc et g peuvent être déterminées à partir de deux dates évènementielles successives: g = (Tn-Tn-1) / (Tn+1-Tn) = (Tn-Tc) / (Tn+1-Tc), soit Tc= (gxTn+1-Tn) / (g-1)
Ainsi connaissant les paramètres Tc et g il est aisé de calculer l'instant Tn+1 de l'événement de rang n+1 à partir de l'instant Tn de rang n : Tn+1 = [(g-1)xTc+Tn] /g (I)
 et ainsi de suite.
Une conclusion notable de ce simple modèle est, s'il est vérifié, d'affirmer qu'il est possible de prédire approximativement les dates des étapes d'une chronologie dont on connaît les ingrédients g et Tc, seulement les dates et non leurs causes et leurs conséquences. Mais la principale question posée est de comprendre l'origine d'une telle loi d'arborescence fractale.
De nombreux textes soulignent que les différents types d'accélérateurs de particules se sont succédés après des innovations très marquantes qui permettaient à l'énergie des projectiles de croître par des sauts notables satisfaisant les désirs des physiciens; ceux-ci souhaitent que cette énergie soit la plus élevée possible pour que la longueur d'onde associée à ces projectiles soit réduite afin de sonder toujours plus finement les noyaux et leurs constituants protons et neutrons, puis les particules plus fondamentales qui constituent ceux-ci. Cette évolution des accélérateurs est particulièrement visible sur le "diagramme de Livingston" (Livingston 1954). On y voit la série de sauts qui ont conduit à différents types d'accélérateurs (cyclotrons, synchrocyclotrons, synchrotrons…), chacun de ceux-ci évoluant après son apparition de façon continue jusqu'à son abandon en tant qu'instrument de physique nucléaire ou corpusculaire. Sur la figure 1 nous avons repris ce diagramme, mais en distinguant les accélérateurs de protons et d'électrons. Comme cette figure reprend le diagramme établi précédemment, le saut lié à la cryogénie (le Large Hadron Collider soit le LHC) n'apparaît pas.

Figure 1 : Diagramme de Livingston donnant séparément la chronologie des accélérateurs de protons et d'électrons. Les noms Coll., Syn., Cyclo. f. signifient respectivement collisionneur, synchrotron, cyclotron, focalisation. Les énergies sont données dans le système projectile-cible fixe.
Résumons ici l'évolution des accélérateurs (Livingston 1954, Bryant 1995) jusqu'à nos jours. Avec la découverte de la radioactivité naturelle à la fin du XIXe siècle, il a été montré que les noyaux radioactifs émettent spontanément des particules  (noyaux d'hélium),  (électrons) et un rayonnement électromagnétique . Tout naturellement ces particules furent rapidement choisies comme sondes pour étudier la constitution de la matière. Ainsi en 1911 E. Rutherford confirme une hypothèse de 1908 : l'existence des noyaux atomiques, en bombardant des feuilles d'or avec des particules . Pour étudier la constitution de ces noyaux Rutherford estima qu'il fallait disposer de sondes plus énergiques que les alpha de la radioactivité naturelle. Après la première guerre mondiale, il exerça une forte pression sur le monde des chercheurs et des industriels afin qu'ils développent des machines délivrant des faisceaux plus intenses et plus énergiques de divers projectiles : protons, électrons etc. (voir L'Apostrophe à la Royal Physical Society à Londres en 19271).
Dès 1924 G. Ising propose de réaliser une accélération par étapes successives en utilisant une tension périodique astucieusement répartie sur une série de tubes. La proposition faite pour un faisceau d'électrons ne fut pas suivie de réalisation. S'inspirant des idées d'Ising, R. Wideröe projette l'accélérateur linéaire (1928) et, parallèlement, R.Lawrence conçoit le cyclotron (1929), puis dès 1932, à Berkeley, fait fonctionner une machine basée sur ce principe, délivrant des protons de 1,25 MeV2; c'est le premier d'une longue série. Simultanément, J.D. Cockroft et E.T. Walton en Grande Bretagne, toujours sous l'impulsion de Rutherford, construisent un générateur à redresseurs et réalisent la première réaction nucléaire. Il faut noter que les "générateurs Cockroft-Walton" sont toujours utilisés. En 1931, aux USA, R.Van de Graaff (qui avait effectué auparavant un long stage d'étude à Oxford !) invente la machine électrostatique qui porte désormais son nom. En 1940 D.W. Kerst construit un bêtatron à électrons de 2,3 MeV dont R. Wideröe avait eu l'idée auparavant sans parvenir à la concrétiser, poursuivant une étude entamée par Slepian (1922). Dix ans plus tard Kerst fait fonctionner un bêtatron d'une énergie record de 300 MeV. Mais ce type d'accélérateur utilisé en tant que générateur de rayons X fut remplacé par les accélérateurs linéaires à électrons beaucoup moins encombrants qui permettent ainsi l'irradiation des tumeurs. Aujourd'hui il n'y a plus de bêtatron en fonctionnement.
Après la seconde guerre mondiale, pour des raisons économiques et politiques évidentes, les recherches en Physique Nucléaire s'amplifient. Le cyclotron dont on avait montré qu'il ne permet pas d'atteindre des énergies élevées à cause des effets relativistes fut supplanté par le synchrocyclotron (1946) équipé d'un modulateur de la radiofréquence. Par la suite, certains synchrocyclotrons purent accélérer des protons jusqu'à 1 GeV environ. Mais rapidement ces machines furent supplantées à leur tour et il n'y en a plus guère en fonctionnement.
En effet, en 1944, une étape très importante est franchie: un américain, E.M. MacMillan, et un soviétique, V. Veksler, d'une façon totalement indépendante, mettent en évidence le principe de la focalisation de phase et donnent naissance au synchrotron; dans une telle machine, malgré la variation temporelle du champ magnétique et de la fréquence d'accélération, les projectiles restent groupés au sein de chaque paquet autour d'une particule de "référence". La conséquence directe et rapide de cette découverte est qu'un premier synchrotron à électrons est mis en service dés 1946 alors qu'un synchrotron à protons est inauguré en 1952. Ces machines sont à gradient constant (focalisation faible comme dans les premiers cyclotrons). Leur limite en énergie est d'environ 10 GeV pour un coût raisonnable. Une nouvelle discipline naît à ce moment-là: la physique des particules car les projectiles délivrées par ces accélérateurs permettent d'étudier les constituants des nucléons. En 1954 le CERN est crée à Genève. Son pendant pour l'Europe de l'Est est établi peu après à Doubna en URSS. Ce sont des contrepoids scientifiques à l'hégémonie américaine d'alors.
Après une étude non publiée de N.C. Christophilos3, ce sont E.D.Courant, M.S.Livingston, H.S.Snyder qui (re)découvrent la focalisation forte (gradient alterné) qui va permettre d'atteindre en 1962 des énergies encore supérieures. Une nouvelle étape est ensuite franchie par J.Budker et S. Van der Meer qui inventent, respectivement, le refroidissement par faisceau d'électrons et le refroidissement stochastique permettant de stocker des particules ou antiparticules dans des anneaux magnétiques. Ceci rend possible les anneaux de collisions ou collisionneurs (en 1972 le collisionneur proton-antiproton ISR du CERN entre en fonctionnement avec un double anneau) et aussi la réalisation de réactions particules-antiparticules à de très grandes énergies dans le système de coordonnées du centre de masse. Il faut rappeler que c'est en 1943 que Wideröe avait souligné que des collisions frontales donnent des énergies dans le système du centre de masse supérieures à celles en jeu dans les collisions sur une cible fixe. Enfin, dernière innovation majeure: la cryogénie autorise l'emploi d'aimants à hauts champs magnétiques (4 Teslas). En 1992, le collisionneur Tevatron du laboratoire Fermi aux USA est équipé ainsi. Ultérieurement la réalisation des cavités accélératrices cryogéniques permet d'atteindre des champs électriques très élevés (40-50 MV/m). De la sorte sont accélérées des particules à des énergies élevées dans des anneaux de longueurs "raisonnables" avec des pertes par rayonnement réduites. Le LHC du CERN sera très prochainement (2008) le collisionneur bénéficiant de toutes ces possibilités.
Parallèlement à cette course aux grandes énergies d'autres types d'accélérateurs sont mis au point : a) Des accélérateurs linéaires à protons (premier exemplaire construit à Berkeley par L. Alvarez en 1951) sont développés et servent surtout d'injecteurs aux synchrotrons. b)Les accélérateurs linéaires à électrons ont vu le jour en 1946 à Stanford et au MIT grâce aux progrès considérables sur les tubes radiofréquence durant la guerre mondiale. Ceux-ci leur permettent d'atteindre peu à peu une vingtaine de GeV dans les années 1980. c) Les performances techniques des Van de Graaff sont grandement améliorées permettant d'accroître la tenue en tension dans des gaz d'isolation parfaitement choisis. Et grâce à l'invention du tandem (Alvarez en 1951) des tensions extrêmes de 25 MV sont actuellement possibles. d) Des cyclotrons à secteurs séparés sont construits dans les années 1950 en reprenant une invention passée complètement inaperçue et formulée en 1938 par R.H. Thomas. Cette "négligence" s'explique sans doute par les faibles moyens de calcul de la forme exacte des secteurs à cette époque. e) Enfin un premier cyclotron à bobines supraconductrices peut fonctionner dés 1982 à l'Université du Michigan.
Les tableaux 1 et 2 résument les dates des sauts d'évolution que nous avons précisées plus haut en distinguant les accélérateurs de protons et d'électrons. La différence de masse et de nature entre ces deux projectiles explique que les appareils les accélérant soient différents, principalement en dimensions, et que le même type d'accélérateur fonctionne avec quelques années d'avance pour les électrons. Les accélérateurs d'ions lourds ne seront pas étudiés ici car ils ne participent pas directement à la course aux hautes énergies et parce qu'ils utilisent généralement les innovations pour les accélérateurs de protons et d'électrons. Les innovations les concernant portent principalement sur les sources d'ions.
 

La figure 2 donne pour les accélérateurs de protons les différences de temps (Tn+1 –Tn) entre sauts dans l'évolution de l'origine jusqu'à maintenant. On distingue tout d'abord une grande rapidité dans le rythme d'apparition de nouveaux accélérateurs protons avant un ralentissement après les années 1950. De son côté, la mise en service des accélérateurs d'électrons, qui sont en usage seulement après la seconde guerre mondiale, montre une évolution proche de celle des accélérateurs de protons; ceci est normal car en dehors du bêtatron ils procèdent de techniques très voisines.Les intervalles de temps Dt entre sauts étant donnés, nous pouvons vérifier qu'ils satisfont aux équations, présentées plus haut, définissant la log périodicité. La recherche sur les deux paramètres g et Tc est effectuée en minimisant par khi2 l'écart entre les dates données plus haut et celles calculées à l'aide de l'équation I avec g et Tc. Le tableau 3 donne les valeurs de g et de Tc déterminées pour les protons et électrons. Il faut noter que pour les accélérateurs de protons, il y a successivement deux processus d'innovations et un seul pour les électrons.

Figure 2 : Intervalles de temps entre sauts d'innovations permettant de passer d'une famille d'accélérateurs (de protons et d'électrons) à une autre avec un gain en énergie important .
Nous constatons que les temps critiques pour les protons (la seconde phase) et les électrons sont très voisins (au voisinage de 1930), c'est à dire pratiquement lors de la réalisation des premiers accélérateurs. A partir des années 1930, la réalisation des premiers types d'accélérateurs de protons est extrêmement rapide. Pour la période 1930-50 la détermination des quantités g et Tc est incertaine car le nombre de données est réduit. Cependant la figure 2 permet d'estimer que le moment où se raccordent les deux phases est environ 1953+/- 5 proche du temps critique de la première phase des accélérateurs protons. Cette date paraît parfaitement valable car c'est l'époque de la maturité de la physique nucléaire (le modèle en couches !), du démarrage de la physique des particules et plus généralement de la renaissance de la recherche en Europe après la seconde guerre mondiale. Quant au paramètre g, sa valeur dépend du nombre de données; en augmentant ce dernier, la valeur de g augmente alors que Tc ne doit pas varier (Nottale, Chaline, Grou 2000)
Le diagramme de Livingston a eu un certain succès car il montre, sans l'expliquer, qu'en portant la variation de l'énergie des accélérateurs en fonction de la date de construction sur une échelle semi-logarithmique, le gain en énergie a une dépendance quasiment linéaire. La droite de lissage énergie-date observée ainsi passe évidemment au voisinage de 1930. Pour confirmer que l'accord est très satisfaisant, le tableau 4 donne les dates calculées par l'équation I avec les ingrédients g=1,37 et Tc=1930 en partant de T1=2008 pour les protons (le LHC) et g=1,68 et Tc=1932 et en partant de T1=1995 (LP2) pour les électrons.
A titre d'exemple, en adoptant la valeur g=1,45 au lieu de 1,36 les dates calculées ainsi sont respectivement : 2008, 1990, 1976, 1965, 1957, 1951 pour les accélérateurs de protons. L'écart avec les données (Tab.1 et 2) devient considérable et montre que g a été déterminé avec une précision satisfaisante.

Tableau 4 : Dates calculées pour g=1,37 et Tc= 1930 et pour g=1,68 et Tc=1932.
L'arbre d'évolution des accélérateurs que l'on peut voir sur le diagramme de Livingston possède quelques branches plus particulières. Ainsi celle des cyclotrons montre des sauts importants: utilisation des secteurs séparés et cryogénie. Faute de données suffisantes la log périodicité de cette famille ne peut être confirmée. Actuellement toutes ces machines sont abandonnées pour les recherches de physique fondamentale.
a) Faisons au préalable une remarque générale sur la chronologie des innovations. La courbe de la figure 3-gauche donne le nombre cumulé de chaque nouveau type d'accélérateurs en fonction de la date de mise en service (repris des données ci-dessus). On voit que ce nombre croît très vite entre 1930 et 1940, c'est à dire au moment où s'aggrave la crise économique (qui se manifeste après 1920), crise qui va déboucher sur la seconde guerre mondiale (1940/45); ensuite, ce nombre marque le pas avant une évolution continue, mais plus lente jusque vers 1970/75, fin des "Trente Glorieuses". Une pause suit autour des années 1975/80 (début de la récession) précédant un redémarrage lors de la construction des collisionneurs en concordance avec la crise des années 1990/95. On peut retrouver ici un exemple des conclusions de J. Schumpeter qui avait mis en évidence que les innovations sont plus fournies aux environs des crises pour sortir de celles-ci qu'au moment de pleine croissance (Schumpeter 1939). Sur la même figure, nous avons porté les dates de quelques idées d'inventions ayant conduit aux innovations proprement dites, étudiées ici. On mesure ainsi l'écart entre l'idée première et la réalisation qui est beaucoup plus tardive. Par exemple, l'invention du bêtatron par Slepian date de 1922 et le premier appareil est là en 1940; l'accélérateur linéaire conçu par Wideröe (1928) fonctionne en 1946 grâce à Kerst; les collisions frontales proposées par Thomas (1943) n'ont été expérimentées qu'à la fin des années 1960. Une exception notable: les synchrotrons à focalisations faible et forte ont été mis en fonctionnement seulement quelques années après l'invention correspondante. Pour ce qui est des inventions, on constate qu'elles sont toutes arrivées au moment de la crise des années 1940 ou après, lors du redressement de l'économie.

Figure 3 : La courbe de gauche donne le nombre cumulé de tous les nouveaux types d'accélérateurs depuis le début du XXe siècle (tableaux 1 et 2)..
Les traits horizontaux donnent le délai séparant l'idée inventive et la réalisation de l'accélérateur correspondant. Les traits en haut de figure schématisent les phases de croissance et décroissance des cycles de Kondratiev4 au cours de cet intervalle de temps (Schumpeter 1939). La courbe de droite donne les trois composantes logistiques issues de la décomposition de la courbe de gauche.
La courbe de la figure 3-gauche montre nettement trois épaulements. A l'aide du programme5 Loglet, nous avons pu décomposer cette courbe en trois composantes décrites chacune par une fonction logistique classique de la forme N=N°/(1+exp-((t-t°)/t)) où N°, t, t° sont respectivement la valeur de N à la saturation, l'intervalle de temps nécessaire pour que N passe de N°/10 à N°90/100 et le temps t où N a atteint la valeur N°/2 (fig.3-droite). Il est bien connu à la suite de nombreux travaux que cette fonction (voir par exemple Verhulst 1838, Grubler 1991, 1991) rend bien compte des diffusions comme celles des épidémies, mais plus particulièrement celles des inventions et innovations. Ainsi, malgré un nombre réduit de données, nous mettons bien en évidence trois périodes de réalisation de nouveaux types d'accélérateurs: une première, de quelques années seulement et d'une croissance rapide, avant la 2e guerre mondiale et centrée sur 1931; une seconde centrée sur 1948 s'étendant sur une trentaine d'années et enfin une dernière centrée sur 1989 d'une durée voisine. Nous notons ainsi une parfaite corrélation entre la chronologie des familles d'accélérateurs et l'évolution économique.
b) Le diagramme de Livingston fait bien apparaître que chaque nœud de l'arbre d'évolution est le point de départ d'une famille d'accélérateurs qui se développe d'une façon continue en évoluant, manifestant une amélioration constante des performances grâce aux progrès des techniques et du matériel. Il y a peu de machines exactement semblables, car elles seraient, alors, sans grand intérêt scientifique. Parmi ces progrès, il faut souligner l'utilisation des ordinateurs qui a permis des réalisations plus précises et plus aisées, par exemple, grâce à des simulations qui ont remplacé avantageusement la cuve rhéographique ou la méthode du fil flottant pour la définition des pièces polaires! Il faut mentionner aussi l'acquisition de connaissances permettant la compréhension des phénomènes qui limitent les performances des accélérateurs (par exemple la charge d'espace, les résonances, l'interaction faisceau-structure environnante…). Si, au départ, l'énergie maximum atteinte par ces accélérateurs d'un même type croît rapidement d'une réalisation à l'autre, après quelques années elle n'augmente plus que lentement; et quand une famille d'accélérateurs offre une énergie maximum qui approche celle du type plus innovant apparu entre temps, elle est abandonnée. Ainsi, les cyclotrons classiques se sont développés rapidement, mais aussitôt après avoir atteint leurs limites ils ont été laissés de coté et les chercheurs ont poursuivi leur travail avec les synchrocyclotrons. Notons cependant que l'on construit toujours des cyclotrons (des centaines) pour des applications principalement médicales (radiothérapie, production de radioéléments). L'énergie des synchrocyclotrons n'a crû que fort peu alors que les synchrotrons avaient déjà montré leurs qualités. Aussi ceux-ci ont-ils été adoptés unanimement lorsque l'énergie des synchrocyclotrons n'a plus été compétitive et ainsi de suite. Cependant, quand les cyclotrons à secteurs séparés ont été construits au milieu des années 1950, ils ont été exploités et développés (pour accélérer des ions légers dans la course aux hautes énergies ) jusqu'à ce que leurs énergies rejoignent celles des synchrocyclotrons. Par la suite ils ont été adaptés pour la physique des ions lourds. On doit remarquer la proximité entre ce processus et celui de l'évolution des espèces proposée par Darwin et complétée par S.J. Gould et N. Eldridge (Gould, Eldridge 1977). Ceux-ci ont proposé que l'évolution ne s'effectue pas de façon continue, mais avec des sauts qui viennent interrompre les périodes "calmes" (les équilibres ponctués). Ainsi ils peuvent rendre compte de l'apparition et de la disparition de nombreuses espèces.
Il faut aussi noter que chaque accélérateur innovant a été développé sur un site différent. Au départ c'est en Grande Bretagne, puis rapidement aux Etats-Unis que les prototypes sont en service. Après la guerre, l'URSS et divers pays Européens Occidentaux sont souvent à l'origine des innovations. Par la suite c'est la globalisation qui se manifeste dans l'étude de projets, en particulier au CERN.
c) Diverses hiérarchies sont visibles dans la succession des accélérateurs.
Tout d'abord la physique étudiée avec eux a évolué depuis la mise en évidence des noyaux, de celle des nucléons puis des quarks dans ceux-ci jusqu'à la recherche du boson de Higgs.
Les dimensions des équipements ont changé d'échelle; il faut rappeler que les premiers cyclotrons avaient des diamètres de moins d'un mètre alors que les synchrotrons et les collisionneurs exigent des anneaux de plusieurs dizaines de kilomètres de circonférence (27 km pour le LHC) !
Les équipes de chercheurs et de techniciens nécessaires au déroulement des expériences et aux développements comprennent des milliers de personnes venus de différents pays du globe au lieu des quelques dizaines résidant en un même établissement, un demi siècle auparavant. Il a fallu créer au fur et à mesure des structures scientifiques et administratives totalement nouvelles. L'organisation du travail a du être remodelée à plusieurs reprises. Vers 1960 les accélérateurs étaient implantés et gérés dans un seul laboratoire. Puis des machines nationales prirent le relais et, bientôt, il n'y aura qu'un seul très grand accélérateur aux performances uniques (le LHC). On est passé par étapes de l'échelle du laboratoire à celle de la planète !
Et pour décider de la construction de telles machines ainsi que de leur fonctionnement, il faut faire "émerger" une organisation comprenant, selon le cas, tous les facteurs scientifiques, économiques, politiques aux niveaux locaux, nationaux et internationaux. Il est évident qu'apparaît ainsi une cascade d'interactions de diverses natures, souvent antagonistes (que l'on peut nommer des contre réactions!).
Il y a aussi une hiérarchie dans les obstacles financiers qui progressent fortement d'un niveau à l'autre avec le type de machine; les coûts des accélérateurs et des équipements associés sont colossaux et ils jouent un rôle primordial, pouvant bloquer la réalisation d'un projet. Que l'on se souvienne du projet de super collisionneur américain (SSC) de 87 km de circonférence, abandonné en 1993 car estimé trop cher, peu après sa mise en chantier. Il y a beaucoup d'autres contraintes fort différentes; comme exemple les aimants des premiers cyclotrons ont été réquisitionnés et détournés de leur vocation initiale par l'armée américaine pour servir de séparateurs d'isotopes dans le "Manhattan Project", lors de la dernière guerre. Ceci montre bien l'existence de fortes contraintes dues au milieu social et politique, contraintes qui limitent les effets du hasard et façonnent l'ensemble.
On peut conclure que le système de développement et d'organisation des accélérateurs avec des hiérarchies, des composants en forte interaction entre eux, des contre réactions, l'émergence de nouvelles structures, des adaptations successives….. présente les caractéristiques définissant un système complexe comme cela a été montré par divers spécialistes de "la complexité" (voir par exemple Morin 1977, Gell-Mann 1991, Péguy 2001). La question est d'expliquer comment ont évolué les accélérateurs dans le cadre de ce système complexe !
d) Une première réponse peut être avancée ci-dessous. L'obligation faite à la recherche scientifique fondamentale pour faire progresser les connaissances est d'obtenir de nouveaux résultats marquants. Dans ce but, périodiquement, les physiciens désirent ouvrir un nouveau champ d'investigation quand le présent commence à se tarir; pour cela les spécialistes souhaitent disposer d'accélérateurs proposant un gain en énergie de plusieurs ordres de grandeur. C'est la condition nécessaire pour atteindre ce que les économistes appellent le "gain de productivité" optimum, gain qui est présenté (Nottale, Chaline, Grou 2000) comme le moteur de l'évolution économique des sociétés. Ce parallèle entre économie et performances des accélérateurs renforce notre proposition que la chronologie des accélérateurs soit analysée comme celle des crises économiques à l'aide d'une loi log périodique. Et effectivement, dans ce qui précède, nous venons de retrouver les caractéristiques de "l'arbre du vivant" qu'ont distinguées les trois auteurs précédents (Chaline, Nottale, Grou 1999, Nottale, Chaline, Grou 2000, Grou, Nottale, Chaline 2002): structuration en niveaux hiérarchiques, sauts régis par du "hasard contraint", contraint par des interactions avec le milieu extérieur et également, déplacement du pole de développement. De son côté, Sornette a fait apparaître dans l'étude de divers krachs boursiers, mais aussi dans celle des séismes, ce mécanisme d'accélération et décélération avant et après "la crise" (Sornette 2002), ce terme adopté dans ces conjonctures fort négatives devant être remplacé dans le cas général plutôt par celui de "transition".
Regardons le parallèle existant entre l'évolution des accélérateurs étudiée ici et celle des krachs étudiée par Sornette. Entre les années 1920 et les années1950 les physiciens sont en compétition entre eux, se copient, s'imitent pour rechercher le meilleur mode d'accélération de particules et développer leur discipline, la physique nucléaire. Dans une telle situation, pleine d'optimisme, les idées et propositions fleurissent dans un milieu que nous avons décrit comme déjà hiérarchisé et que des contre réactions (traditions, conservatismes, finances, objectifs….) vont pouvoir modeler. C'est alors, en usant de termes d'économistes, que se manifestent des "bulles spéculatives" lors de la conception quasi simultanée de plusieurs types de machines (bêtatron, linéaire à protons, cyclotron..). La majorité de ceux-ci ne pourra que se développer médiocrement (si l'on se réfère au seul critère de l'énergie des projectiles !). Finalement, les synchrotrons grâce à leur capacité à atteindre de hautes énergies resteront les seuls pour jouer le premier rôle dans ces disciplines car ils peuvent évoluer et s'adapter. Preuve de leur succès, nous avons souligné plus haut que l'intervalle de temps entre la conception et la réalisation des premiers synchrotrons a été exceptionnellement bref. Il y a eu émergence du meilleur accélérateur possible. Au cours des années 1950, le nombre d'accélérateurs des différents types en usage dans les laboratoires est maximum dans le domaine de la physique nucléaire et corpusculaire; ils n'ont pas encore été intégrés dans d'autres disciplines. Après ces années 1950, après la création des grands laboratoires nationaux et surtout internationaux, quand la course aux hautes énergies est bien lancée, quand la physique des particules est sur les rails, on constate un phénomène de relaxation dans la réalisation des accélérateurs: seuls quelques grands laboratoires et quelques équipes de concepteurs subsistent dans la recherche des hautes énergies; les moyens et les personnes sont de plus en plus regroupés; bien après la vague de "spéculation scientifique", on revient avec les collisionneurs à une situation plus simple car avec moins de laboratoires et de projets concurrents: le système perd de sa complexité en se compliquant sur le plan technique; certains disent qu'il rajeunit pour mieux affronter les difficultés. Remarquons que sur le diagramme de Livingston les courbes d'évolution ont une pente de progression de plus en plus grande à partir des synchrotrons à protons et surtout des collisionneurs. Dans cette nouvelle situation, la recherche nucléaire est acceptée, le spectre de la guerre froide s'éloigne (première grande conférence internationale de physique nucléaire en 1956 à Genève avec la participation des deux blocs européens). La décélération dans la chronologie des sauts innovateurs se déroule d'une façon assez symétrique à l'accélération. Ainsi, comme après un séisme ou un krach boursier, on distingue des "anti-bulles" ou répliques et les intervalles de temps entre sauts sont de plus en plus grands. On se rapproche de la fin de l'aventure à moins que ne s'amorce une nouvelle histoire entraînant un nouvel accroissement de la complexité!
e) La figure 4 présente la variation du nombre d'instructions par seconde disponibles dans les premières machines à calculs, puis dans les ordinateurs, nombre pour un coût de 1000 dollars (Moravec 1998). En 1965, G. Moore en donnant la fameuse loi qui porte son nom (Moore 1965) avait précédemment montré que ce nombre croît exponentiellement avec le temps, doublant tous les deux ans environ. Cette loi est aussi vérifiée pour d'autres composants des ordinateurs, en particulier les disques durs. Il apparaît sur la figure 4 (reprise de Moravec 1998) que la croissance est devenu "super exponentielle" pour les ordinateurs à partir des années 1975 environ. Il est aisé de montrer qu'une telle croissance fait tendre asymptotiquement l'évolution vers une limite (Sornette 2002). Une application de ce qui précède dans ce texte montrerait qu'une "transition majeure" (vers 2030) est à attendre dans le cas des composants comme nous l'avons constaté dans les exemples cités plus haut. Effectivement, on devine que la microtechnique utilisée pour la réalisation de ces composants d'ordinateurs devrait bientôt atteindre ses limites se heurtant à des difficultés incontournables (bruits, effets thermiques..) pour être supplantée par les nanotechnologies.
Sur cette même figure a été rajoutée l'énergie atteinte à chaque innovation majeure visualisant la constatation de Livingston que le gain en énergie (dans le système du centre de masse) croît également exponentiellement, ce qui est à rapprocher des résultats des figures 3. Par contre, si l'on porte sur la figure de Moore, non l'évolution des sauts d'innovations mais l'évolution de l'ensemble des accélérateurs de la figure 1, il apparaît que cette évolution est aussi super exponentielle (fig.4). En conséquence il est raisonnable de prévoir que bientôt la technique des synchrotrons et des collisionneurs avec leurs très longs tunnels garnis d'aimants et de cavités accélératrices cryogéniques aura, elle aussi, atteint ses limites de faisabilité.

Figure 4 : Nombre d'instructions par seconde des ordinateurs (points noirs) pour un même coût de 1000 dollars (de 1997)..
Les droites en pointillés donnent l'évolution des données si on ne tient compte que des ordinateurs construits avant 1975 et avant 1995. Les cercles ronds représentent l'énergie des accélérateurs après chaque saut majeur défini dans le tableau I. La droite est obtenue par optimisation du khi2. La courbe représente l'évolution moyenne de tous les accélérateurs présents sur le diagramme de Livingston.
Dans cette étude nous avons voulu avancer des arguments prouvant qu'une loi log périodique semble parfaitement adaptée pour rendre compte de la chronologie des accélérateurs depuis un siècle. En conséquence, en utilisant le formalisme exposé plus haut, la prochaine innovation majeure devrait déboucher sur un nouveau type d'accélérateurs vers 2037, dans trente ans environ. C'est loin ! Le coût d'un nouveau type d'accélérateurs dans la lignée du LHC est considérable. Sa réalisation s'annonce très difficile dans un contexte international peu aisé. Donc l'avenir sera sombre si la physique des particules n'a pas de grands projets.
Par contre, si cette discipline est toujours très productive, il y aura des pressions pour proposer une nouvelle technique d'accélération et de nouvelles voies de recherche. Un nouveau démarrage serait annoncé évidemment par des "précurseurs" ! Ceci est à rapprocher de l'information suivante: Au mois d'Avril 2007, des physiciens de Stanford ont annoncé dans la revue Nature les premiers résultats concernant une nouvelle technique par interaction d'un faisceau d'électrons avec un plasma. Certains spécialistes ont alors estimé que cette invention serait exploitable d'ici 25 à 50 ans ! Par contre, dans les structures scientifiques internationales appropriées (ICFA), un projet de "collisionneur linéaire international", le ILC, a été présenté récemment pour prendre la succession du LHC (certains ont alors parlé du dernier accélérateur de particules). Si elle voit le jour, ce sera une machine nouvelle, plus moderne certes, mais non issue d'une innovation majeure comme celles que nous avons classées ici. Il y a une autre incertitude: Sornette d'une part, Grou, Chaline, Nottale de l'autre ont indépendamment montré (Sornette 2002, Nottale, Chaline, Grou 2000) à partir de différentes données économiques que vers 2050-2080 +/- 30 ans, un changement radical (une crise majeure ?) perturberait l'économie de nos pays. Cette transition s'étendrait sur une plage de temps assez vaste; on pourrait même y être déjà confronté. Dans ces conditions il n'est pas certain que dans trente ans la situation soit favorable pour que naisse une toute nouvelle technique d'accélération.
Cependant l'interrogation essentielle reste l'origine de cette loi log périodique. Qualitativement une réponse peut être avancée, "l'optimisation sous contrainte". Nottale définit celle-ci ainsi: après une première dilatation générique qui a permis l'optimisation, il y a, à nouveau, blocage du système, qui se trouve ramené à l'état précédent, au facteur d'échelle prés. Le problème étant le même, la solution est également semblable, si bien qu'une nouvelle itération peut opérer (Nottale 1998). Par contre, quantitativement, un certain nombre de suppositions ont été avancées (Sornette 2002, Nottale, Chaline, Grou 2000) pour ébaucher des lois qui expliqueraient comment le hasard peut conduire à des structures ordonnées déterministes. J-F. Gouyet a donné quelques exemples de "hasard contraint"(Gouyet 1994, Vicsek T. 1994); en particulier le modèle de diffusion par agrégation limitée, dit DLA (Diffusion Limited Aggregation), de Witten et Sander est souvent cité pour souligner l'importance de la mémoire dans les processus. Ainsi, ce modèle explique comment des particules d'une solution colloïdale viennent se fixer sur la structure existante pour aboutir à un agrégat d'une arborescence fractale. Or, dans la chronologie des accélérateurs circulaires, on note, également, un effet de mémoire: les innovations, depuis le cyclotron, se sont succédées dans le cadre d'un même schéma technique (rotations des projectiles dans un champs magnétique pour multiplier les accélérations par un champs électrique), chacune de ces innovations s'appuyant sur la précédente comme l'a montré l'historique donné plus haut. Mais pour espérer avancer une justification théorique, il faut certainement rassembler de nombreuses autres études comme celle-ci. Ce travail, après les travaux cités plus haut (Nottale, Chaline, Grou 2000, Chaline, Nottale, Grou 1999, Nottale, Chaline, Grou 2000, Grou, Nottale, Chaline 2002) et quelques autres, doit donc être compris comme préliminaire.
