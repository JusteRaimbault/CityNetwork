Geographical Information Systems (GIS) are used to record and analyse spatial data, retaining the spatial semantics. Temporal GIS (TGIS) attempt to provide the same data integrity for spatial and temporal semantics. Halls and Miller (1995, 1996), working with archaeological data, proposed an approach to TGIS which was capable of handling spatial and temporal uncertainty. The objective was to permit identification (rather than simply modelling) of interrelationships in data over time and space.
Langran (1992) notes that temporal data are rarely collected under a regular sampling strategy. Bagg (1996), too, draws attention to the imprecise nature of much of this data. This imprecision has implications for other studies - whether one event influenced another, what the interactions were between people or environments. Adding to the complication, the data are often incomplete and fail to include all of the factors which shed light upon control rates and directions of change. Halls & Miller (1995, 1996) and Peuquet & Wentz (1994) note that TGIS needs to take this variable temporal precision into account.
Yeh and de Cambray (1995) and Yeh and Feautier (1998) handle uncertainty. Their representational approach, however requires there to be less uncertainty than is our experience in analysing historical, especially archaeological, data. We believe that we need an heuristic approach that will also provide a mechanism to measure confidence in the results. Halls and Miller make some provision for such an approach in their proposal.
This paper reviews existing literature on TGIS, in the light of the need to support exploratory use, as well as modelling, of imprecise spatio-temporal data. It proposes an approach using neural networks for the analysis of trends and identification of possible interrelationships. The motivation lies not only in the archaeological research of Miller and others, but also in ecological research into the triggers of change in plant and animal communities, and environmental research into, for example, the causes of flooding.
Miller (1996) seeks to understand the sequences, in time and space, of human occupation of the City of York. The archaeological record is fragmented as a result of changes in the patterns of human activity over time, and the destructive activities of peoples developing previously-used sites. Also, parts of the medieval core of the city have been occupied and re-occupied continuously for four or five hundred years. Archaeologists must piece together the history of a site from fragmentary evidence. It is desirable to have an understanding of what may be at a given location, in order to aid the planning process should that site become available for development.
In bracken ecology, Scott (1993) is concerned with the plant's continuous spread. The data record the extent of stands of the plant over a period of time, together with details of soil, climate and management. TGIS would facilitate the modelling of biological processes in order to gain a better understanding of factors which are significant in controlling reproduction and spread of the plant. Scott identifies a need for a system that is capable of modelling continua within data.
White, et al (1999, personal communication) are working on the incidence and spread of bovine tubercolosis in the European Badger population of Great Britain. Because of the nocturnal habits of this protected species, and the nature of current techniques for determining whether any animal has contracted the baccilium, determination of infection in the wild population is difficult. Most direct evidence is from reported road kills, leading to irregular records of incidence over both time and space. Some indirect evidence may be available where it is possible to show that badgers are the source of infection of a herd of cattle, but this this evidence is temporally imprecise. White and Harris (1995) propose a model to predict movement of individuals between social groups of badgers. Their model does not fully explain islands of non-incidence, nor the apparent restriction of incidence of the disease to South West Britain. A TGIS application capable of searching for potential interactions presents one possible way forward.
The remainder of this paper is structured as follows : Section 2 lays out the requirements for temporal GIS ; Section 3 reviews the work done in developing GIS to handle time and space and also work in spatio-temporal representation ; Section 4 presents a mechanism for representing rates and directions of temporal variance, which is developed and mapped onto the AURA neural network implementation in Section 5. Conclusions are presented in Section 6.
Geographical Information Systems, GIS, are derived from a number of backgrounds and disciplines. The need for cartographers to automate the more tedious parts of their work, and the traditional requirement of geographers to overlay spatial representations B maps B of different phenomena, means that the GIS concept is focussed around two dimensional representation. Even so, from the earliest days disciplines concerned with altitude and depth, volume, shape, together with descriptive data, have been seeking techniques to assist them in their research work. Many of these disciplines are concerned with change over time B in terms of the environment, climate ; the development of civilisations, plant communities, rocks. Researchers seek to understand, from a necessarily incomplete record, the processes that have led to the development of the observed phenomena. Many need to understand these processes in order to offer explanation or prediction of the effect of recent change on these phenomena, or their contribution to other phenomena. Other workers are concerned with recording present change, with the creation of an audit trail from which to identify precise instants of change. Although primarily concerned with cartographic display, Muehrcke (1978) identified a set of analytic questions which characterises the study of change :
Qualitative change : what changed ?
Quantitative change : by how much did it change ?
Composite change : the process of change
Space-time ratios : descriptors for the mapping between space and time
Different disciplines have different understandings of change, and therefore different requirements. For example, planners and cartographers are concerned with the maintenance of up to date and accurate information. Their data management needs embrace data integrity and data security. From a temporal point of view, they are transaction focussed, and require facilities for rolling back to a previously certified version of their data. Others, such as meteorologists, ecologists, social scientists, archaeologists and geologists, are more often concerned with seeking to understand past, present and future processes. They are concerned with interactions of processes, and may be described as process focussed.
Current temporal techniques are effective for prediction in fields such as meteorology and fluid dynamics, where existing physical laws allow future values and locations to be calculated with reasonable accuracy. A case in point is Miller's (1994) model of groundwater pollution. The allied concept of versioning, which provides roll-back access to data change, is appropriate to evolving collections of data, for example the UK Land Registry.
Commercial GIS support provides little to no support for research in which temporality is a required axis (Johnson, 1997, Walia and Georgopoulos, 1997). Candy (1995), building onto Arc/Info, and Johnson (1997), working with MapInfo, have proposed temporal extensions to existing packages. The lacunae are in analysis and presentation tools, and, more fundamentally, in terms of the provision of specific data structures. Research tools with a temporal perspective tend to use a time-slice structure based upon cellular automata techniques (Codd, 1968, Tobler, 1979), in which data represent precise snapshots of the phenomena, frozen at a specific time. This use of timeslicing is seen in the PCRaster system developed at Utrecht and used by Heil and van Duersen(1996) and Wesseling et al (1996), the technique also adopted by Candy (1995) and Johnson (1997). Time-slice techniques provide support for audit trails, using versioning, as defined by Newell (1994). His definition of support for research involving continuous spatial change is in terms of fixed-time representations and report-generation. Time-slicing may also be suitable for predictive work such as tracking weather systems, in which a series of spatio-temporal data samples can be extended in to some imagined future, following some perceived or formally defined trend.
Where the inter-relationships between the various represented phenomena are known and understood, the version, time-slice, or one of their derivatives, such as Langran’s space-time composite (Langran 1992), offers a viable approach. Where these relationships are themselves the object of study, where the requirement is to enable study of uncertainty, and especially where there is an element of doubt about any of the measurements, these techniques are less appropriate. Time-slice approaches are ill suited to investigations of the past where the available data inevitably refer to irregularly spaced - and often imprecise - units of time and/ or space.
When trying to understand processes and the 'drivers of change', it is temporal, spatial, and other interactions that are of concern : "Is this change likely to be a / the trigger for that change ?". This may require visualisation capabilities (eg Openshaw et al 1994), but there is also a need for objective identification of the potential interaction. When working with spatiotemporal problems it seems reasonable to expect (temporal) GIS to be capable of supporting such analysis.
Langran (1992), in what remains the seminal work, defines TGIS in terms of the search for answers to the following questions.
Where and when did change occur ?
What types of change occurred
What is the periodicity of change ?
She suggests that TGIS require "a conceptual model of spatial change, treatment of aspatial attributes, data processing logistics, a spatiotemporal data access model, and efficient algorithms to operate on the spatiotemporal data" (Langran 1992, p4).
Langran's review of attempts to develop a spatio-temporal database technology concluded that most work had been geared to the needs of the CAD industry, for example, Katz et al, (1986), Beech & Mahbod (1988). Temporal control in CAD maintains the 'original' version at each update such that any set of updates can be wound back, and to provide quality control. For many cartographic applications of GIS this is what is meant by TGIS. Langran states that this approach does not provide an adequate technique for identifying spatial change. Exploring the role of maps in aiding an understanding of geographic process, she quotes Borchert :
"Many time series of maps are in one sense statements of theories, in cartographic language, about geographic development processes, about the functioning and the past and future evolution of some global or regional system. Interpretations of the map patterns involve logical interpolation or extrapolation from mapped observations, in both space and time. Distinctively geographic models are also cartographic generalisations. As four-dimensional descriptions of the geographic evolution of resource and settlement systems, time series of maps are a fundamental element of geographical explanation." (Borchert 1987, p388)
Conventionally, cartography is about the mapping of boundaries, and Langran suggests extending this concept from space to time ;
"While temporal boundaries are no more discrete than spatial ones, sharp lines also prove useful in their representation...temporal episodes can simplify time" (Langran 1992, p30)
Nonetheless, it would appear that the majority of work in this field immediately subsequent to the publication of Langran’s book has concentrated on the approaches of versioning - for example Worboys (1993), MacEachren (1994), Maguire (1994) and Newell (1994).
Ellis (1994), recognised some of the problems inherent in a version-based approach temporality, reiterating that "the present debate on TGIS needs to consider that modelling the past is subtly different from modelling future change" (Ellis 1994). Indeed, the version-based approach actually models time, rather than change over time.
Throughout the history of geography, workers have studied temporal progressions in an analytic manner. Widely cited as the first instance of spatial analysis, the study of London’s cholera epidemic by Dr John Snow (1854, see Tufte (1990), and Unwin (1996)), was a spatiotemporal analysis. By considering time of onset of the disease against location, Snow was able to identify the infected well at the source of the disease. Subsequently, the studies of diffusion (eg Hagerstrand 1952), have in turn led to the ideas of 'Cellular Geography' (Tobler 1979) and 'cellular automata' (Codd 1968 ; Couclelis 1985), together with Hagerstrand's more qualitative study of the effects of time and space on people which he termed 'Time Geography' (1970). Time Geography focuses on the concept of society evolving over time, influenced by intersecting external activities. This is relevent in archaeological study of the development of past societies, and also for many studies in the environmental disciplines. Study that seeks to understand the processes that have brought about a present environment, and especially study that seeks to predict future development, seek a similar method.
Langran's implementation of a TGIS, based on her findings, is a cartographically-oriented system which defines a temporal topology of objects and features based, ultimately, upon the "space-time composite" (Langran 1992, Chapter 4). Langran states that the intersection capabilities of the space-time composite are vital in effective error detection. This approach is suitable for small areas with short temporal spans, but we question its efficacy when applied to realistic geographic data volumes since it remains time-slice based.
The past few years there has been a search for a spatio-temporal logic, a search for a mathematical definition of the processes that are the subject of analysis and modelling. For example, Hermosilla (1994) starting from a CAD approach, develops a unified temporal and spatial reasoning based upon temporal databases, knowledge bases and artificial intelligence. Such a logic would enable the logical expression of such questions as
how has this area changed over the last five years ?
has this feature moved ?
have these two features ever been juxtaposed ?
It would also provide support for what Langran terms 'time modeling' or 'cause and effect', scheduling, or 'triggering'." (Langran 1992, p56)
Taking a rather different theoretical stance, Openshaw (1994) proposes a tri-space concept comprising three core 'data spaces' : geographic or locational information, temporal details, and other attribute data (Openshaw 1994, p88). He seeks algorithms to explore the data spaces simultaneously, without preselection. For Openshaw's specific applications, crime and disease, the purpose of the research is to detect patterns of commonality across time. This is an interesting approach for such analytic tasks in which known patterns are sought and Openshaw's Space-Time Analysis Model (STAM) warrants closer study for this reason.
Peuquet, Qian and Wentz (1994), and subsequently Edsall and Peuquet (1997), and Qian and Peuquet (1998), take a similar line to Openshaw in their definition of the TEMPEST TGIS. They recognise that a major problem for TGIS is the variability in the (temporal) interval between events to be recorded in the GIS. Their solution introduces an elasticity into the 'time line' so that events can be recorded as knots or singularities ; similar solutions are proposed by Yeh and de Cambray (1994), and Halls and Miller (1995, 1996). TEMPEST has been taken further, by Qian, et al, (1997), with the definition of a logic for operations to support visualisation and analysis of spatiotemporal data. Qian and Peuquet (1998) design a spatiotemporal query language. This work derives a set of operations, which they term Universal Space/Time Operations. These are decomposed into sets, elements and operations. The query language is object-oriented, based on data flow (Qian and Peuquet, 1998). This presumes that the object relationships are not themselves the focus of study.
From the study of transportation, and seeking to determine an algorthmic framework to support temporal planning applications, Claramunt and Bai (1999) explore the propogation of temporal constraints. They seek a generic abstraction level that will support predictive modelling for strategic decision making. Their algorithm represents each constraint as a binary tree in order to determine the temporal interval appropriate to that constraint and to propagate it into the overall timetabling structure. Whilst demonstrably applicable to the transportation scheduling arena, this does not appear to offer anything to the generic problem of temporal GIS, nor to the analysis of (partially) historic phenomena.
Much research relevant to TGIS development takes place under the more general auspices of database research. Spatial and temporal issues for databases are considered independently, from theoretical and practical standpoints. Some research also combines temporal and spatial aspects.
An important area of research, not yet reflected in commercial GIS, concerns data types which preserve the semantics of spatial and temporal data. Spatial literature explores approaches to the storage and retrieval of spatial data in ways that accurately reflect the spatial reality. A base line for this work is represented in the papers of Gunther & Lamberts and Worboys contained in the thematic issue of the Computer Journal (Vol. 37 no 1 1994). This culminates in theories of spatial data types, eg Schneider (1997). The research assumes a two-dimensional space, although Kuijpers and Van den Bussche, (1999) introduce a logic based on cones to reason about topologies in three dimensions.
Temporal database literature includes theories of time, of which one of the more extensive is Goralwalla, Özsu, and Szafron, (1998). As well as general instant and interval temporal primitives, this introduces concepts of anchoring, continuity and determinacy :
"Y there are cases when the knowledge of the time or the duration of a particular activity is known only to a certain extent. For example, we do not know the exact instant when the Earth was formed though we may speculate on an approximate time of this event. Y. it is desirable for such a model to be able to capture both determinate and indeterminate temporal primitives." (Goralwalla, Özsu, and Szafron 1998, p6).
The authors construct a model of time based on these primitives and implement a toolkit in C++. Although they make some reference to spatial problems with time, these are not developed.
Research often focuses on or is constrained by the problem of dimensionality. The traditional approach of mathematicians is to reduce the number of dimensions until the problem is manageable. This arises in various researches, typified by Claramunt and Jiang (1999). Tsotras, et al (1997) present a theory combining three spatial and two temporal dimensions, with conventional attribute data. However, simplification of the spatial dimensions loses critical semantic information. If it is relevent to the study to know distance and direction between objects of interest, these dimensions must be preserved in the database. Many workers, geologists and archaeologists for example, are concerned with volume and depth. They are concerned with spatial relationships surrounding each object of interest, as well as over time.
Studies of movement motivate much spatio-temporal work. Galton (1995) looks at movement as requiring integration of theories of time, space, objects and position. The combined theory, which he refers to as a theory of dominance is presented as a tool for investigating the structure of qualitative space-time in an underlying continuous space. Elmasri and Lee (1998) cover relational and object-oriented temporal data storage. Their temporal approach extends that of Galton with concepts of temporal anchorage and determinacy. This is a necessary extension for any research into interrelationships, but the technique of these authors presumes that the interrelationships are already understood and modelled.
Research into temporal query languages tends to derive from work on the extension to spatio-temporal dimensions of conventional relational databases (RDB), for example, Tansel and Tin (1998) ; Snodgrass et al, (1998), and TIMECENTER, Denmark (http ://www.cs.anc.dk/research/DBS/tdb/TimeCenter/). This work is closely related to the transaction-audit-trail-related proposals of the ISO Temporal Query Language (TSQL2) Working Group. It presumes a versioning approach to TGIS, using time-stamping. Böhlen et al, (1998) discuss the integration of data from conventional SQL-92 DBMS applications in spatio-temporal structures, and propose STSQL, a spatio-temporal query extension to Oracle SQL-92. The authors claim that their work is extensible to many temporal dimensions.
In recent years, a small number of studies have appeared which present a coherent, new, approach to spatio-temporal support. Renolen, (1998) devises a complete set-based theory of temporal objects, which he asserts can handle multiple temporal dimensions, although current publications are restricted to two spatial ones. Erwig et al (1998) propose a less-general approach using the concept of abstract data types (ADT) for temporal objects. Spatio-temporal objects are a special case of temporal objects. The paper considers two approaches to embedding spatial and/or temporal data in RDB. For either, objects could be embedded directly as attributes referencing the relevant ADT type. The spatial alternative is to record co-ordinates as conventional data attributes. For temporal data, the alternative is to store a time stamp and a flag indicating the future behaviour.
Causality and the analysis of spatio-temporal data is explored by Allen et al, (1995) in the form of a generic model for representing causal links in spatio-TGIS. The review notes that causal logics are nonmonotonic (subsequent information can change the truth of existing information). The frame and qualification problems arise in relation to large sets of propositions. However, much of the work referenced in this section is system development rather than data analysis work. The work does not cover the interaction of causal events ; it essentially models causality in real problems, rather than providing a basis for detecting potential causes from trend analysis of data. In this respect, data-mining literature proposes the use of inductive logic programming (Popelínský, 1998), genetic programs (Martin, et al 1998) and trained neural networks to detect trends in data (Matsumoto, 1998). Examples relate to conventional and spatial databases, constraint databases and to time-series data derived from spatial as well as temporal sources.
Yeh and de Cambray (1995) seek to model the behaviour of entities (things of interest) in both space and time, using mathematical functions to capture the observed semantics of entity variability. They describe how these can be used to derive geometric representations. This approach introduces the unit-of-time sequence (UTS) to represent an observation of a phenomenon over a unit of time. The UTS is a quadruple of an identifier, some spatial or aspatial variable value, a time unit, and a behavioural function B a known law or one derived from the shape of an interpolated curve. The behavioural function models the entity's behaviour throughout the UTS time unit. A temporal sequence of UTSs for an entity form a behavioural time sequence (BTS). This powerful approach allows the functions to take many forms (punctual for events, stepped, linear or spline - the latter is also suggested by Erwig et al (1998) and by Halls and Miller (1995, 1996), whose version was partially implemented by Burbidge (1998)). At the join between UTS, functions may be continuous (representing continuity of observed behaviour), or discontinuous. Discontinuities, referred to as accidents, equate to knots in Halls and Miller (1995, 1996), and in Peuquet et al (1997, 1998).
Evolution, development, growth or decay, are readily represented by behaviour functions and BTS. These can be visualised as plotted curves, in which a knot or accident is a point at which the smoothness of the curve is interrupted, representing an altered rate of change (noting that the scale of measurement or representation can affect the visual identification of knots).
Yeh and de Cambray illustrate their approach by modelling the movement of a herd of cows. The approach is viable if it is known that the observations relate to a herd. However, if the data were observations of the location of a single cow (or even a part of the herd), it would not be possible to determine whether the observed phenomena represented one cow in different locations over time, a herd in one place over time, or a moving herd. It is fundamental to the model presented that the observed cow represents a herd of determined size. When working with historical data it is often not possible to determine how the observed sample represents the whole, and, indeed, potential relationships may be the subject of the research. Many curves might be interpolated from the same data points, representing different levels of certainty in the position of these data points. Halls and Miller (1995, 1996), and the current authors, are interested in precisely this problem, of how to work with imprecise data and underspecified relationships.
The work of Yeh and de Cambray (1995) is extended by Yeh & Feautrier (1998) to the manipulation of multi-dimentional spatial data. The representation is polyhedra, which permits the use of a unifed manipulation algebra. Spatial dimensionality is expressed as cartographic 2-D representation and primitives. The Chernikova algorithm (Chernikova 1968) is used to define the intersection, union and difference of the polyhedra, but is limited to convex representations. However, this extended model allows spatial phenomena to merge and divide, extending the expressive power of the earlier models. A prototype implementation, GeoLis, is described This is a mathematically elegant but complex approach. The mapping between the polyhedra operators and the spatio-temporal questions of what happened here then, when and where did that change, or Adid anything else change around that time and place is not clear.
Halls and Miller (1995, 1996) propose a mechanism for recording the rate and direction of temporal variance, with an assessment of confidence in any measured point. The approach is similar to that of Galton (1995) or Elmasri and Lee (1998), above. Halls and Miller, however, assume incomplete and inexact data. From the data, possible states of the model at any past time can be interpolated, using splines and recording the functions or equations for these, after the manner of Yeh and de Cambray (1995). Rather than considering UTS sequences, Halls and Miller derive a single spline curve for a prolonged period (referred to as a worm) based on the temporal nodes (todes). This is analogous to the timeline concept of Peuquet et al (1994). A single curve might model a number of different equations, and display the same discontinuity representations as occur between UTS. However, the continuous spline can be manipulated, using the associated data on the relative precision of each tode. Todes are considered to exert a 'pull' upon the curve, with the strength of the pull related to the subjective precision assigned to the recorded attribute value. For example, a tode recorded as +/- 1 year would pull the curve towards it with greater strength than a tode in the same series defined as +/- 50 years. Statistical descriptors can be used to describe the precision of the spline fit. The insertion of additional records into the set requires recalculation of the curve, and the update of the stored curve parameters (eg equation and/or gradient) for each tode.
In the current work, and that of Halls and Miller (1995, 1996), discontinuities in the gradient of a curve (knots) become the focus of analysis. Although some knots represent lack of precision in the todes before or after the discontinuity, others suggest the presence of temporally-related events that could be of interest to the researcher. Halls and Miller also suggest the comparison of curves representing different factors varying over time might allow detection of proximal temporal and / or spatial changes, or suggest possible effects of changes over space and time within the object under scrutiny. The approach is not modelling time but the rate of change over time ; time is a constant dimension.
The approach described by Halls and Miller has been explored by two subsequent Masters projects, McKay (1996, supervised by Halls), and Burbidge (1998, supervised by Halls and Polack). Studying bracken ecology, McKay (1996) used existing ARC/INFO databases. He added representation of the temporal value (a date, for example) and precision of values. This simple approach allowed McKay to correctly identify known points in time relating to the management of areas of bracken. In addition, McKay’s implementation generated one knot that was unexpected. This point was found to correlate with management activities on an adjacent plot of heather (burning). Subsequent work has demonstrated that this is a previously unrecognised influence in bracken ecology (Scott, 1997, personal communication).
Burbidge (1998), following up McKay’s work, attempted a more rigorous implementation using environmental data B rainfall, river levels and flooding for the Yorkshire Ouse catchment. The technique correctly identified the time points at which rainfall and river flow rates approached those critical for the onset of flooding. Burbidge noted that many minor data variations resulted in knots ; a simple visual analysis of his curves was not feasible. The project explored data smoothing to remove the less significant knots and concluded that automated discrimination was needed.
Knowledge based computing research is currently dominated by activity focussed on neural networks. In particular, neural networks are gaining favour for pattern recognition and classification applications (Fitzgerald and Lees, 1996, O. Keefe and Austin, 1999). Neural networks have the ability to handle fuzziness, especially in the implementation form adopted for the AURA architecture (Zhou et al, 1999). The AURA architecture handles partial match database queries very well and has also been used to implement statistical classifiers in a fast and robust way.
The discussion above has identified the principle concern as being the identification of possible causal relationships between variables. This section presents an analysis of the spatio-temporal geometry implicit in Halls and Miller's approach to todes and worms, and outlines a means of addressing this requirement by application of the AURA neural network techniques.
Assume that there exists a large body of data, in the form of irregular points representing some continuous process or processes. The data should be represented by a model that expresses the temporal geometry of the process in an appropriate way. The model should describe the evolution of the process over time for any arbitrary point in time.

Such a technique is proposed by Halls & Miller, namely the modelling of behaviour by spatio-temporal Aworms@ and the corresponding identification of temporal nodes or Atodes@ which indicate which significant changes in the behaviour of the system have been observed.
The question to be addressed is the identification of variables within the body of data that may be causes of change in some variable of interest. The points of change in the current variable are identified by the todes. Similarly, todes in other variables represent events that may have some influence on the behaviour of the current variable. The body of data must therefore be queried to find the related variable or variables that might influence the current variable. The data is queried by looking for correlations between the sets of todes for the current and related variables.
The sequence of todes gives a variable a particular temporal geometry, expressed in the temporal intervals between the todes. Variables that have the same geometry may have a relationship to the current variable. That is, if two variables have the same temporal pattern of todes, it indicates the possibility that either one variable is influencing the other, directly or indirectly, or that both variables are influenced by a third unobserved variable. The influence of one variable on another may not be instantaneous, so allowance must be made for temporal displacements between the geometric objects of different variables.
Allowance must also be made for incomplete data B there may be todes which have not been identified, so any search algorithm must allow for partial or incomplete matches between sequences of todes.
The problem therefore becomes one of attempting to match sequences of todes in one variable with sequences of todes in other variables, with potentially incomplete data for all variables. The measurement of the point in time corresponding to any particular todes may be subject to noise, adding an element of uncertainty to the temporal location of todes.
One approach is to represent the complete sequence as a set of sub-sequences, and to look for matches between sub-sequences of variables. This allows for the possibility of missing data B the particular sub-sequences including the missing point will not be matched, but the remainder will. As an example, consider a sequence of todes e and intervals between them t (figure 1). We can take sub-sequences <e1,e2,e3> of todes, and represent each sub-sequence by the pair of intervals < t1,t2>. In temporal-geometric terms, the sub-sequence is a point in a 2-dimensional space spanned by the intervals t1,t2 (figure 2).

Figure 2 : Single ubsequence represented as a point in query space
The whole sequence is then represented by the set of interval pairs {< t1,t2>}. This set of interval pairs is represented by a set of points in the 2-dimensional space (figure 3). The data for the whole collection of variables is represented and stored in this manner. Note that by storing sub-sequences of events, the ordering of the whole sequence is not preserved. That is, the information that sub-sequence a precedes sub-sequence b is not stored. The set of points for the current variable thus becomes the query with which to extract matching variables from the data. The space in which the sub-sequences of data are embedded is referred to as the query space.

Figure 3 : Complete series represented as a set of points in query space.
The appropriate dimensionality for the query space is crucial. If the data for each variable were stored as a set of intervals between one point and the next, the probability is high that another variable will have some similar intervals between events. Querying the database with such a general query would not produce much useful information. As the length of the sub-sequence is increased, more structure is included in each point comprising the query set and the precision of the query increases. In the example cited above, the data are represented as pairs of successive intervals, giving a 2-dimensional query space. Triples of successive intervals would give a 3-dimensional query space, and so on. However, longer sequences are less likely to match exactly because of the possibility of missing todes, and the uncertainty in the measurement of intervals between todes.
A suitable mechanism to implement this form of representation and query is provided by the AURA system (Austin, et al, 1995a, 1995b). The AURA system is based around the use of correlation matrix memory (CMM) neural networks and the associated techniques for managing inputs and outputs to the CMMs. The CMM operates by storing relationship between inputs and outputs, and retrieving outputs when presented with an input. The input may be inexact or noisy, in which case the CMM will retrieve the closest matching output from those it has been trained with. AURA systems also support parallel access of the information stored in the CMM. By suitable coding of the inputs, a set of queries may be presented together to the CMM, and the output will be the superimposition of the corresponding outputs, with some element of noise added. AURA systems (and their predecessors) have been used in a number of processing tasks, including image processing and molecular matching (Austin, 1995, Lees, et al, 1995, Austin, 1997, O'Keefe, and Austin 1999). The AURA system can store and access very large numbers of associations (of the order of millions of associations). Training time for the system is very short, compared to other neural networks ; each data item need only be presented once. Retrieval time is dependent on the size of the database stored, but can be accelerated using hardware developed at York (Kennedy and Austin, 1997).
The CMM is a single layer neural network with a matrix of binary weights M. Inputs and outputs are in the form of vectors. Each element of the input vector is an input node, and each element of the output vector is an output node. The network is fully connected, in that the elements of the matrix represent the weights from each input node to each output node. The weights are binary, that is they represent the presence or absence of each particular connection. The input and output vector are binary, that is each element of the vector is represented by a single bit. For most applications, some form of mapping is required to transform the inputs and outputs into vectors having this form.
In the training process, each input vector pi is associated with an output vector or separator si. The CMM learns their association by performing the following logical ORing operation :

In a recall process, for a given test input vector pk, the CMM performs

followed by a threshold operation on vk to reduce it to a binary vector. The vector vk itself has integer elements, the value of each element representing the strength of activation of that element. The threshold process sets to zero elements that are not strongly activated, removing noise from the output to leave a pattern of elements representing the required output. The thresholding process may apply either a fixed or an adaptive threshold, depending on the properties required of the memory. Fixed thresholds are used where the properties of the input string are known and an exact match is required. If an approximate or best match is required, an adaptive threshold is used. In practice, a number of superimposed binary codes will be generated by the recall and threshold process. These overlapped codes represent the set of possible matches to the query. Given a suitable orthogonal coding of the outputs separation of the responses is not difficult.
The raw input values (points in the query space) must be encoded. The encoding process maps each value into a sparse binary string that forms the CMM input. To do this the continuous values are quantized into a set of intervals. The division of the input variables into quantization bins may be performed based on the data stored in the CMM. An algorithm that allocates bin boundaries such that the given data are distributed uniformly between the number of bins is discussed in (Zhou and Austin 1998).
Each quantization bin is represented by a distinct token. The actual data values are mapped into quantization bins, and represented by the corresponding tokens. The quantization bins are of non-uniform width, narrower bins being formed where the training data are more clustered. For a given number of bins a greater precision is obtained where values are clustered, and less precision is obtained for values which are infrequent. The mapping of input values to bin tokens is a pre-processing stage, converting from the input data to the bit patterns that make up the input to the CMM. Thus, where the data represent intervals or sets of intervals from the query space, the variable quantization algorithm will represent more precisely those intervals that occur frequently, and less precisely the intervals that are less frequent. This algorithm may therefore automatically take account of the different timescales at which events occur, and act with greatest precision on the scale or scales at which most observations are found.
Each point in the query space can be represented by the set of projections onto the basis of the query space, that is by the set of intervals defining the point. There are a number of ways to reduce the query space to a binary vector. In particular, to preserve the relationships between the values of each interval, the values may be bound together using a tensor product variable binding (Smolensky 1990).
The CMM should store the relationship between each sub-sequence in the data (that is, each point in the query space which represents a sub-sequence) and the identifier for the variable that the sub-sequence is part of. Each variable may be identified by a unique binary string. If required, the identifier for the variable may be combined with an index indicating the absolute time from which the sub-sequence is taken, by binding the variable name and the absolute time together.
Thus for the formulation of the query problem given in Section 5.1, a potential solution has been outlined. We have a set of inputs, each of which represents a sub-sequence or element of the temporal geometry. The outputs required are the identifier for the variable, and possibly the absolute location of the sub-sequence. The encoding used for the query takes each point in the query space and encodes it as a sparse binary string. These strings are then superimposed, to form a single string that represents the whole constellation of points in the query. Presentation of this query will prompt the recall of the identifiers for each stored sub-sequence that matches some point in the query. The relative abundance of identifiers for each variable can give an indication of the quality of the match between the query and the stored data.
As AURA stands at present, support for spatio-temporal matching can be implemented by the construction of the pre-processing stage, to convert query space inputs to binary strings. Whilst expected to be straightforward, amongst decisions that will need to be made is the extent to which we desire to interface to existing GIS or to effectively generate an entirely new GIS. We will also need to explore changes to the Halls and Miller model in order to support AFuzzy@ matching for uncertain points and to realise their concept of weighted todes, that reflect the degree of change.
Supporting spatiotemporal analysis, where time is treated as a constant axis, and the rates of change, movement, are represented by spline curves, offers an approach that handles uncertainty and imprecision. Such a concept was initially advanced by Yeh and de Cambray (1995), Halls and Miller (1995, 1996), and Peuquet, et al (1994). The initial concept requires extension, in particular to provide query and matching techniques and some measure of intelligent identification of significance at discontinuities of the shape of the curve (Burbidge, 1998).
Developments in the application of neural networks, especially with regard to fuzzy pattern matching (O. Keefe and Austin, 1999) appear to offer both the necessary query facilities and uncertainty.
We are attempting to reimplement the Halls and Miller approach using the constructs used by AURA in order to determine the extent of developments needed to employ AURA technology. It is not yet clear whether to inputs from some existing database structures, and thus GIS, would be practical. The nature of the underlying data structures need further development.
