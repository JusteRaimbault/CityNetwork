La question de la lisibilité de la carte1 est aussi ancienne que la réflexion théorique en cartographie elle-même. Outre son intérêt évident pour le praticien, elle fait en effet partie de la problématique de l’utilisation de la carte, et donc de la définition même de l’objet scientifique carte. La conceptualisation de cette question a connu un fort développement à la fin des années 1960, à la suite de l’utilisation de la théorie mathématique de la communication de C. E. Shannon et W. Weaver (1949). Dans ce cadre, la carte est un moyen de communication, qui possède donc des caractéristiques de support d’une quantité d’information plus ou moins grande, plus ou moins facilement compréhensible par le lecteur/récepteur de la carte2.
On a souvent abordé le problème de la lisibilité cartographique selon l’angle de la complexité, ce qui permet de le simplifier et d’offrir des perspectives de comparaison, d’appréciation, voire de mesure, selon le principe implicite qu’une plus grande complexité amène inévitablement une lecture plus difficile3.
Or la lecture d’une image cartographique met en œuvre plusieurs processus, de perception visuelle tout d’abord, de cognition intellectuelle ensuite, qui sont de mieux en mieux connus, voire modélisés4. La disponibilité d’outils logiciels pour apprécier la complexité visuelle, mais aussi pour la modélisation de la perception humaine des images rend possible un développement des méthodes de mesure de la complexité, en apportant notamment une visualisation directe des résultats, pour une appréciation plus naturelle.
L’objectif du présent travail est donc d’une part de rendre compte d’expérimentations effectuées pour le développement et l’utilisation d’outils de visualisation de la complexité d’images cartographiques. D’autre part, nous proposons d’apprécier l’utilité de ces outils dans l’évaluation de la lisibilité potentielle de cartes thématiques. Cette évaluation présente un intérêt pour le cartographe en cours de travail, notamment l’étudiant, qui cherche à évaluer plus finement les conséquences de ses choix de représentation, du point de vue de la complexité. De plus, ces analyses sont étayées par plusieurs approches théoriques que nous tentons d’articuler et de prolonger par de nouveaux moyens d’investigation. Enfin, tous les outils développés lors de ce travail sont disponibles sous la forme de services Web, et donc accessibles facilement par toute personne intéressée (cf. annexe).
Cependant, le concept de complexité lui-même recouvre des situations multiples et nécessite une définition plus précise dans ce contexte.
L’article de D. Fairbairn sur la mesure de la complexité de l’image cartographique, paru dans The Cartographic Journal en 2006, comporte une très complète analyse de la question et sera notre base pour cette approche d’une définition de la complexité.
Le premier problème dans la définition de la complexité, c’est celui de sa nature par rapport à l’observateur : est-elle objective, extérieure, ou subjective, intérieure, dépendante de l’expérience ? La question est de savoir si la complexité est déterminée par la difficulté de la perception visuelle (nombre, densité, diversité, résolution des motifs et signes de la carte), ou bien par celle de la compréhension du message intellectuel de la carte.
On retrouve dans cette formulation de D. Fairbairn l’opposition entre les notions de complexité structurelle / fonctionnelle, ou complexité visuelle / intellectuelle. On retrouve également ici une nouvelle évocation du double héritage de la carte, artistique (ensemble de formes graphiques, esthétiques) et fonctionnel (représentation d’idées sur la localisation et les relations spatiales d’un territoire donné)5.
Naturellement, la réponse est hybride : la carte procède des deux types de complexité, que l’on peut articuler en se basant sur le processus de lecture d’une carte. Le lecteur rencontre la complexité visuelle tout d’abord, puis la complexité intellectuelle, en interprétant les signes que la carte présente et en les confrontant avec ses acquis et ses objectifs (cf. figure 1). Une définition plus précise des termes utilisés est ici nécessaire, à cause de leur polysémie.
Figure 1 : Types de complexité et opérations cognitives associées

La complexité caractérise ce qui est « composé d’éléments qui entretiennent des rapports nombreux, diversifiés, difficiles à saisir par l’esprit, et présentant souvent des aspects différents. »6
Si l’on prend comme guide l’acte de lecture d’une carte, on rencontre tout d’abord la complexité dans l’opération de perception visuelle. Cette perception est au départ pré-attentive (avant que n’interviennent les opérations conscientes de direction de l’attention) et est traitée par les moyens physiologiques de l’œil et du cerveau. Cette complexité est donc visuelle, et comme elle se base sur des éléments physiques, concrets, qui sont des éléments graphiques (les formes, textures et couleurs dessinées physiquement sur la carte), on peut aussi la qualifier de graphique (type 1). De même, elle est liée à la structure de la carte (par opposition à sa fonction), et D. Fairbairn la qualifie de structurelle. Enfin on verra plus loin que du point de vue de la théorie de l’information, ce type de complexité se caractérise directement par une valeur élevée de densité d’information, d’où le qualificatif d’informationnelle. C. Cauvin et al. parlent de « densité graphique »7, en suivant J. Bertin. A. M. MacEachren quant à lui définit la complexité visuelle comme « le degré de combinaison des éléments de la carte » (cité par C. Cauvin et al.), et il distingue la complexité visuelle due à la répartition spatiale du phénomène cartographié d’une part, et celle due à la symbolisation choisie pour le représenter d’autre part.
L’étape suivante de la lecture d’une carte thématique recouvre en effet la combinaison (et donc la reconnaissance) des éléments graphiques repérés auparavant en unités de figures et de formes, qui sont les signifiants de signes visuels, au sens de la sémiotique. Ces signes peuvent être nombreux, variés, et difficiles à saisir, d’où la possibilité d’un autre type de complexité, sémiotique. La sémiotique mise en œuvre ici étant celle de la sémiologie graphique, on peut aussi qualifier cette complexité de graphique (type 2), à ne pas confondre avec la précédente. Cette étape est intermédiaire du point de vue de l’activité consciente du lecteur : certains signes sont repérables inconsciemment, automatiquement, avec l’aide de l’expérience, d’autres demandent un apprentissage.
Nous arrivons enfin à l’étape de la compréhension du sens des signes repérés à l’étape précédente. Il s’agit de l’interprétation de leur position et de leurs interactions sur le fond de carte, qui doit normalement aboutir à la compréhension du « message » de la carte, si son efficacité est bonne. Cette complexité est donc intellectuelle, conceptuelle, liée à la fonction de communication de la carte. La carte présente donc différents types de complexité, imbriqués, ce qui implique des approches différentes et coordonnées pour les mesurer et potentiellement les visualiser.
La disponibilité d’algorithmes et de composants logiciels de traitement en temps réel des images permet aujourd’hui de concevoir des outils qui offrent une visualisation de méthodes de mesure de la complexité évoquées dans la littérature. Cette visualisation pourra aussi constituer un moyen d’en vérifier l’utilité ou les limites. Le type de complexité visé par ces outils est uniquement perceptif, graphique (type 1), mais on cherchera à en prendre en compte des aspects variés, de la simple densité d’information graphique à la capacité de correspondre au fonctionnement du système perceptif humain, l’adaptation aux processus mentaux de la perception visuelle. On se rapproche ainsi de l’expérience du lecteur de la carte, et donc de la notion de lisibilité.
Les méthodes de mesure de la complexité visuelle citées dans la littérature ont pour résultat une seule valeur numérique, un indice global. Cet indice résume la complexité d’une image cartographique en une valeur numérique, ce qui permet ensuite de comparer des images entre elles, par exemple après modification d’un paramètre de la représentation (généralisation cartographique, changement du style de la représentation d’un type d’information, ajout d’informations géographiques complémentaires, etc.). Cette mesure est donc très utile, mais il peut sembler malgré tout étonnant que n’aient pas été développées des méthodes de visualisation, examinant la complexité interne de l’image pour repérer les régions plus complexes que d’autres. L’hypothèse centrale reste celle d’une corrélation entre complexité (ici visuelle, graphique de type 1) et difficulté de lecture de la carte.
Nous avons développé des outils logiciels apportant une telle visualisation, en choisissant comme méthode de mesure de la complexité deux techniques différentes : les arbres quaternaires (quadtrees) et la compression par transformée cosine discrète. La première technique a été choisie à titre d’expérience, comme approche visuelle de l’idée de complexité interne d’une image. La seconde technique a été utilisée car elle possède un caractère généraliste et une validation scientifique existante, comme poursuite des travaux de D. Fairbairn sur la compression. D’autres indices de complexité auraient été envisageables, par exemple à partir des approches statistiques de dissimilarité utilisées en écologie du paysage, mais l’objet d’étude en est différent, et leur examen aurait nécessité une recherche à part entière.
Les quadtrees8 (arbres quaternaires) correspondent à un type de structure de données informatique issue de la théorie des graphes, représentant la généralisation à deux dimensions des arbres binaires. Ces arbres permettent de séparer un ensemble de valeurs hiérarchiquement, sur un critère choisi d’avance. L’intérêt ici est de pouvoir utiliser cette méthode pour dessiner des formes dont la complexité visuelle (degré de ramification) sera une illustration plus claire (car extraite, isolée) de la complexité de l’image.
Sur une matrice de pixels, les quadtrees réunissent les pixels adjacents dont la différence de valeur est inférieure à un seuil donné, afin de donner une feuille de l’arbre9. En raison de son fonctionnement sur la valeur des pixels, cette méthode est donc limitée au traitement d’images sur un seul canal ou en niveaux de gris10. Les images en couleur sont donc converties en niveaux de gris par l’algorithme classique qui affecte à chaque canal coloré d’origine (rouge, vert et bleu) une pondération en fonction du niveau de perception des couleurs de ce canal par le système psychovisuel humain11. Il aurait en effet été difficile de synthétiser trois quadtrees, un par canal, en une seule image, ou d’utiliser un seuil de détection valable sur trois canaux. Outre une perte évidente d’information, le simple fait de convertir en niveaux de gris ajoute des valeurs intermédiaires dans les intensités, et augmente ainsi artificiellement le type de complexité mesuré. C’est donc une contrainte importante.
La racine de l’arbre va correspondre à l’intégralité de l’image, et l’algorithme va subdiviser l’image successivement en quadrants de taille plus petite à chaque étape. Lorsque les différences entre pixels au sein d’un même quadrant sont inférieures au seuil prédéfini, le quadrant n’est plus subdivisé et devient une feuille de l’arbre final (cf. illustration figure 2).
Dans le cadre de l’analyse de complexité d’une image, nous pouvons choisir un critère comme la différence de valeur d’intensité entre pixels adjacents. Ainsi, lorsque l’image présente un grand nombre de pixels de différentes valeurs, les feuilles de l’arbre seront nombreuses, pouvant aller jusqu’à une feuille par pixel. Lorsqu’une image comprendra une plage de pixels de valeur proche, cette plage sera englobée au sein d’une feuille unique. La complexité de l’image peut alors se mesurer en comptant le nombre de feuilles au sein d’une zone prédéfinie, et être visuellement appréciée : la complexité de l’arbre, le nombre de ses ramifications, étant une métaphore pour la complexité de l’image. Il suffit alors de dessiner l’arbre, c’est-à-dire le contour de ses feuilles plus ou moins grandes, pour apprécier la densité de feuilles et donc la complexité de l’image source.
Figure 2 : Exemple de calcul d’un quadtree

Le fait que de nombreuses petites feuilles soient adjacentes peut être assimilé à une densité importante des variations locales de valeur, donc une quantité d’information (brute) importante. Cette complexité est donc assez abstraite, et ne comprend qu’une petite partie de la complexité du point de vue de la perception humaine.
Par ailleurs, cette détection est dépendante d’un seuil de différence fixé à l’avance, et ce seuil n’est pas adaptatif : valable pour toute l’analyse de l’image, il ne varie pas en fonction du contenu.
Figure 3 : Image cartographique de test n° 1

L’image cartographique de test n° 1 (figure 3) est une carte choroplèthe, distribuée sur son site Internet par le National Renewable Energy Laboratory (NREL), une entité du ministère de l’Énergie des USA12. Elle a été choisie pour son caractère coloré et sa complexité interne variable, la donnée statistique étant représentée à l’échelon des comtés, qui sont d’une superficie très hétérogène sur le territoire du pays. Par ailleurs, elle présente une utilisation sémiotiquement fausse de la variable visuelle de couleur, pour traduire une relation d’ordre entre des valeurs quantitatives, et donc une complexité graphique (type 2) et intellectuelle artificielle.
La première série d’illustrations (figure 4) présente les résultats d’un calcul des quadtrees, sur une version à résolution réduite de l’image, au format JPEG13, dans l’objectif de tester un type de format numérique d’image cartographique très courant dans les publications en ligne.
Figure 4 : Traitement de l’image en utilisant la méthode des quadtrees

La première visualisation de la complexité (image 1b, figure 4) utilise la méthode des quadtrees directement à partir de l’image originale. Le résultat semble assez conforme aux prévisions, avec une complexité qui apparaît dès que la valeur des pixels change par rapport au fond blanc. Cependant, cette complexité atteint très rapidement son maximum et presque l’ensemble du pays se retrouve avec des « feuilles » de quadtree très petites. L’origine de ce problème se situe premièrement dans un effet de la compression JPEG utilisée, auquel la méthode se révèle très sensible. La compression JPEG a en effet comme inconvénient de produire des « artefacts », de légères distorsions de valeur dans l’image14, à proximité des changements brusques et localisés de couleur (dont les caractères du texte, entre autres). Deuxièmement, la faible résolution de l’image implique que la densité d’information est grande, par rapport au nombre de pixels utilisés, et les variations de valeur n’ont pas physiquement la place de se répartir sur plusieurs pixels. L’algorithme de mesure de la complexité utilisé ici va détecter ces variations et les restituer.
Pour compenser la sensibilité de l’algorithme à ces variations rapides d’intensité (i.e. sur une faible distance dans l’image), les dégradés, nous avons testé l’utilisation d’un lissage intermédiaire, par un filtre lissant simple de type gaussien15. Ce filtrage peut être appliqué potentiellement plusieurs fois de suite dans les cas de forte hétérogénéité dans l’image source. Ce type de filtrage ajoute un « flou » à l’information de départ. Du fait de la représentation de l’image sous forme d’une grille de pixels, il peut parfois être nécessaire d’appliquer ce genre de filtre plusieurs fois, afin de permettre à l’effet de flou de dépasser le seuil d’échantillonnage introduit par la matrice de pixels, et d’affecter alors les pixels adjacents. Le résultat correspond à l’image 1c. La différence se remarque surtout sur la zone la plus dense en comtés, au centre de la partie orientale des USA. L’application du lissage gaussien provoque un décalage artificiel des zones de complexité forte, qui n’a que peu d’intérêt pour la mesure de la répartition spatiale de la complexité. Sur les lettres du titre de la carte, une complexité artificielle est créée en étalant le fort contraste entre le noir des lettres et le blanc du fond.
Est-ce à dire que les images JPEG très compressées et de faible résolution sont plus complexes ? D’après le critère des quadtrees, oui, et l’on commence à mesurer ici la distance existant entre la complexité informationnelle liée à la différence de valeur de pixels et la complexité de perception, où les faibles variations sont gommées naturellement, c’est-à-dire non perçues par la vision humaine.
Figure 5 : Image cartographique de test n° 2

Pour réduire les problèmes posés par la compression JPEG et la faible résolution, le second test a porté sur la version originale de la carte du NREL, en pleine résolution (1920x 1484 pixels, JPEG 24 bits faiblement compressé, figure 5).
Figure 6 : Traitement de l’image en utilisant la méthode des quadtrees

Les visualisations de la complexité obtenues (2b et 2c, figure 6) sont beaucoup plus claires et contrastées. L’augmentation de la résolution par rapport au test précédent apporte un plus grand espace de dilution des valeurs des pixels. On remarque surtout la mise en évidence très forte des contours des comtés, et plus généralement on retrouve les changements de valeur rapides, donc les contrastes marqués. Ainsi, les bords des lettres du titre sont bien identifiés, mais aussi les petits textes de la légende et de la péricarte16. On note aussi la forte présence de l’ombrage ajouté aux frontières et aux côtes : il s’agit d’un dégradé du noir vers le blanc, donc de nombreuses variations d’intensité sur une faible épaisseur.
L’algorithme de détection de la complexité joue son rôle dans notre exemple, en rendant plus visibles les changements rapides d’intensité de valeur. Ici, ce ne sont pas les différentes valeurs de la variable représentée en choroplèthe qui introduisent de la complexité, mais bien l’interférence des contours des comtés, de l’ombrage et des éléments de la péricarte, qui ressortent très fortement.
Figure 7 : Image cartographique de test n° 3

Nous avons voulu ensuite tester la méthode sur des images moins courantes aujourd’hui, utilisant une seule couleur, du noir sur un fond blanc. Ces images compensent l’absence des variables visuelles de couleur et de valeur pour produire des représentations tout aussi efficaces, dont la complexité visuelle est d’un autre ordre que dans les images précédentes. Le fait que ces images en noir et blanc soient aussi très présentes dans les travaux fondateurs de la sémiologie graphique nous permet de bénéficier d’un point d’ancrage méthodologique solide pour l’évaluation des méthodes de visualisation de la complexité.
Nous avons utilisé une numérisation de la carte des densités de population dans les départements français de Jacques Bertin17, qui utilise la méthode du semis régulier de points (figure 7). Pour préserver le caractère dichromatique de l’image, et ne pas introduire d’artefacts de compression, c’est la version noir et blanc (1 bit) qui a été exploitée.
Figure 8 : Traitement de l’image en utilisant la méthode des quadtrees

Les résultats sont assez peu interprétables lorsque l’on ne lisse pas l’image avant d’y dessiner les feuilles du quadtree (image 3b, figure 8) : la densité de points dessinés interfère avec la résolution de la recherche de complexité, cette dernière travaillant sur une zone assez large pour que s’y situe au moins un des points de la carte. Ne ressortent que la Lozère ou les départements alpins, assez peu denses pour offrir de larges espaces blancs. Ce type de représentation cartographique va donc réduire à néant la pertinence de la méthode de mesure de la complexité. L’utilisation d’un lissage gaussien de dix passes permet de simplifier l’image pour dégager les contours de la carte et les zones de densité homogène (3c).
Ainsi, même sur une image dichromatique, les quadtrees vont être très sensibles à la répartition dans l’espace des zones de valeurs différentes, et peuvent s’avérer peu informatifs.
Nous l’avons vu, la méthode de visualisation de la complexité interne d’une image par l’utilisation d’un quadtree permet de repérer des régions de l’image qui présentent de nombreux changements d’intensité localisés, et d’obtenir une vision générale de la répartition de cet aspect de la complexité visuelle à l’aide d’une forme simple : des carreaux de taille variable formant des grilles dont le pas est fonction inverse de la complexité.
Néanmoins, cette méthode ne fonctionne que sur des images monocanal, en niveaux de gris. De plus, elle est très sensible à la résolution de l’image de départ (taille de l’image et densité de pixels par unité de surface), et à la présence éventuelle d’artefacts de compression JPEG. Or, d’une part, la résolution n’est pas un critère influent sur la complexité perçue visuellement, par un observateur humain : deux images dont le contenu graphique est identique, mais dont la résolution change ne seront pas appréciées comme visuellement très différentes, alors que les quadtrees en donnent une visualisation distinctement différente. D’autre part, les faibles fluctuations ajoutées par la compression JPEG sont très peu perceptibles visuellement, par définition (cf. partie suivante), alors que les quadtrees les font ressortir. Cependant, cet outil a permis de relever la forte influence des contours fins, des petits textes, des ombrages, sur la complexité visuelle, alors que ces éléments sont souvent secondaires dans la fonction de communication de la carte.
La méthode de mesure de la complexité par les taux de compression utilisée ensuite résout une partie des problèmes relevés ici, tout en produisant une représentation plus synthétique de la complexité visuelle.
La compression d’une image numérique, la réduction de la taille de son fichier, est directement fonction de la quantité d’information qu’elle contient. Le concept d’information est ici entendu au sens de la théorie de l’information18. Il s’agit d’une grandeur mesurable, et, dans un contexte d’image numérique de type raster, l’information correspond aux valeurs non redondantes des éléments constitutifs de l’image, les pixels.
En effet, étant donnée la finalité des méthodes de compression qui est de réduire l’image à sa plus petite taille possible en supprimant les éléments dupliqués ou non perceptibles par le système auquel l’information est destinée, ici le système psychophysiologique humain, le taux de compression devient un indice utile de la quantité d’information, de l’entropie du message. On pose donc l’hypothèse de la relation entre une grande quantité d’information et la complexité (structurelle) de l’image, et donc potentiellement la difficulté de lecture de la carte.
D. Fairbairn a examiné différentes méthodes de compression et les a testées sur une variété de types d’images cartographiques, en faisant varier différents paramètres et conclut :
« From a mechanistic viewpoint, the amount of information in an image can be measured and the reduction factor achievable by applying image compression can be determined. We have seen that such image compression is a valid technique for examining maps and their structural complexity. » (Fairbairn, 2006, p. 235).
Cependant à aucun moment n’est envisagée la mesure de la complexité interne de l’image cartographique, sa répartition dans l’image, ce qui permettrait une visualisation directe de ses variations, tout en préservant, dans une certaine mesure, les possibilités de comparaison entre images.
L’objectif est donc de produire une visualisation de la complexité interne d’une image, en utilisant le taux de compression comme unité de mesure. Cela suppose de décomposer l’image en sous-éléments, de mesurer leur taux de compression par rapport à l’original, et enfin de produire une représentation graphique de ce taux.
Le choix de la méthode de compression, paramètre central, est plus délicat, car les techniques sont nombreuses et produisent des résultats sensiblement différents. Nous avons choisi d’utiliser une compression structurelle utilisant une transformée fréquentielle, pour sa robustesse et son adaptation à nos besoins.
Joseph Fourier propose en 1822 une méthode pour décomposer un signal périodique continu régulier en une somme infinie de sinus et de cosinus : la transformée de Fourier. Une image numérique peut être assimilée à un signal, acquis à l’aide d’un convertisseur analogique vers numérique, ou directement créé sur un système informatique. Cette image est composée de matrices de pixels (une matrice unique dans le cas d’une image en niveaux de gris, trois pour une image couleur), chaque pixel mesurant l’intensité de l’information lumineuse à sa localisation.
La DCT (Discrete Cosine Transform19) est une transformée du même type que celle de Fourier, mais plus adaptée à une utilisation informatisée. Elle permet de décomposer une information spatiale (la matrice de pixels) en information fréquentielle consistant en une suite finie de cosinus. L’algorithme de compression JPEG utilise la DCT comme méthode principale pour obtenir une compression des images numériques. Cette transformée est très utilisée en imagerie et en traitement du signal en raison de sa propriété de regroupement de l’énergie (et donc de l’information portée par le signal).
D. Fairbairn présente dans son article l’utilisation de deux techniques de compression : l’algorithme de Huffman, et le Run Length Encoding (RLE), qui sont des techniques de compression par codage entropique. Le codage entropique permet d’encoder une information sans perte, de manière non destructrice, pour ramener son expression à son entropie.
Le codage sans perte est utile pour transmettre l’information de machine à machine, ou pour son archivage. Mais le système psychovisuel humain n’est pas une machine parfaite, et effectue une série d’approximations lors de l’acquisition d’informations. En se basant sur cette propriété, l’algorithme JPEG/DCT supprime de l’image originale les faibles variations de valeur qui sont très peu perceptibles. Le système peut ainsi stocker moins d’informations qu’initialement, tout en offrant un aspect visuel relativement proche de l’image d’origine. Ce paramètre est de plus modulable dans l’algorithme.
Ainsi, contrairement à un algorithme de codage entropique, proposé par D. Fairbairn, nous avons choisi d’utiliser la compression par DCT dans notre mesure de ce type de complexité, car on se rapproche un peu plus de la perception de l’image par un être humain20.
Deux procédures ont été testées pour l’application de cette méthode aux images cartographiques : par carroyage et par fenêtre glissante (figure 9). Le carroyage produit une image-résultat de plus faible résolution, mais simple, facilement lisible. La mesure par fenêtre glissante produit une image-résultat de même résolution que l’image originale, donc potentiellement plus précise, mais aussi plus difficile à interpréter. Les deux procédures comprennent une application sur chacun des trois canaux (rouge, vert et bleu) des images couleur. On calcule ensuite une moyenne pondérée, tenant compte de la perception humaine différenciée de ces canaux (cf. note No 11, supra).
Figure 9 : Les deux procédures utilisées pour le calcul du taux de compression par la DCT

Cette technique suppose de diviser l’image en sous-parties carrées, puis de compresser ces dernières afin de calculer le taux de compression obtenu en comparant le poids numérique du résultat de la compression à la fraction d’image originale.
Le choix de la méthode de subdivision de l’image s’est porté sur un carroyage régulier, qui a l’avantage de permettre une comparaison aisée entre l’image-résultat et l’image originale. Il aurait aussi pu être envisagé de réaliser une segmentation de l’image en zones de contenu textural proche, pour obtenir une visualisation de la complexité interne plus fine et directement liée au contenu, mais cela impliquait une série supplémentaire de choix algorithmiques (types de segmentation) qui aurait nui à l’universalité de la méthode et à la facilité de l’interprétation des résultats sur des séries d’images différentes (des atlas par exemple), par un large public.
Reste la question de la résolution du carroyage à utiliser, c’est-à-dire la taille des carreaux. Cette question est très générale, et non spécifique à ce problème de compression. La valeur optimale est fonction de la taille de l’image d’origine, et se détermine généralement de façon empirique entre deux extrêmes à éviter : trop peu de carreaux et le résultat n’est pas assez fin tout en étant difficilement comparable à l’original, trop de carreaux et la compression ne travaille plus sur un nombre suffisant de pixels pour être un indice pertinent de complexité. L’outil que nous proposons permet à son utilisateur de déterminer directement les dimensions des carreaux désirés, tout en proposant une valeur par défaut établie en fonction des dimensions de l’image à analyser (simple pourcentage).
Dans l’algorithme JPEG original, deux tailles principales de carreaux sont utilisées pour appliquer la DCT sur l’image, 8 x 8 pixels et 16 x 16 pixels. Les carreaux sont carrés, pour des raisons mathématiques, liées à la périodicité de la transformée en cosinus. Leur dimension est un multiple de 8, pour des raisons de performances des calculs sur les architectures informatiques des microprocesseurs.
Lorsque l’image d’origine est rectangulaire, un « remplissage » artificiel (avec des valeurs nulles) est effectué afin de ramener la carte à des dimensions carrées, pour éviter les effets de bordure.
Cette technique consiste à travailler pixel par pixel sur l’image originale, en mesurant la complexité de la zone de l’image qui entoure ce pixel, cette « fenêtre » étant une forme géométrique, généralement un carré ou un losange, de largeur constante déterminée à l’avance. Il s’agit là d’une technique classique en traitement d’image, notamment utilisée pour calculer les filtres21 courants. Elle permet d’obtenir un résultat possédant le même nombre de pixels que l’original, donc d’une précision potentiellement identique. Son paramétrage n’implique que le choix de la « qualité » de la compression, c’est-à-dire la quantité d’information à préserver. Dans cette variante également, un remplissage est effectué lorsque l’image n’est pas carrée, pour éviter les effets de bordure.
Figure 10 : Traitement de l’image n° 1 en utilisant la méthode de la DCT par carroyages

Figure 11 : Traitement de l’image n° 1 en utilisant la méthode de la DCT par fenêtre glissante

Nous retrouvons en première image-test la carte de la biomasse du NREL, en basse puis haute définition, selon les deux modalités, carroyages (images 1d à 1f, figure 10) et fenêtre glissante (images 1g à 1i, figure 11).
La complexité est représentée par une gamme de couleurs, allant d’un vert sombre ne signifiant aucune complexité à un rouge violacé pour la complexité maximale, en passant par des jaunes intermédiaires. Ces bornes s’entendent par rapport au contenu de l’image, la valeur de complexité étant normalisée image par image. La gamme colorée choisie permet de mettre en relief de manière perceptible les zones peu et fortement complexes, la transition étant représentée par un camaïeu de jaunes.
La figure 10 présente tout d’abord la visualisation de l’image basse définition, utilisant un carroyage de 8 puis 16 pixels de côté (1d et 1e), puis sur l’image en haute résolution avec un carroyage de 16 pixels. On remarque que, malgré la complexité de la procédure de production de ces images-résultats, leur lecture est très simple. On repère facilement les zones les plus complexes, et la comparaison avec l’image originale est aisée. Les faibles variations de valeur de pixel sur de courtes distances ne sont plus surévaluées comme avec la méthode des quadtrees : par exemple sur l’ombrage du contour du pays sur l’image en haute résolution. Sur cette image, les zones de complexité intuitive se retrouvent bien : textes de la péricarte, limites de comtés et frontières, ainsi que la zone très dense de l’Est du pays. Il est intéressant de noter que les limites des éléments du fond de carte, son niveau de généralisation, et le figuré utilisé pour les représenter, une fine ligne noire, vont avoir une influence directe sur le type de complexité mesuré.
On remarque aussi que la complexité prend très rapidement des valeurs élevées, les teintes correspondant à des valeurs de moyennes à faibles et moyennes (verts clairs et jaunes) sont peu représentées, surtout lorsque l’on choisit des carreaux de petite taille (image 1d). L’utilisation de carreaux plus grands va permettre à la méthode de couvrir des zones de complexité plus hétérogène, et donc de produire des valeurs de complexité moyenne (image 1e). L’analyse sur l’image de plus haute résolution (1f) apporte une plus grande finesse des résultats, comme on pouvait s’y attendre, mais l’image reste simple à lire. Le texte situé sous la carte présente ici une forte complexité, qui n’était pas apparue aussi vivement dans les analyses précédentes.
L’utilisation d’une procédure par fenêtre glissante (figure 11) apporte un gain en résolution, et une plus grande ressemblance avec l’image d’origine (images du bas). Par exemple, le très grand comté de San Bernardino, au sud de la Californie, ressort avec une forme proche de son territoire réel, alors que sur l’image de la procédure par carroyage, les formes sont masquées. Cependant, les temps de calcul de cette variante de la mesure sont sensiblement plus longs.
Figure 12 : Test de la méthode DCT sur une carte visuellement et sémiotiquement complexe

Pour tester la méthode avec une carte volontairement (visuellement) complexe, nous avons choisi cette superposition des parcours des tempêtes dans l’Atlantique Nord entre 1851 et 2004, produite par la NOAA en 200522. 1325 lignes de la même couleur noire se superposent sur une image de dimensions modestes. Paradoxalement, les analyses indiquent une zone de moindre complexité au milieu du fatras de lignes, sur les deux types de résultats (images 4b et 4c, figure 12). En effet, cette zone étant presque complètement noire, elle n’est pas complexe au sens des algorithmes utilisés. On met ainsi bien en évidence l’écart dans la définition même de la complexité, entre la quantité d’information graphique, informatique, et la quantité de signes, de symboles (complexité graphique de type 2, sémiotique), et leur repérage spatial sur le fond de carte, qui sont des opérations intellectuelles.
Figure 13 : Test de la méthode DCT sur un semis régulier de points

La carte en semis régulier de points de J. Bertin offre un nouveau défi d’application. Avec le carroyage (image 3d, figure 13), si la complexité indiquée détecte bien les éléments de la péricarte, la régularité de la représentation elle-même, de plus en noir et blanc, semble perturber la recherche des zones complexes, comme pour la méthode des quadtrees. En utilisant la procédure par fenêtre glissante, nous obtenons une représentation plus contrastée, avec une légère mise en avant des limites régionales et de deux zones autour de Paris et Lyon. Alors que visuellement on repère bien les variations de valeur (de signes), les algorithmes de mesure par compression sont à la peine. Il semblerait que le système perceptif humain dépasse la complexité créée par régularité du semis de points pour plutôt détecter les variations de densités à l’intérieur du semis.
Ainsi, contrairement à la méthode d’évaluation visuelle de la complexité par les quadtrees, la méthode du taux de compression n’est pas très sensible aux variations localisées de valeurs des pixels, pourvu qu’on choisisse des paramètres de résolution de manière adaptée. Ces contraintes étant prises en compte, on obtient une visualisation simple de la complexité graphique interne d’une image, qui reste assez facilement lisible malgré une modification possible des formes par rapport à l’image originale. De plus, cette méthode fonctionne directement sur les images en couleur, ce qui n’était pas le cas des quadtrees.
Cependant, ces outils d’analyse restent dépendants de la résolution de l’image source. Comme ils cherchent à synthétiser la complexité de l’image en mesurant celle de sous-parties, cette subdivision doit pouvoir disposer d’un nombre assez grand de pixels sur lesquels travailler tout en n’englobant pas trop de signes pour ne pas les confondre. L’application à des images de petite taille et de faible résolution ou à des représentations de texture régulière, comme les semis réguliers, reste donc problématique.
Il est possible de reprocher à ces approches un traitement uniforme de l’image, à une même échelle d’analyse correspondant à la largeur du carreau de base ou de la fenêtre. Or la perception d’une image travaille selon plusieurs échelles successives, passant d’un regard général de départ à une série de régions de taille plus réduite observées séquentiellement. À chaque échelle d’analyse perceptive correspond un niveau de netteté et d’attention, donc de facilité de lecture, comme cela a été démontré notamment par les études des mouvements oculaires23.
Enfin, et surtout, nous mettons bien en évidence le fait que la complexité visualisée ici, la complexité informationnelle et graphique (type 1), peut donner des résultats parfois en désaccord complet avec la lisibilité cartographique, et l’intuition de l’utilisateur quelque peu familiarisé avec la lecture de cartes. C’est la définition même de la complexité utilisée qui est mise en balance : il est souvent illusoire d’utiliser la complexité informationnelle pour estimer la complexité perceptive et d’autant plus la complexité cognitive, intellectuelle, d’une carte. La visualisation de ce deuxième type de complexité est toutefois aujourd’hui approchable, grâce aux algorithmes de modélisation de la saillance visuelle.
La question de la compréhension des processus de cognition des représentations graphiques externes (images physiques), de leur interaction avec les capacités de représentation interne des individus (images mentales), a été récemment placée aux premières places des enjeux de recherche dans le domaine de la cartographie (cf. notamment MacEachren, 1995 ; Montello, 2002 et Cauvin et al., 2007). Le besoin d’une infrastructure théorique basée sur des principes cognitifs, qui permettrait de s’approcher d’une meilleure compréhension de la perception cartographique du côté des utilisateurs a été notamment formalisé par S.I. Fabrikant et A. Skupin (2005), en cartographie et géovisualisation. Ces auteurs formulent la question de l’adéquation cognitive des images cartographiques, c’est-à-dire de leur ressemblance / proximité avec les représentations et images mentales existantes des utilisateurs, et plus concrètement tout d’abord avec le fonctionnement des processus neuraux de perception visuelle.
Ces travaux ont mené S.I. Fabrikant à envisager l’utilisation du concept de saillance visuelle pour mesurer l’adéquation cognitive des variables visuelles, et plus généralement de la composition cartographique (le « map design »), dans un article fondateur de 2005. Ce transfert méthodologique en provenance de travaux de modélisation de la perception visuelle, élaborés en pluridisciplinarité par des psychologues cogniticiens et des informaticiens en modélisation cybernétique de la vision, se révèle très intéressant dans le contexte de visualisation de la complexité. Telle qu’elle est définie dans cet article, la saillance visuelle est fonction de la rapidité avec laquelle une position de l’image observée est le sujet de l’attention visuelle, et de la durée du maintien de cette attention. Les éléments considérés comme saillants sont donc repérés rapidement dans l’image par l’observateur, qui leur consacre de plus un certain temps d’attention. Ces paramètres sont estimés à partir d’un modèle neurobiologiquement vraisemblable de l’attention visuelle. S.I. Fabrikant a en l’occurrence utilisé le modèle d’Itti et al. (1998), utilisant des réseaux de neurones, et qui est disponible sous la forme d’une boîte à outils logicielle.
Il existe aujourd’hui de plusieurs variantes d’algorithmes de modélisation de la saillance visuelle d’une image, les précurseurs et les plus cités dans ce domaine étant les chercheurs évoqués ci-dessus, L. Itti et C. Koch (1998, 2001) du laboratoire Ilab de l’USC à Los Angeles. Ces derniers proposent une implémentation logicielle de leur algorithme, au travers d’une boîte à outils, l’iLab Neuromorphic Vision C++ Toolkit24, ou iNVT.
Par ailleurs, l’un des algorithmes les plus récents et les plus performants est celui présenté par l’équipe de L. Zhang (2008), dénommé SUN pour « Saliency Using Natural statistics ». Il a été implémenté de façon pratique par N. Butko et al. dans une boîte à outils logicielle (Butko, 2008).
L’objectif de ces algorithmes est de produire une image-résultat représentant l’ordre et/ou de la durée estimée de fixation de l’oeil sur les régions de l’image source. Il faut préciser que cette méthode cherche à estimer une perception « préattentive », c’est-à-dire globale et non directive, avant que le sujet n’ait entamé une recherche consciente d’éléments spécifiques de l’image. Il faut noter de plus que cette implémentation est prévue pour l’utilisation d’images « naturelles », c’est-à-dire de scènes visuelles telles qu’un œil humain rencontre habituellement. Les images (cartographiques) que nous analysons ici ne sont pas naturelles, ce sont des représentations graphiques en deux dimensions réalisées artificiellement, et possédant un certain code d’utilisation (titre, légende, autres éléments, auxquels nous sommes éduqués et habitués). Ce sont des biais importants dans le contexte de la lecture d’une carte, nous y reviendrons.
D’un point de vue pratique, nous obtenons donc une image-résultat qui va indiquer, par un dégradé de valeurs colorées tiré d’une légende volontairement contrastée, les régions de l’image faisant l’objet d’une attention de moins en moins précoce et de longue durée. Ainsi, nous pouvons en déduire assez directement une complexité de perception qui en serait l’inverse : les zones les plus difficiles à percevoir faisant l’objet d’une attention moindre et plus tardive dans l’examen de l’image source. Cet outil complète donc ceux utilisés précédemment, en apportant une tentative de visualisation de la complexité telle qu’elle est expérimentée par le système psychovisuel humain, et donc le lecteur de l’image cartographique.
Figure 14 : Saillance SUN et iNVT sur l’image n° 1

Les images-résultats produites par les algorithmes SUN et iNVT sont elles aussi normalisées, c’est-à-dire que les valeurs produites sont réparties sur tout l’intervalle minimum - maximum pour chaque image. L’analyse sera donc valable image par image, et l’on ne peut comparer directement les valeurs issues de deux images différentes.
Appliqué sur la carte de la biomasse du NREL, en basse résolution (image 1j, figure 14), l’algorithme SUN produit une image résultat bien contrastée, avec des éléments saillants spatialement compacts et relativement peu nombreux. Ils semblent positionnés sur des zones comportant des taches de couleur denses et contrastées, par exemple au centre de la frange nord du pays (cf. l’image de détail). Certaines couleurs sont nettement isolées, comme le rouge et le cyan (cf. le caisson cyan de la légende). La complexité graphique induite par les limites des comtés semble aussi prise en compte : les zones très découpées ne ressortent pas comme saillantes. Le cadre de la légende et les textes de péricarte sont détectés comme moyennement saillants (tons jaunes), on peut penser que c’est à cause du contraste localisé qu’ils représentent sur le fond blanc.
L’algorithme iNVT propose une image-résultat beaucoup plus simple (image 1k), avec simplement quelques taches colorées sur un fond uni de faible saillance. Son objectif est de fournir une hiérarchie de petites zones visuelles indiquant le parcours de l’attention, la série de points où devrait se poser successivement le regard lors de la présentation de l’image. Les régions mises en avant comme saillantes sont à peu près les mêmes, hormis celles caractérisées par des aplats cyan relevés par la méthode SUN. La Californie, qui présente sur la carte originale des taches colorées contrastées et de bonne taille, est considérée comme la région la plus saillante. La légende est là encore clairement reconnue comme remarquable.
Figure 15 : Saillance SUN et iNVT sur l’image n° 4

Sur la carte des tempêtes de la NOAA (images 4d et 4e, figure 15), les deux algorithmes semblent égarés par la composition de la carte. Le gribouillis central est cependant bien qualifié comme non saillant. À l’inverse, les zones saillantes sont nombreuses et marquées en ce qui concerne l’image produite par SUN, en fait les zones océaniques libres de tempêtes, larges plages de bleu. L’iNVT repère quant à lui des zones de transition marquée entre plages de couleurs différentes, mais dans un ordre qui n’apparaît pas immédiatement compréhensible.
Figure 16 : Saillance SUN et iNVT sur l’image n° 3

Enfin sur le semis régulier de cercles de J. Bertin, l’algorithme SUN (image 3f, figure 16) repère certaines zones denses (Paris, Bouches-du-Rhône et Rhône), mais échoue sur le cercle de grande taille autour de la région parisienne. L’iNVT, lui (image 3g), repère bien les fortes densités du Nord et de la région parisienne, ainsi que les densités secondaires dans une moindre mesure, mais il fait ressortir la pointe bretonne et la côte basque, pour des raisons peu évidentes.
L’apport de la modélisation de l’attention, proposant une visualisation simple de la hiérarchie visuelle des images, permet de compléter de manière intéressante les analyses précédentes sur la complexité graphique. Les principales limites relevées sur les outils précédents sont ici dépassées, notamment les questions du travail sur des images en couleur, de la survalorisation des dégradés rapides, et de la prise en compte de multiples échelles de perception. L’analyse d’images de petite taille et de faible résolution ne pose pas de problème non plus.
Surtout, est ici mise en œuvre une tout autre méthodologie de modélisation de la perception d’une image, qui fait intervenir plusieurs facteurs, plusieurs processus de détection, successifs et organisés en système perceptif (ce qui suppose des renforcements, des rétroactions et des inhibitions, etc.). Le passage d’une mesure mécanique de la quantité d’information d’une image à des algorithmes visant à rendre compte de la façon dont le système psychovisuel humain va percevoir cette image permet d’obtenir une compréhension plus grande de la lisibilité d’une image.
Cependant, l’analyse reste encore au niveau d’une complexité visuelle, graphique (de type 1) et structurelle, elle ne prend pas en compte l’adaptation de la carte aux principes sémiotiques relevant d’une complexité plus intellectuelle, de compréhension des signes. Ainsi la carte de l’image de test n° 1 (NREL, biomasse) a aussi été choisie car elle est très mauvaise de ce point de vue. Elle ne tient pas compte des principes sémiotiques usuels de représentation cartographique en choroplèthe, utilisant plusieurs couleurs pour représenter une variation (quantitative) ordonnée simple qui demandait une variation de valeur. Les algorithmes SUN et iNVT montrent bien que les parties saillantes de cette carte ne sont pas celles où la valeur de la variable représentée présente ses valeurs les plus élevées, mais l’analyse ne peut être poussée plus loin avec ces outils.
Cette amélioration de la performance de compréhension de la complexité d’une image cartographique se réalise toutefois avec une augmentation considérable de la difficulté à paramétrer les outils, dans le cas de l’iNVT surtout. En effet, si la méthode SUN fonctionne presque directement, sans longue phase de spécification, l’iNVT propose une impressionnante quantité de paramètres possibles25, pour affiner la modélisation de l’attention à chaque niveau du modèle (réaction des différentes régions corticales, types d’interactions), mais aussi pour adapter l’outil à plusieurs hypothèses.
Figure 17 : Test des saillances SUN et iNVT sur une image « naturelle » (photographie L.Jégou, 2011)

L’algorithme SUN produit un résultat facile à interpréter et sensiblement conforme à l’intuition du cartographe, si ce n’était sa trop grande sensibilité à certaines couleurs (rouge et cyan purs). Peut-être est-ce dû aux images « naturelles » utilisées comme référence dans son élaboration26, où ces couleurs doivent être relativement peu présentes. Les images artificielles qu’on lui a présentées ici sortent du cadre normal de son utilisation. Pour confirmer cette hypothèse, les figures 17 et 18 illustrent son utilisation sur des scènes « naturelles ». Sur la photographie n° 1 cependant (et un peu moins sur la photographie n° 2), le ciel bleu ressort tout de même comme saillant, alors que les formes sont très peu marquées et les contrastes inexistants. L’algorithme iNVT s’en sort mieux et identifie des zones plus conformes avec l’expérience empirique.
Ces comparaisons avec l’estimation a priori, intuitive, de la complexité des images cartographiques, outil accessible à toute personne ayant quelque peu l’expérience de la lecture de cartes, indiquent une piste pour la poursuite des travaux. Nous envisageons la vérification expérimentale de ces essais de mesure par la constitution d’une enquête, basée sur un questionnaire visuel. Une autre piste peut être recherchée du côté des éléments jouant sur le caractère clair et esthétique d’une image cartographique, facteur qui va influer sur son attrait et sa lisibilité.
Figure 18 : Test des saillances SUN et iNVT sur une image « naturelle » (photographies L. Jégou, 2011)

Les premiers outils présentés ici, quadtrees et taux de compression DCT, sont tout d’abord le reflet d’une approche simpliste, mécanique, utilisée pour tenter de mesurer et visualiser la complexité d’une image cartographique. Cette mesure ne s’applique qu’à la composante informationnelle de la complexité, elle est largement insuffisante pour exprimer la lisibilité totale d’une image cartographique. Ce simplisme reconnu, et ses principaux biais identifiés, nous avons pu cependant relever certains apports de la visualisation de ce type de complexité, qui reste une composante intrinsèque, parfois non négligeable, de la complexité perceptive totale.
L’idée ici n’est donc pas seulement de proposer une mesure mécaniste de la répartition de la complexité visuelle dans une image, qui servirait à la « corriger » en indiquant les zones à simplifier. Il semble utile de donner une idée de cette complexité informationnelle dans le premier temps d’une étude plus complète de la lisiblité de l’image, qui servirait à l’apprentissage de cet aspect. Car l’apprenti cartographe, potentiellement trop familier de ses propres choix graphiques, a peut-être perdu une partie de sa capacité d’estimation de l’effet produit par l’image sur le lecteur. En outre, cette information peut aussi être utile dans le cas où la réalisation de l’image cartographique a été confiée à un système logiciel automatique, comme c’est de plus en plus souvent le cas. Ces systèmes ne proposent souvent que des possibilités très limitées de création cartographique. Leur utilisateur peut être alors intéressé par une information neutre sur la complexité visuelle du résultat, même très partielle, et donc un indice sur sa lisibilité. Par exemple, on a pu relever la forte influence sur la complexité visuelle de l’utilisation de fonds de carte trop précis (du point de vue de la généralisation des contours et de la subdivision en régions) et trop visibles : ils conduisent à la présence sur l’image de très nombreux éléments graphiques fins et contrastés. La réflexion sur l’adéquation du figuré et du degré de généralisation du fond de carte avec l’objectif de la carte se trouve à nouveau justifiée, si besoin en était.
Ensuite, nous proposons d’approfondir la visualisation de la complexité en utilisant cette fois un outil qui se rapproche de la perception humaine, et informe sur une répartition hiérarchique des zones de l’image captant l’attention. La complexité perceptive de l’image cartographique y est donc potentiellement lisible en creux. On approche la notion de lisibilité en visualisant de manière simplifiée une estimation de la façon dont l’image sera perçue. Il est donc possible d’apprécier si cet ordre correspond à celui qui est adapté à une bonne compréhension de la carte, c’est-à-dire si les éléments saillants de l’image sont ordonnés de façon à correspondre aux zones importantes du point de vue du message de la carte et de sa compréhension par son public. Cette notion de hiérarchie visuelle et de contenu des zones composant une image, évoquée notament par la psychologie visuelle gestaltiste, est aussi très importante dans le domaine de la conception graphique27. C’est donc une piste intéressante de poursuite des recherches.
Cependant, les méthodes d’estimation de la saillance visuelle sont plutôt adaptées à l’analyse de scènes dites « naturelles », ce dont les cartes sont assez éloignées, à la fois du point de vue du type de contenu graphique, mais aussi de celui de leur lecture. Les cartes ne sont pas des images banales, elles portent une information structurée, et nécessitent un certain apprentissage pour leur bonne compréhension. Cet apprentissage, cette culture sont de plus liés à une tradition ou habitude de présentation et à des styles, qui évoluent selon les supports, les outils, les usages. Les modèles de lecture de carte élaborés jusqu’ici28 pourront servir à comparer les algorithmes de saillance visuelle disponibles et à évaluer leur capacité à travailler avec des images cartographiques.
Enfin, des recherches restent à mener dans l’analyse des autres types de complexité, plus évolués, notamment la complexité sémiotique et intellectuelle des cartes. Cette approche de la lisibilité par la complexité visuelle est potentiellement utile dans d’autres disciplines liées à l’image, en particulier dans le domaine artistique (esthétique, critique d’art), et l’on peut espérer des bénéfices croisés à une poursuite de ces travaux.
D’un point de vue pratique, au-delà de la mise à disposition des outils présentés ici sous la forme de services logiciels isolés, nous envisageons de les proposer en tant que fonctionnalité complémentaire de logiciels de cartographie, de SIG29, par exemple en tant qu’extension de logiciels libres. On aurait ainsi un accès simplifié à ce type de méthodes, et la possibilité de les utiliser de manière progressive et itérative, avec une progression pédagogique.
