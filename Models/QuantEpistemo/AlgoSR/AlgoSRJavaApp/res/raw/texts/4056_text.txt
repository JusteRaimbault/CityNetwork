L’époque héroïque est révolue, qui voyait le manuscrit de l’ouvrage "EDA" circuler sous le manteau, sous forme de liasses de photocopies. Les idées de John Tukey [Tukey, 1977] ont fait leur chemin, suscitant leur lot de réactions de rejet catégorique et d’adoption inconditionnelle. C’est certainement auprès des praticiens-thématiciens, soucieux d’extraire de leurs données des significations fondamentalement non statistiques, que ces idées ont trouvé le meilleur écho. Certains spécialistes d’analyse spatiale, qu’ils soient géographes, économistes ou écologues, ont cherché à adapter les principes de Tukey à leur discipline, participant ainsi à l’émergence d’une voie nouvelle, l’analyse spatiale exploratoire des données.
En tant que données statistiques, les données spatiales peuvent évidemment être analysées à travers les méthodes et outils de l’analyse exploratoire des données. Les travaux précoces de David Sibley ou Paul Slater [Sibley, 1985 ; Slater, 1974a, 1974b] sont d’excellents exemples de cette stratégie immédiate. Par ailleurs, on remarquera que les textes de référence consacrés à l’analyse statistique spatiale font souvent référence à l’EDA, encourageant l’analyste à mettre en oeuvre les préceptes de Tukey, tout au moins dans une phase préliminaire [Bailey et Gatrell, 1995 ; Cressie, 1993 ; Fotheringham et al., 2000 ; Haining, 1990]. Toutefois, l’information spatiale possède des spécificités réelles, lourdes de conséquences : depuis longtemps consommatrice de méthodes a-spatiales, importées d’autres disciplines, la géographie doit en effet reconnaître à quel point "spatial is special", et développer ses propres méthodes, explicitement spatiales [Fotheringham et al., 2000, p.6].
L’objectif de cet article est double : introduire la philosophie exploratoire et son orientation spatiale d’une part ; dresser d’autre part un panorama de méthodes et outils à notre sens pertinents, qui mériteraient d’être disponibles en série au sein des systèmes d’information géographique commerciaux.
Outil d’exploration par excellence, la carte possède cette qualité unique d’offrir à son lecteur un référentiel spatial, sinon maîtrisé, tout au moins plus familier que l’espace abstrait des autres types de graphiques statistiques. Cette qualité inhérente à son objet même (l’espace), peut être grandement amplifiée, jusqu’à permettre une véritable "déambulation" spatiale.
L’EDA se caractérise par une ingéniosité graphique tout à fait exceptionnelle. Les ouvrages de Tukey, Tufte, Cleveland ou Buja en sont autant d’exemples frappants [Buja et Tukey, 1991 ; Cleveland, 1993, 1994 ; Tufte, 1986, 1990, 1996 ; Tukey, 1977]. Les géographes ont su s’inspirer de cette créativité contagieuse, ainsi qu’en témoignent de nombreux travaux [Brunsdon, 2001 ; Cahegan, 2000 ; Dykes, 1994 ; Dorling , 1994]. Toutefois, cette quête d’outils nouveaux, toujours plus performants, devrait également s’accompagner d’une amélioration des outils existants, dont certains ont fait leurs preuves depuis longtemps déjà. La puissance évocatrice des cartes choroplètes classiques peut ainsi être décuplée, par la simple adjonction de méthodes de lissage, conférant à ces graphiques traditionnellement cantonnés dans un rôle d’affichage strict, une capacité de révélation des structures spatiales sous-jacentes. Le lissage spatial n’est évidemment pas une nouveauté : il serait toutefois temps qu’il devienne une procédure standard, disponible en série au sein des logiciels de cartographie automatique et autres systèmes d’information géographique. Nous soutenons en effet que le lissage spatial devrait être à l’analyse de l’organisation spatiale, ce que le lissage temporel est à l’analyse des séries temporelles : un recours évident. On notera d’ailleurs que l’analyse d’image exploite depuis fort longtemps déjà, et ce de manière routinière, ce type de procédure [Voiron, 1995].
L’idée de filtrer l’information, afin d’en extraire temporairement les fluctuations les plus manifestement aléatoires, pour n’en retenir que les principales structures, s’inscrit naturellement dans la philosophie exploratoire : "data = smooth + rough" [Tukey, 1986, p.595]. La carte lissée doit donc devenir d’un usage aisé, quasi instantané, de manière à multiplier les vues plus ou moins épurées que l’on peut avoir d’un même phénomène. On remarquera d’ailleurs que, de manière générale, le panel de méthodes utilisées reste souvent limité à un petit nombre de solutions classiques et peu robustes, alors même qu’une diversité remarquable existe dans d’autres disciplines. L’article de Ladiray et Roth [Ladiray et Roth, 1987], constitue par exemple une mine de renseignements sur le lissage robuste de séries temporelles, que l’on pourrait fort bien mettre en œuvre dans une perspective spatiale. Différentes techniques basées sur la médiane peuvent ainsi être aisément adaptées à la problématique du lissage spatial, comme l’illustre la figure 1.

Figure 1. La carte lissée : au delà d’une fonction d’affichage, une fonction d’analyse
On remarquera que l’un des dangers majeurs du lissage est de faire émerger des structures artificielles, purement induites par la méthode elle-même. L’objectif d’une telle "multiplication des vues" n’est donc pas seulement de choisir une représentation parmi un ensemble de solutions. Il s’agit également de favoriser l’émergence d’hypothèses concernant l’organisation spatiale sous-jacente à la carte "brute", au moyen de vues aux propriétés variées mais néanmoins connues. L’idéal serait d’ailleurs de pouvoir disposer ici d’un "curseur de voisinage", permettant de régler en permanence l’ordre de voisinage souhaité, afin de mettre en valeur d’éventuels effets d’échelle. Cette dernière proposition n’a rien d’une idée en l’air, et s’inscrit même dans une logique ergonomique aujourd’hui largement partagée.
Les progrès de l’informatique aidant, l’utilisateur s’est considérablement rapproché de ses données, jusqu’à pouvoir quasiment les toucher au sein d’environnements tactiles virtuels ! L’ordinateur n’est plus ce ventre électronique ingrat, limité à des fonctions d’entrée et sortie. Aujourd’hui, l’utilisateur peut naviguer en toute liberté dans un jeu de données au moyen de graphiques interactifs, déambulant ainsi au grès de ses intuitions, se frayant son propre chemin dans cette jungle de détails à grands coups d’hypothèses. Cette capacité de navigation suppose néanmoins l’existence de graphiques actifs et réactifs, dynamiquement reliés les uns aux autres (figure 2). L’idée de faire interagir directement une carte avec des graphiques statistiques n’est pas nouvelle [Monmonier, 1989], et fait l’objet de travaux aussi nombreux que variés [Anselin, 1996, 1998 ; Banos, 1999, 2001a, 2001b ; Bao et Anselin, 1997 ; Brunsdon, 1998 ; Haslett et al., 1991 ; Josselin, 1999, 2000 ; Wise et al., 1998]. La souris devient un prolongement naturel de l’esprit, l’utilisateur interrogeant ses données à travers les multiples vues à sa disposition, alternant les angles d’approche, naviguant en surface comme en profondeur par simple requête graphique, s’immergeant dans ses données jusqu’à en avoir une connaissance intime. Les formidables capacités humaines, en terme de visualisation, d’intuition, de raisonnement par analogie et de génération d’hypothèses, sont ainsi pleinement mises à contribution, dans le cadre d’une relation homme-machine ludique et réaliste, exploitant au mieux les qualités de chacune des parties. Le logiciel utilisé ici est un environnement de programmation statistique, Xlisp-Stat, mis à disposition gratuitement sur internet. Développé par Luke Tierney [Tierney, 1990], cet outil aux propriétés graphiques remarquables a fait l’objet de développements statistiques [Cook et Weisberg, 1999] et géographiques [Brunsdon, 1998] dissociés, que nous avons entrepris de réunir au sein d’un même ensemble ergonomique.
Cette capacité nouvelle d’exploration graphique, incitant à l’exploration spatiale, possède cependant ses limites. L’explorateur se frayant un chemin dans la forêt équatoriale à grands moulinets de sa machette sait qu’il est condamné à avancer : le chemin qu’il vient de tracer se referme très vite dans son dos, lui interdisant tout retour en arrière et lui laissant peu d’espoir de retrouver un jour cet itinéraire précis, fruit d’un enchaînement de décisions prises dans des contextes particuliers et dont les raisons s’estompent très vite. Sa progression est par ailleurs lente car, même doté d’un machette particulièrement affûtée et maniable, il doit recueillir en permanence toute indication – souvent fugace – lui permettant de progresser dans la bonne direction.

Figure 2. Naviguer en toute liberté dans ses données
Hors, l’interprétation des signes qui jalonnent son itinéraire suppose qu’il ait un objectif, mais également qu’il soit ouvert à toute autre possibilité : obsédé par ce qu’il s’attend à trouver – et dont il ne peut avoir qu’une idée très vague, puisqu’il explore – il risque fort de passer à côté de merveilles qu’il n’attendait pas. Enfin, rien ne dit que la connaissance intime de la forêt équatoriale qu’il aura pu se forger tout au long de son chemin sera suffisante à ses collègues de la société de géographie pour comprendre cet organisme complexe, dont les règles de fonctionnement ne se réduisent pas à cette seule dimension locale.
Cette interactivité graphique doit donc se doubler :
d’indicateurs pertinents et fiables, adaptés aux conditions locales qu’ils sont sensés décrire ;
d’une force d’investigation plus automatisée, à même d’opérer un premier tri dans la jungle foisonnante des détails ;
d’outils de valuation et d’évaluation des structures et phénomènes découverts, mais également des indicateurs et opérations de tris réalisés par l’explorateur lui même.
La statistique classique, non spatiale par tradition, est largement bâtie sur le postulat d’un espace neutre, simple support des phénomènes étudiés. Dans cette perspective, la localisation d’observations dans l’espace (absolu) et leur position les unes par rapport aux autres (espace relatif), n’exercent aucune influence sur la nature même de ces observations. Cette négation de la première loi de la géographie de Tobler, selon laquelle "chaque phénomène est relié à tous les autres, mais des phénomènes proches dans l’espace auront tendance à être d’avantage liés que des phénomènes éloignés" [Tobler, cité dans Fotheringham et al., 2000, p.26], ne peut que faire frémir le géographe. En effet, on peut admettre que "ce ne sont pas les distributions (d’observations) elles mêmes qui excitent le géographe, mais plutôt le fait que ces distributions varient en configuration et en intensité de place en place [...] La géographie n’aurait pas lieu d’être si l’ubiquité caractérisait toute chose" [Abler et al., 1971, p. 58]. En 1934, Stephan déclarait déjà : "les observations réalisées sur des unités spatiales sont liées les unes aux autres comme des grappes de raisins, et non pas séparées comme des boules dans une urne. Bien entendu, l’existence d’une simple contiguïté dans le temps et dans l’espace ne saurait impliquer une absence d’indépendance entre les valeurs prises par les attributs de ces unités géographiques, mais en travaillant sur des données sociales, nous savons que du fait même de leur caractéristique sociale, les personnes, les groupes ainsi que leurs caractéristiques, sont liés les uns aux autres et non indépendants. Des méthodes spécifiquement adaptées à la nature spatiale de cette information doivent donc être développées et en attendant, toute mesure statistique doit être utilisée avec la plus grande précaution" [Stephan, cité dans Cressie, 1993, p. 24].
Ces exigences de stationnarité et d’isotropie sont donc plus que de simples postulats pour un géographe. Contre-natures, elles représentent la négation même de sa raison d’être ! Par ailleurs, en favorisant la production automatisée d’indicateurs numériques uniques, censés résumer de manière représentative des distributions statistiques spatiales, c’est à dire attachées à des lieux, elles ont conduit depuis longtemps à nier la composante spatiale même de nombreux phénomènes. Ainsi que le remarque Noel Cressie, "parmi les sciences exactes, il est commun de penser que la seule source de variabilité relève d’erreurs de mesure, le plus souvent modélisées sous forme de processus bruités purement aléatoires" [Cressie, 1993, p. 25]. Pourtant, observer que, non seulement des phénomènes peuvent varier de place en place, mais que les relations mêmes entre ces phénomènes peuvent faire l’objet de fluctuations locales, relève avant tout du bon sens. "L’effet de lieu" est ainsi une réalité, étudiée depuis fort longtemps déjà en géographie et en sociologie électorale par exemple. Par ailleurs, d’un point de vue exploratoire, l’utilisation exclusive d’indicateurs globaux, supposés représenter correctement les inévitables variations spatiales locales, peut être interprétée comme une pratique fort peu optimale. Pourquoi s’imposer de telles contraintes, dont on peut aisément démontrer les faiblesses, alors même qu’une approche locale pourrait être tout à fait envisageable ? C’est justement l’un des points forts de l’analyse spatiale exploratoire des données que d’avoir vu, dans l’intérêt que porte l’EDA à l’individu statistique, les germes d’une revalorisation de la dimension locale en analyse spatiale. La quête de régularités, susceptibles de conduire à l’énoncé de règles sinon de lois, s’enrichit alors d’un regain d’intérêt pour les exceptions à ces règles, porteuses d’inattendu et d’étonnement.
Le lissage spatial par moyennes mobiles, pondérées ou non en fonction de la distance séparant les centroïdes des unités spatiales, est un premier exemple évident d’indicateur local. Un indicateur classique, la moyenne, y est estimé non pas sur l’ensemble d’un territoire, mais bien en chaque lieu de ce territoire, sur la base d’informations relatives au voisinage de chaque lieu. Potentiellement, tout indicateur statistique peut ainsi faire l’objet d’une application locale. Toutefois, les contraintes de programmation inhérentes à ce genre d’approche expliquent en grande partie leur faible diffusion. Commentant les travaux de Geary et Moran, ainsi que ceux de Cliff et Ord sur l’autocorrélation spatiale, Joël Charre remarquait ainsi en 1995 : le calcul de coefficients d’autocorrélation "...nécessite évidemment que l’on se donne une représentation formalisée de l’espace, sous forme d’une matrice de contiguïté ou de distances, information si lourde à construire qu’elle est presque systématiquement ignorée, même si les fonds de carte élaborés pour la cartographie assistée par ordinateur contiennent les données permettant de construire automatiquement ces matrices" [Charre, 1995, p.35].
En dépit de ces contraintes réelles, des versions locales de ces indicateurs fondamentaux en analyse spatiale ont été proposées [Anselin, 1995 ; Ord et Getis, 1995]. Luc Anselin propose ainsi de décomposer le coefficient d’autocorrélation de Moran en coefficients locaux, dont la somme est proportionnelle au coefficient global [Anselin, 1995]. Ces composantes locales, qui ne sont plus bornées entre -1 et 1 comme le coefficient global, peuvent alors être cartographiées, conduisant à la mise en évidence de structures spatiales locales. La figure 3 illustre ce principe, sur la base de données fournies par l’URCAM. La variable d’intérêt est ici la population du Doubs couverte par le régime général de l’assurance maladie et s’étant vu prescrire, entre 1998 et 1999, un médicament utilisé pour le traitement de l’asthme et des affections respiratoires.
Plusieurs remarques accompagnent cette figure : tout d’abord, il ne s’agit pas de remplacer le coefficient global par ses composantes locales, mais bien d’utiliser les deux dimensions conjointement. En effet, dans sa dimension locale, cet indicateur est particulièrement utile pour souligner des discontinuités spatiales, et il n’est pas rare qu’un coefficient global positif soit composé de contributions aussi bien positives que négatives. Par ailleurs, la significativité statistique des valeurs obtenues est ici cruciale, et plus encore dans la perspective locale, non bornée. Des formulations analytiques de ces tests statistiques existent, basées sur des hypothèses mathématiques prégnantes. Toutefois, en bon explorateur, on leur préfèrera des versions moins contraignantes, mais plus exigeantes en calculs. Un test de permutation, basé sur une simulation de type Monte Carlo, est un exemple de solution pratique, utilisant la distribution empirique disponible pour déterminer un seuil de significativité. Dans sa version globale, il s’agit de répartir les valeurs de la distribution au hasard dans l’espace, puis de calculer le coefficient de Moran pour chacune de ces pseudos distributions aléatoires. La proportion de coefficients simulés supérieurs ou égaux au coefficient initial "réel" donne ainsi une bonne estimation de la significativité de ce dernier. L’histogramme supérieur de la figure 3 illustre le résultat de cette simulation appliquée à la variable étudiée, et indique le coefficient obtenu ainsi que la valeur de la P-value estimée. Dans la version locale, une procédure proche peut être mise en oeuvre, impliquant des volumes de calcul considérables. Les P-values inférieures ou égales au seuil de 5% ont ainsi été sélectionnées sur l’histogramme inférieur, surlignant en blanc les contours des communes concernées.

Figure 3. Le coefficient d’autocorrélation de Moran, dans ses dimensions globale et locale, pour un ordre de voisinage unitaire
(Source des données : Urcam)
Ces indicateurs locaux d’association spatiale ont été appliqués dans des circonstances variées [Sokal et al., 1998 ; Kitron et al., 1997], mais aussi étendus à d’autres types d’objets spatiaux, qu’il s’agisse de semis de points [Banos, 2001a], de matrices de flux [Berglund et Karlstöm, 1999], ou de réseaux [Flahaut, 2001 ; Huguenin-Richard, 1999, 2000].
Dans le même esprit, il est possible de mettre en oeuvre des modèles locaux, s’appliquant de manière différenciée en tout point de l’espace. Lors de l’estimation d’un modèle statistique, tel que le modèle linéaire classique par exemple, on suppose en effet que les différents paramètres issus de la phase de calibration s’appliquent de manière constante à travers l’espace (stationnarité). En appliquant un modèle à chaque unité spatiale, sur la base de ses unités spatiales voisines, il est au contraire possible d’obtenir un aperçu des variations locales de ces paramètres. La régression géographique locale, imaginée par Brunsdon, Charlton et Fotheringham [Brunsdon et al., 1996 ; Charlton et al., 1997 ; Fotheringham et al., 2000], est sans doute l’exemple le plus achevé en la matière. La figure 4 illustre un exemple simple d’application de ce modèle local, toujours sur la base de données fournies par l’URCAM. La variable d’intérêt est ici la même (la population du Doubs couverte par le régime général de l’assurance maladie et s’étant vu prescrire, entre 1998 et 1999, un médicament utilisé pour le traitement de l’asthme et des affections respiratoires). Cette variable est naturellement étroitement corrélée à la distribution de la population totale dans le département, même si la contrainte d’homoscédasticité des résidus ne semble guère respectée. L’approche classique consiste alors à ajuster un modèle global à ce nuage de points (ici une droite), puis à cartographier les écarts à ce modèle (carte supérieure). Cette stratégie n’a rien d’une nouveauté [Brunet et Vanduick, 1975 ; Thomas, 1968], et figure même parmi les grands classiques de la discipline. La cartographie des coefficients de régression (pentes) de ce modèle linéaire appliqué, pour chaque unité spatiale, sur la base d’un voisinage par contiguïté d’ordre 2, offre une vision différente du phénomène (carte inférieure). Les pôles urbains font ainsi l’objet de relations locales plus vigoureuses que celles à l’œuvre dans les zones rurales, produisant une carte remarquablement proche, quoique inversée, de la carte des distances au médecin pneumologue le plus proche. Le second nuage de point confirme d’ailleurs bien cette relation inverse. Ce phénomène, non décelable à partir de l’examen des seuls résidus, conduit alors à une re-formulation du modèle, par l’intégration de cette variable spatiale sous une forme quadratique.

Figure 4. La régression géographique locale
(Source des données : URCAM)
On remarquera que le point le plus délicat dans la mise en oeuvre de la régression géographique locale concerne l’estimation de tests de significativité locaux : chaque paramètre du modèle doit en effet être analysé, et cela pour chacune des unités spatiales. La solution que nous proposons ici repose sur une simulation de type boostrap, consistant à comparer, pour chaque unité spatiale, chaque paramètre avec sa distribution empirique obtenue par tirage aléatoire avec remise. Le volume de calculs à mettre en oeuvre est donc une fois de plus considérable, mais permet de disposer d’indicateurs statistiques fiables. Les unités spatiales pour lesquelles les paramètres locaux sont suspects apparaissent ainsi sous forme de carrés rouges dans le nuage de points inférieur.
Ces quelques exemples ne sauraient être considérés comme représentatifs de l’ensemble des développements menés dans le champ bouillonnant de l’analyse spatiale exploratoire des données. Ils suffisent néanmoins à donner un premier aperçu de la philosophie de cette approche moderne, ainsi que de la puissance des outils mis en œuvre. Les rares implémentations logicielles disponibles aujourd’hui relèvent toutefois le plus souvent de prototypes de recherche et freinent incontestablement sa diffusion, y compris parmi la communauté des géographes. Cette situation est d’autant plus dommageable que l’analyse spatiale exploratoire des données pourrait constituer un terrain d’entente privilégié, pour les partisans et les opposants de la démarche modélisatrice en analyse spatiale.
