


%\chapter*{Part II Introduction}
\chapter*{Introduction de la Partie II}


% to have header for non-numbered introduction
%\markboth{Introduction of Part II}{Introduction of Part II}
\markboth{Introduction de la Partie II}{Introduction de la Partie II}


%\headercit{}{}{}


%---------------------------------------------------------------------------



\bigskip


% metaphore de metaphore de ... merde marche pas, et pas de fil directeur.
%. -> trouver fil directeur avec autres intros ? // conclusions ? // personnages ?


\textit{Il aura finalement pu le faire, ce voyage. Pas, ou très peu de villes. Quelle âme dans ces \emph{street} et \emph{avenues} perpendiculaires, qu'on traverse nécessairement en bagnole. Encore un plein, à croire que c'est fait exprès, pour le charme de l'odeur d'essence. Tiens ça serait amusant de regarder ce que racontent ces stations d'ailleurs, à garder en tête. Un aller-retour au pas de course au Mont Elbert, puis à Longs Peak. On sort bientôt du Colorado, faudra dire au revoir au gummy bears. Damn it, Denver est si proche, ça vaudrait la peine. Tant pis, the mountains are calling and I must go, comme dirait l'autre. Que connait-on finalement d'un territoire en conséquence de nos découvertes si sélectives ? Une infime partie du spectre des échelles ? Une infime étendue spatiale : on ne s'invente pas une dimension supplémentaire si facilement. Peut être au moins la prise de conscience des antagonismes, des dualités. Et la conscience d'avoir à chaque fois dû privilégier l'un des aspects. Pour faire des ponts il faut être préparé. Pour voir le monde par un regard qui en capture plusieurs, il faut déjà avoir \emph{compris}, c'est à dire intégré subjectivement, les processus correspondants. Souvenir d'une des premières courses sérieuses : les arêtes de la Meije, 23h consécutives pour terminer par des hallucinations sur le chemin à prendre que les étincelles des crampons sur les éboulis ne suffisaient plus à éclairer dans la nuit qui était retombée. Ce ressenti concret du gouffre de part et d'autre qui implique le tâtonnement, s'ancre dans l'inconscient avant même d'avoir atteint le stade des hallucinations : nous parcourons à chaque instant une fine arête, qui est autant celle de l'arbitraire du road trip que celle des ponts qui résistent difficilement quand vient la crue. Sur cette arête, les points d'ancrage bien évidemment solides mais aussi hétérogènes sont gages de vie : la diversité combat l'adversité.}

% suite des histoires : c'est quoi ma vie finalement ? quels souvenirs chers ? quelle identite ? psych necessaire ! catas - co ... ; a quelle periode, pourquoi, comment.
% \emph{A fine line}


\bigskip

%\stars


Un paradoxe intrinsèque à nombre de démarches de production de connaissance est un besoin de consistence intrinsèque et d'une portée satisfaisante d'explication des phénomènes concernés, qui s'oppose à une inévitable réduction des dimensions explorées mais également à la fragilité des ponts qu'elle tente de former vers d'autres corpus de connaissances. L'image prise ci-dessus suggère que le tâtonnement, c'est à dire une progression pas à pas sans précipitations, ainsi que la solidité des ancrages, sont des atouts solides pour affronter ce paradoxe.

% POURQUOI ces thématiques : rajouter un paragraphe !

Cette partie ouvre directement les pistes de réponse thématiques pour la modélisation de la co-évolution que nous avons évoqué en conclusion de la première partie, et pose ainsi ces ancrages forts. Elle pose toutefois des bases sans entrer dans le coeur du sujet par ce souci de robustesse par entrée progressive, et construit donc les \emph{briques élémentaires} de notre démarche. Deux chapitres traitent ainsi successivement les thématiques suivantes :
\begin{enumerate}
	\item Un premier chapitre s'intéresse à la Théorie Evolutive Urbaine est une entrée privilégiée sur les systèmes urbains d'un point de vue évolutif, et intègre en son coeur un point de vue multi-scalaire de ces systèmes. Il éclaire des propriétés fondamentale des systèmes territoriaux impliquées par la théorie évolutive, en introduisant une première analyse empirique de la variabilité spatiale des interactions entre forme urbaine et forme de réseau, puis en développant une méthodologie de caractérisation statistique de la co-évolution (au sens intermédiaire de la population). Il introduit ensuite un premier modèle d'interaction entre système de ville et flux du réseau de transport, avec réseau statique.
	\item Un second chapitre explore le concept de morphogenèse, qui permet une entrée conceptuelle à la caractéristique de modularité nécessaire pour avoir co-évolution. Après avoir développé une définition interdisciplinaire de la morphogenèse, il introduit un modèle de morphogenèse urbaine basé sur des processus d'agrégation-diffusion pour la densité de population, et est ensuite couplé séquentiellement à un modèle de génération de réseau.
\end{enumerate}







\stars





%---------------------------------------------------------------------------


\chapter*{Préliminaires mathématiques}

% to have header for non-numbered introduction
\markboth{Préliminaires mathématiques}{Préliminaires mathématiques}
%\markboth{Mathematical preliminaries}{Mathematical preliminaries}

\label{ch:preliminarymath}



Afin de toucher l'audience la plus large possible, nous proposons de preciser dans cet intermède préliminaire les definitions de notions ou méthodes clés qui seront utilisées de manière par la suite, souvent hors d'un cadre mathématique. Ce choix permet de garder un cadre rigoureux sans rendre indigeste la lecture du manuscrit à une grande partie de son public légitime. Sauf indication contraire, les specifications données ici feront référence lors de l'utilisation des termes correspondants.



\subsection*{Statistics}{Statistiques}


Nous noterons $\Pb{\cdot}$ une probabilité, $\Eb{\cdot}$ une espérance, $\hat{\mathbb{E}}\left[\cdot\right]$ un estimateur associé, et $\Covb{\cdot}{\cdot}$ une covariance.

\paragraph*{Correlation}{Corrélation}

Sauf indication contraire, nous estimerons la covariance entre deux processus par estimateur de Pearson, c'est à dire si $(X_i,Y_i)_i$ est un jeu d'observations des processus $X,Y$, la correlation est estimée par

\[
\hat{\rho} = \frac{\hat{\Cov}\left[X,Y\right]}{\sqrt{\hat{\sigma} \left[X\right] \cdot \hat{\sigma}\left[Y\right]}}
\]

où la covariance est estimée par l'estimateur non biaisé $\hat{\Cov}$.




\paragraph*{Granger causality}{Causalité de Granger}


Une série temporelle multi-dimensionnelle $\vec{X}(t)$ présente une causalité de Granger si avec
\[
\vec{X}(t) = \mathbf{A}\cdot \left(\vec{X}(t-\tau)\right)_{\tau >0} + \varepsilon
\]
il existe $\tau,i$ tels que $a_{i\tau}>0$ significativement. Nous utiliserons une version faible de la causalité de Granger, c'est à dire un test sur les correlations retardées définies par
\[
\rho_{\tau}\left[X_i,X_j\right] = \hat{\rho}\left[X_i(t-\tau),X_j(t)\right]
\]
avec $\tau$ retard ou avance. Cela nous permettra de quantifier des relations entre variables aléatoires définies dans l'espace et dans le temps.



\paragraph*{Geographically Weighted Regression}{Regression Géographique Pondérée}


La Régression Géographique Pondérée est une technique d'estimation de modèles statistiques permettant de prendre en compte la non-stationnarité spatiale des processus. Si $Y_i$ est une variable à expliquer et $X_i$ un jeu de variables explicatives, mesurés en des mêmes points de l'espace, on estime un modèle $Y_i = f(X_i,\vec{x}_i)$ à chaque point $\vec{x}_i$, en prenant en compte les observations par pondération spatiale autour du point, où les poids sont fixés par un noyau pouvant prendre plusieurs formes, par exemple un noyau exponentiel est de la forme
\[
w_i(\vec{x}) = \exp\left(- \norm{\vec{x} - \vec{x_i}} / d_0\right)
\]
L'échelle de stationnarité spatiale supposée par le modèle est alors de l'ordre de $d_0$. Celle-ci peut être ajustée par validation croisée par exemple.




\paragraph*{Machine learning}{Apprentissage statistique}


On désignera par \emph{Apprentissage supervisé} toute méthode d'estimation d'une relation entre variables $Y=f(X)$ où la valeur de $Y$ est connue sur un échantillon de données. On parlera de classification si la variable est discrete. La classification non-supervisée consiste à construire $Y$ lorsque seul $X$ est donné. On utilisera pour classifier une technique basique qui donne de bons résultats sur des données qui n'ont pas une structure exotique : la méthode des \emph{k-means}, répétée un nombre suffisant de fois pour prendre en compte son caractère stochastique. Le complexité du \emph{k-means} est polynomiale en moyenne, bien que la résolution exacte du problème de partition soit NP-difficile.




\paragraph*{Overfitting}{Overfitting}

% AIC

La question du sur-ajustement (\emph{overfitting}) est particulièrement importante lors de l'estimation de modèles, puisque un nombre trop important de paramètres pourra conduire a capturer le bruit de realisation comme structure. Lors de l'estimation de modèles statistiques, des critères d'information sont mobilisables pour quantifier le gain d'information produit par l'ajout d'un paramètre, et obtenir un compromis entre performance et parcimonie.


Le \emph{Critère d'Information d'Akaike} (AIC) permet de quantifier le gain d'information permis par l'ajout de paramètres dans un modèle. Pour un modèle statistique qui dispose d'une Fonction de Vraisemblance (\emph{Likelihood}), l'AIC est alors défini par
\[
AIC = 2k - 2 \ln{\mathcal{L}}
\]
si $k$ est le nombre de paramètres du modèle et $\mathcal{L}$ la valeur maximale de la fonction de vraisemblance. \cite{akaike1998information} montre que cette expression correspond à une estimation du gain d'information de Kullback-Leiber. Une correction pour les petits échantillons de taille $n$ est donnée par
\[
AICc = 2\cdot\left(k + \frac{k^2 + k}{n-k -1} - \ln{\mathcal{L}}\right)
\]

Un critère similaire mais dérivé dans un cadre bayésien est le \emph{Critère d'Information Bayésien} (BIC)~\cite{burnham2003model}, qui conduit à une pénalisation plus forte du nombre de paramètres : $BIC = \ln n \cdot k - 2 \ln{\mathcal{L}}$.


Ces critères sont appliqués pour la sélection de modèles en étudiant leur différences entre modèles (seules les différences ont un sens, ceux-ci étant définis à une constante près) : le ``meilleur'' modèle est celui ayant le critère le plus faible. Dans le cas de modèles de performance comparables, il peut être pertinent de combiner les modèles par les poids d'Akaike $w_i = \exp (- \Delta AIC / 2)$.


Cette question du sur-ajustement est également sous-jacente dans le cas des modèles de simulation, mais il n'existe à notre connaissance pas de méthode établie permettant de la traiter.



\subsection*{Stochastic Processes}{Processus stochastiques : Stationnarité}

%\subsubsection*{Stationarity}{Stationnarité}

Les propriétés de stationnarité informent sur la variabilité de la distribution d'un processus stochastique. Soit $(\vec{X}_i)_{i\in I}$ un processus stochastique multidimensionnel. Il sera dit fortement stationnaire si sa loi ne dépend pas de $i$, c'est à dire si $\Pb{\vec{X}_i} = \Pb{\vec{X}_{i+1}}$. La stationnarité forte implique l'égalité de tous les moments pour tout $i$.

Nous utiliserons une notion plus faible de la stationnarité des processus stochastiques, ou \emph{Weak Stationarity}, qui utilise les deux premiers moments : $(\vec{X}_i)_{i\in I}$ est faiblement stationnaire si
\begin{enumerate}
	\item $\Eb{\vec{X}_i} = \Eb{\vec{X}_0}$ pour tout $i$
	\item $\Covb{\vec{X}_i}{\vec{X}_j}$ ne dépend que de $i-j$
\end{enumerate}
On peut parler de stationnarité faible du premier ordre si seulement la condition sur l'espérance est vérifiée, et du second ordre si on a aussi la condition sur l'autocovariance~\cite{zhang2014test}.

% on spatial stationarity / spatio-temporal stationarity ? -> better be in 4.1



%%%%%%%%
%% -- ON HOLD --
%\subsubsection*{Ergodicity}{Ergodicité}

%La notion d'ergodicité sera utilisée en lien avec l'universalité et la dépendance au chemin des processus spatio-temporels. Un processus stochastique $(\vec{X}_i)_{i\in I}$ sera dit ergodique s'il y a correspondance entre moyenne dans l'espace d'indexation et l'espace des phases, c'est à dire si
%\[
%\Eb{X_i} = \frac{1}{|I|}\int_i X(i)di
%\]
%Dans le cas des processus spatio-temporels, l'indexation pourra être l'espace, le temps ou les deux.


%un processus stochastique spatio-temporel, défini à chaque instant $t$ et à chaque point de l'espace $\vec{x}$. En supposant que $X$ réalise l'ensemble de son espace des phases dans l'espace, il est dit ergodique si pour tous $t$ et tout $\vec{x}$, sa moyenne temporelle $\mathbb{E}_t\left[X(\vec{x})\right]$ est égale à sa moyenne spatiale $\mathbb{E}_{\vec{x}}\left[X(t)\right]$.


%%%%%%%%
%% -- ON HOLD -- not really used. (or check definition in 8.2)
% -> et en fait en lien avec l'ergodicité (chaos spatio-temporel) : serait un sujet en lui même !

%\subsection*{Dynamical systems}{Systèmes dynamiques}

%\paragraph{Chaos}{Chaos}


%Un système dynamique $\dot{X}=\Phi(X,t)$ sera dit chaotique si ses exposants de Liapounov sont strictement positifs dans une region non-négligeable de l'espace des conditions initiales. Pour rappel, le flot de Poincarré est défini de manière univoque par $X_0 \mapsto \Phi(X_0,t_0)$.



\subsection*{Model Exploration}{Exploration de modèles de simulation}

Nous désignerons par modèle de simulation tout algorithme associant une réalisation $\mathcal{M}\left[\vec{x},\vec{\alpha}\right]$ à des données $\vec{x}$ étant donné des paramètres $\alpha$. L'enjeu est alors de comprendre le comportement du modèle de manière empirique, en le simulant, possiblement avec plusieurs répétitions pour des mêmes paramètres si celui-ci est stochastique. Il est alors par exemple possible de calibrer le modèle, c'est à dire trouver un jeu de paramètres permettant de remplir des objectifs donnés (qui peuvent être des distances à des données observée).


\subsubsection*{Experience plan by Sampling}{Plan d'expérience par échantillonnage}

Le sort de la dimension (\emph{Dimensionality Curse}), qui correspond simplement au fait que la taille de l'espace des paramètres est exponentielle en le nombre de paramètre. Lorsque celle-ci grandit mais qu'on veut garder un aperçu du comportement d'un modèle sur des valeurs très variées des paramètres d'entrée, on peut alors échantillonner l'espace par un nombre donné de point.


L'échantillonnage par Hypercube Latin (LHS) permet d'assurer que pour chaque dimension, l'ensemble de la plage des valeurs est couverte lorsqu'on projette les points générés sur la dimension. L'échantillonnage par suite de Sobol permet de générer des points de discrépance faible (voir~\ref{app:sec:robustness} pour une définition précise de la discrépance, qu'il faut comprendre comme une couverture de l'espace), et est particulièrement adapté au calcul d'intégrales.


L'échantillonnage peut devenir laborieux si le modèle est très irrégulier, ou pour un objectif précis de calibration. Pour cela, il existe des algorithmes spécifiques d'exploration et de calibration pour lesquels nous pouvons donner des exemples.

\subsubsection*{Genetic algorithm calibration}{Calibration par algorithme génétique}

Les algorithmes génétiques sont une alternative largement utilisée en optimisation, et font plus généralement partie des méta-heuristiques de computation évolutionnaire~\cite{rey2015plateforme}. Nous utiliserons généralement pour la calibration des modèles l'algorithme standard implémenté dans OpenMole, décrit en détails par~\cite{pumain2017evaluation}. Il s'agit d'une extension stochastique de l'algorithme NSGA2 pour l'optimisation multi-objectif. Il possède les caractéristiques principales suivantes :
\begin{itemize}
	\item étant donné une population de paramètres candidats comme solution au problème multi-objectif, le front de Pareto est déterminé comme les points non-dominés ;
	\item un ensemble est construit à partir de ce front en prenant en compte une contrainte de diversité ;
	\item une descendance est générée à partir de cet ensemble par croisements et mutations et évaluée pour sa performance ;
	\item l'algorithme itère sur la nouvelle population.
\end{itemize}

\cite{pumain2017evaluation} ajoute l'objectif du nombre de réplications aux objectifs de l'algorithme, afin de prendre en compte la stochasticité et trouver un compromis entre optimalité et robustesse des solutions.


\subsubsection*{Specific algorithms}{Algorithmes spécifiques}

Se basant sur des algorithmes génétiques, divers algorithmes ont été proposés pour raffiner l'exploration des modèles. Mentionnons deux exemples développés dans le cadre d'OpenMole : l'algorithme PSE (\emph{Pattern Space Exploration})~\cite{10.1371/journal.pone.0138212} vise à découvrir l'ensemble de l'espace des sorties d'un modèle, dans l'idée d'une recherche de l'ensemble des comportements possibles. L'algorithme \emph{Calibration Profile}~\cite{reuillon2015} vise quant à lui à établir le caractère nécessaire d'un paramètre pour arriver à un objectif, indépendamment des autres paramètres.





\stars






