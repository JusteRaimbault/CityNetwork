


%\chapter*{Part II Introduction}{Introduction de la Partie II}
\chapter*{Introduction de la Partie II}


% to have header for non-numbered introduction
%\markboth{Introduction}{Introduction}


%\headercit{}{}{}


%---------------------------------------------------------------------------


















%---------------------------------------------------------------------------


\chapter*{Préliminaires mathématiques}


Afin de toucher l'audience la plus large possible, nous proposons de preciser dans cet intermède préliminaire les definitions de notions ou méthodes clés qui seront utilisées par la suite. Sauf indication, les specifications données ici feront reference lors de l'utilisation des termes correspondants.



\subsection*{Stochastic Processes}{Processus stochastiques}

\paragraph{Stationarity}{Stationnarité}

Nous utiliserons une notion faible de la stationnarité des processus stochastiques, puisqu'on la mobilisera pour des analyses empiriques sur lesquelles la taille des données ne permettra pas des tests sur les distributions completes. Soit $X_i$ un processus stochastique


\paragraph{Ergodicity}{Ergodicité}

La notion d'ergodicité sera utilisée dans le cadre de l'universalité des processus spatio-temporels. Soit $X(t,\vec{x})$ un processus stochastique spatio-temporel



\subsection*{Dynamical systems}{Systèmes dynamiques}

\paragraph{Chaos}{Chaos}


Un système dynamique $\dot{X}=\Phi(X,t)$ sera dit chaotique si ses exposants de Liapounov sont strictement positifs dans une region non-négligeable de l'espace des conditions initiales. Pour rappel, le flot de Poincarré est défini de manière univoque par $X_0 \mapsto \Phi(X_0,t_0)$.


\subsection*{Agent-based modeling}{Modélisation basée-agents}





\subsection*{Statistics}{Statistiques}


\paragraph{Correlation}{Correlation}

Sauf indication contraire, nous estimerons la covariance entre deux processus par estimateur de Pearson non-biaisé, c'est à dire si $(X_i,Y_i)_i$ est un jeu d'observations des processus $X,Y$, la correlation est estimée par

\[
\hat{\rho} = \frac{\hat{\Varb{XY}}}{\sqrt{\hat{\sigma} \left[X\right] \cdot \hat{\sigma}\left[Y\right]}}
\]




\paragraph{Granger causality}{Causalité de Granger}


Une série temporelle multi-dimensionelle $X(t)$ présente une causalité de Granger si avec $X(t) = A\cdot \left(X(t-\tau)\right)_{\tau >0} + \varepsilon$, il existe $\tau,i$ tels que $a_{i\tau}>0$ significativement. On peut montrer que dans ce cas $\rho_{\tau}=a_{i\tau}^2$. Nous utiliserons cette version faible de la causalité de Granger, c'est à dire un test sur les correlations retardées.




\paragraph{Geographically Weighted Regression}{Regression Géographique Pondérée}


La Régression Géographique Pondérée est une technique d'estimation de modèles statistiques permettant de prendre en compte la non-stationnarité spatiale des processus. Si $Y_i$ est une variable à expliquer et $X_i$ un jeu de variables explicatives, mesurés en des mêmes points de l'espace, on estime un modèle $Y_i = f(X_i,\vec{x}_i)$ à chaque point $\vec{x}_i$, en prenant en compte les observations par pondération spatiale autour du point, où les poids sont fixés par un noyau pouvant prendre plusieurs formes (par exemple noyau exponentiel $w_i(\vec{x}) = \exp\left(- \norm{\vec{x} - \vec{x_i}} / d_0\right)$).




\paragraph{Machine learning}{Apprentissage statistique}


On désignera par \emph{Apprentissage supervisé} toute méthode d'estimation d'une relation entre variables $Y=f(X)$ où la valeur de $Y$ est connue sur un échantillon de données. On parlera de classification si la variable est discrete. La classification non-supervisée consiste à construire $Y$ lorsque seul $X$ est donné. On utilisera pour classifier une technique basique qui donne de bons résultats sur des données qui n'ont pas une structure exotique : la méthode des \emph{k-means}, répétée un nombre suffisant de fois pour prendre en compte son caractère stochastique. Le complexité du \emph{k-means} est polynomiale en moyenne, bien que la résolution exacte du problème de partition soit NP-difficile.




\paragraph{Overfitting}{Overfitting}

% AIC

La question de l'overfitting est particulièrement importante lors de l'estimation de modèles, puisque un nombre trop important de paramètres pourra conduire a capturer le bruit de realisation comme structure. Lors de l'estimation de modèles statistiques, des critères d'information sont mobilisables pour quantifier le gain d'information produit par l'ajout d'un paramètre, et obtenir un compromis entre performance et parcimonie.





\subsection*{Model Exploration}{Exploration de modèles}


\paragraph{Discrepancy}{Discrépance}




\paragraph{LHS Sampling}{Echantillonnage LHS}




\paragraph{NSGA}{NSGA}







