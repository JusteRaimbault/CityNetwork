


%\chapter*{Part II Introduction}
\chapter*{Introduction de la Partie II}


% to have header for non-numbered introduction
%\markboth{Introduction of Part II}{Introduction of Part II}
\markboth{Introduction de la Partie II}{Introduction de la Partie II}


%\headercit{}{}{}


%---------------------------------------------------------------------------



\bigskip


% metaphore de metaphore de ... merde marche pas, et pas de fil directeur.
%. -> trouver fil directeur avec autres intros ? // conclusions ? // personnages ?


\bpar{
\textit{He finally would have realized his trip. No cities, or a very few. Which soul in these perpendicular \emph{streets} and \emph{avenues}, that we necessarily discover by car. Fill the tank again, maybe it's on purpose, just for the charm of the fuel smell. Anyway it would be funny to look at what these stations have to say, to keep in mind. A running return journey to Mount Elbert, then Longs Peak. Soon out of Colorado, goodbye my gummy bears. Damn it, so close to Denver, it could be worth it. Never mind, the mountains are calling and I must go, as someone used to say. What do we finally know of a territory as a consequences of our so selective discoveries ? A very narrow band on the spectrum of scales ? A tiny spatial extent: one supplementary dimension is not invented so easily. Maybe at least an awakening of conscience for antagonisms, of dualities. And the conscience to necessarily choose one of the aspects each time. To build bridges one must be prepared. To see the world with an eye catching multiple views, one must already have \emph{understood}, i.e. subjectively integrated, the corresponding processes. Memory of one of the first serious routes: Meije ridge traverse, 23 hours without interruption to end with hallucinations on the path to follow whereas the sparkles of crampons on the scree were not anymore enough to light the fallen night. This concrete feeling of the void on each side which imposes to grope, anchors in the subconscious even before reaching the stage of hallucinations: we travel at each moment a fine line, which is as much that of the arbitrariness of road trips as that of the bridges which difficultly resist the flood. On this ridge, anchors that are of course strong but also heterogenous are a pledge of life: diversity overcomes adversity.}
}{
\textit{Il aura finalement pu le faire, ce voyage. Pas, ou très peu de villes. Quelle âme dans ces \emph{streets} et \emph{avenues} perpendiculaires, qu'on traverse nécessairement en bagnole. Encore un plein, à croire que c'est fait exprès, pour le charme de l'odeur d'essence. Tiens ça serait amusant de regarder ce que racontent ces stations d'ailleurs, à garder en tête. Un aller-retour au pas de course au Mont Elbert, puis à Longs Peak. On sort bientôt du Colorado, faudra dire au revoir au gummy bears. Damn it, Denver est si proche, ça vaudrait la peine. Tant pis, the mountains are calling and I must go, comme dirait l'autre. Que connait-on finalement d'un territoire en conséquence de nos découvertes si sélectives ? Une infime partie du spectre des échelles ? Une infime étendue spatiale : on ne s'invente pas une dimension supplémentaire si facilement. Peut être au moins la prise de conscience des antagonismes, des dualités. Et la conscience d'avoir à chaque fois dû privilégier l'un des aspects. Pour faire des ponts il faut être préparé. Pour voir le monde par un regard qui en capture plusieurs, il faut déjà avoir \emph{compris}, c'est-à-dire intégré subjectivement, les processus correspondants. Souvenir d'une des premières courses sérieuses : les arêtes de la Meije, 23h consécutives pour terminer par des hallucinations sur le chemin à prendre que les étincelles des crampons sur les éboulis ne suffisaient plus à éclairer dans la nuit qui était retombée. Ce ressenti concret du gouffre de part et d'autre qui implique le tâtonnement, s'ancre dans l'inconscient avant même d'avoir atteint le stade des hallucinations : nous parcourons à chaque instant une fine arête, qui est autant celle de l'arbitraire du road trip que celle des ponts qui résistent difficilement quand vient la crue. Sur cette arête, les points d'ancrage bien évidemment solides mais aussi hétérogènes sont gages de vie : la diversité combat l'adversité.}
}


% suite des histoires : c'est quoi ma vie finalement ? quels souvenirs chers ? quelle identite ? psych necessaire ! catas - co ... ; a quelle periode, pourquoi, comment.
% \emph{A fine line}


\bigskip

%\stars



\bpar{
A paradox which is intrinsic to numerous knowledge production approaches is a need of an intrinsic consistence and of a reasonable reach of explication for concerned phenomena, which is opposed to an inevitable reduction of explored dimensions, but also to the fragility of bridges that it aims at creating towards other corpora of knowledge. The image we took above suggests that groping, i.e. a step-by-step progression without precipitation, and also the solidity of anchors, are solid assets to tackle this paradox.
}{
Un paradoxe intrinsèque à nombre de démarches de production de connaissance est un besoin de consistence intrinsèque et d'une portée satisfaisante d'explication des phénomènes concernés, qui s'oppose à une inévitable réduction des dimensions explorées mais également à la fragilité des ponts qu'elle tente de former vers d'autres corpus de connaissances. L'image prise ci-dessus suggère que le tâtonnement, c'est-à-dire une progression pas à pas sans précipitations, ainsi que la solidité des ancrages, sont des atouts solides pour affronter ce paradoxe.
}


% POURQUOI ces thématiques : rajouter un paragraphe !


\bpar{
This part directly opens thematic directions of answer for modeling co-evolution that we mentioned when concluding the first part, and thus builds these strong anchors. It however constructs basements without going into the heart of the problem in a spirit of robustness through progressive entries, and constructs therefore the \emph{elementary bricks} of our approach. Two chapters deal thus successively with the following thematics:
\begin{enumerate}
	\item A first chapter focuses on the evolutive urban theory, which is a privileged entry on urban systems from an evolutive point of view, and integrates in its core a multi-scalar approach to these systems. It unveils fundamental properties of territorial systems implied by the evolutive theory, by introducing a first empirical analysis of the spatial variability of interactions between urban form and network topology, then by developing a methodology to statistically characterize co-evolution (in the intermediate sense of population). It introduces then a first model of interaction between urban system and flows of the transportation network, with a static network.
	\item A second chapter explores the concept of morphogenesis, which allows a conceptual entry to the characteristic of modularity necessary to have co-evolution. After having developed an interdisciplinary definition of morphogenesis, it introduces a model of urban morphogenesis based on aggregation-diffusion processes for population density, and is then sequentially coupled to a network generation model.
\end{enumerate}
}{
Cette partie ouvre directement les pistes de réponse thématiques pour la modélisation de la co-évolution que nous avons évoqué en conclusion de la première partie, et pose ainsi ces ancrages forts. Elle pose toutefois des bases sans entrer dans le coeur du sujet par ce souci de robustesse par entrée progressive, et construit donc les \emph{briques élémentaires} de notre démarche. Deux chapitres traitent ainsi successivement les thématiques suivantes :
\begin{enumerate}
	\item Un premier chapitre s'intéresse à la théorie évolutive des villes, qui est une entrée privilégiée sur les systèmes urbains d'un point de vue évolutif, et intègre en son coeur un point de vue multi-scalaire de ces systèmes. Il éclaire des propriétés fondamentale des systèmes territoriaux impliquées par la théorie évolutive, en introduisant une première analyse empirique de la variabilité spatiale des interactions entre forme urbaine et forme de réseau, puis en développant une méthodologie de caractérisation statistique de la co-évolution (au sens intermédiaire de la population). Il introduit ensuite un premier modèle d'interaction entre système de ville et flux du réseau de transport, avec réseau statique.
	\item Un second chapitre explore le concept de morphogenèse, qui permet une entrée conceptuelle à la caractéristique de modularité nécessaire pour avoir co-évolution. Après avoir développé une définition interdisciplinaire de la morphogenèse, il introduit un modèle de morphogenèse urbaine basé sur des processus d'agrégation-diffusion pour la densité de population, et est ensuite couplé séquentiellement à un modèle de génération de réseau.
\end{enumerate}
}






\stars





%---------------------------------------------------------------------------


\chapter*{\bpar{Mathematical preliminaries}{Préliminaires mathématiques}}

% to have header for non-numbered introduction
\markboth{Préliminaires mathématiques}{Préliminaires mathématiques}
%\markboth{Mathematical preliminaries}{Mathematical preliminaries}

\label{ch:preliminarymath}



\bpar{
In order to be readable by the largest audience possible, we propose to precise in the preliminary interlude the definitions of notions or key methods that will be regularly used in the following, often out of a mathematical framework. This choice allows to keep a rigorous frame without making indigestible the reading of this manuscript to a large part of its legitimate audience. Without noted otherwise, the specifications given here will be the reference during the use of the corresponding terms.
}{
Afin de toucher l'audience la plus large possible, nous proposons de preciser dans cet intermède préliminaire les definitions de notions ou méthodes clés qui seront utilisées de manière régulière par la suite, souvent hors d'un cadre mathématique. Ce choix permet de garder un cadre rigoureux sans rendre indigeste la lecture du manuscrit à une grande partie de son public légitime. Sauf indication contraire, les specifications données ici feront référence lors de l'utilisation des termes correspondants.
}


\subsection*{Statistics}{Statistiques}


\bpar{
We will denote by $\Pb{\cdot}$ a probability, $\Eb{\cdot}$ an expectation, $\hat{\mathbb{E}}\left[\cdot\right]$ an associated estimator, and $\Covb{\cdot}{\cdot}$ a covariance.
}{
Nous noterons $\Pb{\cdot}$ une probabilité, $\Eb{\cdot}$ une espérance, $\hat{\mathbb{E}}\left[\cdot\right]$ un estimateur associé, et $\Covb{\cdot}{\cdot}$ une covariance.
}


\paragraph*{Correlation}{Corrélation}

\bpar{
Unless otherwise stated, we will estimate the covariance between two processes with a Pearson estimator, i.e. if $(X_i,Y_i)_i$ is a set of observations of processes $X,Y$, the correlation is estimated by
}{
Sauf indication contraire, nous estimerons la covariance entre deux processus par estimateur de Pearson, c'est-à-dire si $(X_i,Y_i)_i$ est un jeu d'observations des processus $X,Y$, la correlation est estimée par
}

\[
\hat{\rho} = \frac{\hat{\Cov}\left[X,Y\right]}{\sqrt{\hat{\Cov} \left[X\right] \cdot \hat{\Cov}\left[Y\right]}}
\]

\bpar{
where the covariance is estimated with the unbiased estimator $\hat{\Cov}$.
}{
où la covariance est estimée par l'estimateur non biaisé $\hat{\Cov}$.
}



\paragraph*{Granger causality}{Causalité de Granger}

\bpar{
A multi-dimensional time-series $\vec{X}(t)$ exhibits a Granger causality if with
\[
\vec{X}(t) = \mathbf{A}\cdot \left(\vec{X}(t-\tau)\right)_{\tau >0} + \varepsilon
\]
there exists $\tau,i$ such that $a_{i\tau}>0$ significantly. We will use a weak version of Granger causality, i.e. a test on lagged correlations defined by
\[
\rho_{\tau}\left[X_i,X_j\right] = \hat{\rho}\left[X_i(t-\tau),X_j(t)\right]
\]
with $\tau$ lag or advance. This will allow us to quantify relations between random variables defined in space and time.
}{
Une série temporelle multi-dimensionnelle $\vec{X}(t)$ présente une causalité de Granger si avec
\[
\vec{X}(t) = \mathbf{A}\cdot \left(\vec{X}(t-\tau)\right)_{\tau >0} + \varepsilon
\]
il existe $\tau,i$ tels que $a_{i\tau}>0$ significativement. Nous utiliserons une version faible de la causalité de Granger, c'est-à-dire un test sur les correlations retardées définies par
\[
\rho_{\tau}\left[X_i,X_j\right] = \hat{\rho}\left[X_i(t-\tau),X_j(t)\right]
\]
avec $\tau$ retard ou avance. Cela nous permettra de quantifier des relations entre variables aléatoires définies dans l'espace et dans le temps.
}


\paragraph*{Geographically Weighted Regression}{Regression Géographique Pondérée}


\bpar{
The Geographically Weighted Regression is an estimation technique for statistical models that allow to take into account the spatial non-stationarity of processes. If $Y_i$ is an explicated variable and $X_i$ a set of explicative variables, measures in the same points in space, we estimate a model $Y_i = f(X_i,\vec{x}_i)$ at each point $\vec{x}_i$, by taking into account the observations with spatial weighting around the point, where weights are fixed by a kernel that can take several forms, for example an exponential kernel is of the form
\[
w_i(\vec{x}) = \exp\left(- \norm{\vec{x} - \vec{x_i}} / d_0\right)
\]
The stationarity scale assumed by the model is then of the same order as $d_0$. It can be adjusted by cross-validation for example.
}{
La Régression Géographique Pondérée est une technique d'estimation de modèles statistiques permettant de prendre en compte la non-stationnarité spatiale des processus. Si $Y_i$ est une variable à expliquer et $X_i$ un jeu de variables explicatives, mesurés en des mêmes points de l'espace, on estime un modèle $Y_i = f(X_i,\vec{x}_i)$ à chaque point $\vec{x}_i$, en prenant en compte les observations par pondération spatiale autour du point, où les poids sont fixés par un noyau pouvant prendre plusieurs formes, par exemple un noyau exponentiel est de la forme
\[
w_i(\vec{x}) = \exp\left(- \norm{\vec{x} - \vec{x_i}} / d_0\right)
\]
L'échelle de stationnarité spatiale supposée par le modèle est alors de l'ordre de $d_0$. Celle-ci peut être ajustée par validation croisée par exemple.
}



\paragraph*{Machine learning}{Apprentissage statistique}


\bpar{
We will designate by \emph{Supervised Learning} any method to estimate a relation between variables $Y=f(X)$ where the value of $Y$ is known on a data sample. This will be a classification when the variable is discrete. Non-supervised classification consists in constructing $Y$ when only $X$ is given. In order to classify data, we will use a basic technique which gives good results on data without an exotic structure: the method of \emph{k-means}, repeated a sufficient number of times to take into account its stochastic character. The complexity of \emph{k-means} is in average polynomial, even if the exact solution of the partitioning problem is NP-hard.
}{
On désignera par \emph{Apprentissage supervisé} toute méthode d'estimation d'une relation entre variables $Y=f(X)$ où la valeur de $Y$ est connue sur un échantillon de données. On parlera de classification si la variable est discrete. La classification non-supervisée consiste à construire $Y$ lorsque seul $X$ est donné. On utilisera pour classifier une technique basique qui donne de bons résultats sur des données qui n'ont pas une structure exotique : la méthode des \emph{k-means}, répétée un nombre suffisant de fois pour prendre en compte son caractère stochastique. Le complexité du \emph{k-means} est polynomiale en moyenne, bien que la résolution exacte du problème de partition soit NP-difficile.
}



\paragraph*{Overfitting}{Overfitting}

% AIC

\bpar{
The issue of \emph{overfitting} is particularly important during the estimation of models, since a too large number of parameters can lead to the capture of the realization noise as a structure. During the estimation of statistical models, information criteria can be used to quantify the gain in information produced by the addition of a parameter, and obtain a compromise between performance and parcimony.
}{
La question du sur-ajustement (\emph{overfitting}) est particulièrement importante lors de l'estimation de modèles, puisque un nombre trop important de paramètres pourra conduire a capturer le bruit de realisation comme structure. Lors de l'estimation de modèles statistiques, des critères d'information sont mobilisables pour quantifier le gain d'information produit par l'ajout d'un paramètre, et obtenir un compromis entre performance et parcimonie.
}


\bpar{
The \emph{Akaike Information Criteria} (AIC) allows to quantify the gain in information allowed by the addition of parameters in a model. For a statistical model which has a Likelihood function, the AIC is then defined by
\[
AIC = 2k - 2 \ln{\mathcal{L}}
\]
if $k$ is the number of parameters in the model and $\mathcal{L}$ the maximal value of the likelihood function. \cite{akaike1998information} shows that this expression corresponds to an estimation of the gain in Kullback-Leibler information. A correction for small samples of size $n$ is given by
\[
AICc = 2\cdot\left(k + \frac{k^2 + k}{n-k -1} - \ln{\mathcal{L}}\right)
\]
}{
Le \emph{Critère d'Information d'Akaike} (AIC) permet de quantifier le gain d'information permis par l'ajout de paramètres dans un modèle. Pour un modèle statistique qui dispose d'une Fonction de Vraisemblance (\emph{Likelihood}), l'AIC est alors défini par
\[
AIC = 2k - 2 \ln{\mathcal{L}}
\]
si $k$ est le nombre de paramètres du modèle et $\mathcal{L}$ la valeur maximale de la fonction de vraisemblance. \cite{akaike1998information} montre que cette expression correspond à une estimation du gain d'information de Kullback-Leibler. Une correction pour les petits échantillons de taille $n$ est donnée par
\[
AICc = 2\cdot\left(k + \frac{k^2 + k}{n-k -1} - \ln{\mathcal{L}}\right)
\]
}



\bpar{
A similar criteria but derived within a bayesian framework is the \emph{Bayesian Information Criterion} (BIC)~\cite{burnham2003model}, which leads to a stronger penalty for the number of parameters: $BIC = \ln n \cdot k - 2 \ln{\mathcal{L}}$.
}{
Un critère similaire mais dérivé dans un cadre bayésien est le \emph{Critère d'Information Bayésien} (BIC)~\cite{burnham2003model}, qui conduit à une pénalisation plus forte du nombre de paramètres : $BIC = \ln n \cdot k - 2 \ln{\mathcal{L}}$.
}

\bpar{
These criteria are applied to model selection by studying their differences between models (only differences have a meaning, since they are defined with an arbitrary constant): the ``best'' model is the one having the lowest criterion. In the case of models with comparable performances, it can be more relevant to combine models with the Akaike weights $w_i = \exp (- \Delta AIC / 2)$.
}{
Ces critères sont appliqués pour la sélection de modèles en étudiant leur différences entre modèles (seules les différences ont un sens, ceux-ci étant définis à une constante près) : le ``meilleur'' modèle est celui ayant le critère le plus faible. Dans le cas de modèles de performance comparables, il peut être pertinent de combiner les modèles par les poids d'Akaike $w_i = \exp (- \Delta AIC / 2)$.
}


\bpar{
This issue of overfitting is also implicit in the case of simulation models, but to the best of our knowledge there does not exist an established method allowing to tackle it.
}{
Cette question du sur-ajustement est également sous-jacente dans le cas des modèles de simulation, mais il n'existe à notre connaissance pas de méthode établie permettant de la traiter.
}


\subsection*{Stochastic processes: stationarity}{Processus stochastiques : stationnarité}

%\subsubsection*{Stationarity}{Stationnarité}


\bpar{
Stationarity properties inform on the variability of the distribution of a stochastic process. Let $(\vec{X}_i)_{i\in I}$ a multidimensional stochastic process. It will be said to be strongly stationary if its law does not depends on $i$, i.e. if $\Pb{\vec{X}_i} = \Pb{\vec{X}_{i+1}}$. Strong stationarity implies the equality of all moments for all $i$.
}{
Les propriétés de stationnarité informent sur la variabilité de la distribution d'un processus stochastique. Soit $(\vec{X}_i)_{i\in I}$ un processus stochastique multidimensionnel. Il sera dit fortement stationnaire si sa loi ne dépend pas de $i$, c'est-à-dire si $\Pb{\vec{X}_i} = \Pb{\vec{X}_{i+1}}$. La stationnarité forte implique l'égalité de tous les moments pour tout $i$.
}


\bpar{
We will use a wealer notion of stationarity for stochastic processes, or \emph{Weak Stationarity}, which uses the first two moments: $(\vec{X}_i)_{i\in I}$ is weakly stationary if
\begin{enumerate}
	\item $\Eb{\vec{X}_i} = \Eb{\vec{X}_0}$ for all $i$
	\item $\Covb{\vec{X}_i}{\vec{X}_j}$ depends only on $i-j$
\end{enumerate}
We can have a weak stationarity at the first order if only the condition on the expectation is verified, and at the second order if there is also the condition on autocovariance~\cite{zhang2014test}.
}{
Nous utiliserons une notion plus faible de la stationnarité des processus stochastiques, ou \emph{Weak Stationarity}, qui utilise les deux premiers moments : $(\vec{X}_i)_{i\in I}$ est faiblement stationnaire si
\begin{enumerate}
	\item $\Eb{\vec{X}_i} = \Eb{\vec{X}_0}$ pour tout $i$
	\item $\Covb{\vec{X}_i}{\vec{X}_j}$ ne dépend que de $i-j$
\end{enumerate}
On peut parler de stationnarité faible du premier ordre si seulement la condition sur l'espérance est vérifiée, et du second ordre si on a aussi la condition sur l'autocovariance~\cite{zhang2014test}.
}


% on spatial stationarity / spatio-temporal stationarity ? -> better be in 4.1



%%%%%%%%
%% -- ON HOLD --
%\subsubsection*{Ergodicity}{Ergodicité}

%La notion d'ergodicité sera utilisée en lien avec l'universalité et la dépendance au chemin des processus spatio-temporels. Un processus stochastique $(\vec{X}_i)_{i\in I}$ sera dit ergodique s'il y a correspondance entre moyenne dans l'espace d'indexation et l'espace des phases, c'est-à-dire si
%\[
%\Eb{X_i} = \frac{1}{|I|}\int_i X(i)di
%\]
%Dans le cas des processus spatio-temporels, l'indexation pourra être l'espace, le temps ou les deux.


%un processus stochastique spatio-temporel, défini à chaque instant $t$ et à chaque point de l'espace $\vec{x}$. En supposant que $X$ réalise l'ensemble de son espace des phases dans l'espace, il est dit ergodique si pour tous $t$ et tout $\vec{x}$, sa moyenne temporelle $\mathbb{E}_t\left[X(\vec{x})\right]$ est égale à sa moyenne spatiale $\mathbb{E}_{\vec{x}}\left[X(t)\right]$.


%%%%%%%%
%% -- ON HOLD -- not really used. (or check definition in 8.2)
% -> et en fait en lien avec l'ergodicité (chaos spatio-temporel) : serait un sujet en lui même !

%\subsection*{Dynamical systems}{Systèmes dynamiques}

%\paragraph{Chaos}{Chaos}


%Un système dynamique $\dot{X}=\Phi(X,t)$ sera dit chaotique si ses exposants de Liapounov sont strictement positifs dans une region non-négligeable de l'espace des conditions initiales. Pour rappel, le flot de Poincarré est défini de manière univoque par $X_0 \mapsto \Phi(X_0,t_0)$.



\subsection*{Exploration of simulation models}{Exploration de modèles de simulation}


\bpar{
We will designate by simulation model any algorithm that associates a realization $\mathcal{M}\left[\vec{x},\vec{\alpha}\right]$ to data $\vec{x}$ given parameters $\vec{\alpha}$. The question is then to understand the behavior of the model in an empirical way, by simulating it, possibly with several repetitions for the same parameters if it is stochastic. It is then for example possible to calibrate the model, i.e. find a set of parameters allowing to fulfill given objectives (that can be distances to observed data). 
}{
Nous désignerons par modèle de simulation tout algorithme associant une réalisation $\mathcal{M}\left[\vec{x},\vec{\alpha}\right]$ à des données $\vec{x}$ étant donnés des paramètres $\vec{\alpha}$. L'enjeu est alors de comprendre le comportement du modèle de manière empirique, en le simulant, possiblement avec plusieurs répétitions pour des mêmes paramètres si celui-ci est stochastique. Il est alors par exemple possible de calibrer le modèle, c'est-à-dire trouver un jeu de paramètres permettant de remplir des objectifs donnés (qui peuvent être des distances à des données observées).
}


\subsubsection*{Experience plan by sampling}{Plan d'expérience par échantillonnage}


\bpar{
The dimensionality curse corresponds to the fact that the size of the parameter space is exponential in the number of parameters. When it increases but we want to keep an overview of the behavior of a model on a large variety of input parameters, we can sample the space with a given number of points.
}{
Le sort de la dimension (\emph{Dimensionality Curse}) correspond simplement au fait que la taille de l'espace des paramètres est exponentielle en le nombre de paramètres. Lorsque celle-ci grandit mais qu'on veut garder un aperçu du comportement d'un modèle sur des valeurs très variées des paramètres d'entrée, on peut alors échantillonner l'espace par un nombre donné de points.
}


\bpar{
The Latin Hypercube Sampling (LHS) allows to ensure that for each dimension, the full range of values is covered when generated points are projected on the dimension. The Sobol sampling allows to generate point clouds with a weak discrepancy (see~\ref{app:sec:robustness} for a precise definition of discrepancy, that should be understood as a covering of space), and is particularly suited for the computation of integrals.
}{
L'échantillonnage par Hypercube Latin (LHS) permet d'assurer que pour chaque dimension, l'ensemble de la plage des valeurs est couverte lorsqu'on projette les points générés sur la dimension. L'échantillonnage par suite de Sobol permet de générer des points de discrépance faible (voir~\ref{app:sec:robustness} pour une définition précise de la discrépance, qu'il faut comprendre comme une couverture de l'espace), et est particulièrement adapté au calcul d'intégrales.
}


\bpar{
Sampling can become cumbersome is the model is very irregular, or for a precise calibration objective. Therefore, there exists specific algorithms for exploration and calibration, for which we can give some examples.
}{
L'échantillonnage peut devenir laborieux si le modèle est très irrégulier, ou pour un objectif précis de calibration. Pour cela, il existe des algorithmes spécifiques d'exploration et de calibration pour lesquels nous pouvons donner des exemples.
}


\subsubsection*{Genetic algorithm calibration}{Calibration par algorithme génétique}


\bpar{
Genetic algorithms are an alternative largely used in optimization, and are more generally a case of evolutionary computation meta-heuristics~\cite{rey2015plateforme}. We will generally use for the calibration of models the standard algorithm implemented in OpenMole, described in details by~\cite{pumain2017evaluation}. It is a stochastic extension of the NSGA2 algorithm for multi-objective optimization. It has the following main characteristics:
\begin{itemize}
	\item given a population of parameters that are candidates as solutions of the multi-objective problem, the Pareto front is determined as the non-dominated points;
	\item a set is constructed from this front by taking a constraint of diversity into account;
	\item an offspring is generated from this set by crossovers and mutations, and evaluated for its performance;
	\item the algorithm iterates on the new population. 
\end{itemize}
}{
Les algorithmes génétiques sont une alternative largement utilisée en optimisation, et font plus généralement partie des méta-heuristiques de computation évolutionnaire~\cite{rey2015plateforme}. Nous utiliserons généralement pour la calibration des modèles l'algorithme standard implémenté dans OpenMole, décrit en détails par~\cite{pumain2017evaluation}. Il s'agit d'une extension stochastique de l'algorithme NSGA2 pour l'optimisation multi-objectif. Il possède les caractéristiques principales suivantes :
\begin{itemize}
	\item étant donné une population de paramètres candidats comme solution au problème multi-objectif, le front de Pareto est déterminé comme les points non-dominés ;
	\item un ensemble est construit à partir de ce front en prenant en compte une contrainte de diversité ;
	\item une descendance est générée à partir de cet ensemble par croisements et mutations et évaluée pour sa performance ;
	\item l'algorithme itère sur la nouvelle population.
\end{itemize}
}


\bpar{
\cite{pumain2017evaluation} adds the objective of the number of replications to the objectives of the algorithm, in order to take into account stochasticity and find a compromise between optimality and robustness of solutions.
}{
\cite{pumain2017evaluation} ajoute l'objectif du nombre de réplications aux objectifs de l'algorithme, afin de prendre en compte la stochasticité et trouver un compromis entre optimalité et robustesse des solutions.
}


\subsubsection*{Specific algorithms}{Algorithmes spécifiques}

\bpar{
Based on genetic algorithms, various algorithms have been proposed to refine model exploration. We can mention two examples developed in the frame of OpenMole: the \emph{Pattern Space Exploration} algorithm (PSE)~\cite{10.1371/journal.pone.0138212} aims at discovering the set of outputs of a model, in the idea of a search for all feasible behaviors. The \emph{Calibration Profile} algorithm~\cite{reuillon2015} aims on the other hand at establishing the necessary character of a parameter to fulfill an objective, independently of other parameters.
}{
Se basant sur des algorithmes génétiques, divers algorithmes ont été proposés pour raffiner l'exploration des modèles. Mentionnons deux exemples développés dans le cadre d'OpenMole : l'algorithme PSE (\emph{Pattern Space Exploration})~\cite{10.1371/journal.pone.0138212} vise à découvrir l'ensemble de l'espace des sorties d'un modèle, dans l'idée d'une recherche de l'ensemble des comportements possibles. L'algorithme \emph{Calibration Profile}~\cite{reuillon2015} vise quant à lui à établir le caractère nécessaire d'un paramètre pour arriver à un objectif, indépendamment des autres paramètres.
}




\stars






